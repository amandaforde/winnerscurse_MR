---
title: "Removing Winner's Curse bias in two-sample Mendelian randomisation with summary
  data"
author: "Amanda Forde"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
  pdf_document:
    toc: yes
editor_options:
  markdown:
    wrap: 72
---

Source:
<https://github.com/amandaforde/winnerscurse_MR/MR_SimSS_derivation.Rmd>

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(dplyr)
library(TwoSampleMR)
library(ggplot2)
library(patchwork)
library(ggpubr)
library(gridExtra)
library(tidyr)
library(RColorBrewer)
col <- brewer.pal(8,"Dark2")
```

<br>

## Proposed Method Derivation

If we randomly split the full data set into two fractions $\pi$ and
$1-\pi$, conditional on the full $\beta_{X1}$ and $\beta_{Y1}$
estimators, $\hat\beta_{X1}$ and $\hat\beta_{Y1}$, it is possible to
simulate values for $\hat \beta_{X1_\pi}$ and $\hat \beta_{Y1_\pi}$, the
estimators in the first fraction of the full sample. Thus, we have
$\hat\beta_{X1} = \pi\hat \beta_{X1_\pi} + (1-\pi)\hat \beta_{X1_{1-\pi}}$
and similarly,
$\hat\beta_{Y1} = \pi\hat \beta_{Y1_\pi} + (1-\pi)\hat \beta_{Y1_{1-\pi}}$,
in which $\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ are the
estimators in the second fraction of the sample. With the simulated
values for $\hat \beta_{X1_\pi}$ and $\hat \beta_{Y1_\pi}$, values for
$\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ can be easily
obtained using the equations above. This provides us with an alternative
method which removes the problem of *Winner's Curse* using repeated
randomisation and selection of SNPs using $\hat \beta_{X1_\pi}$. The IVW
method, or indeed, any MR method of choice, is then fitted using
$\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ and these IVW
estimators are averaged over differing repeated runs.

Therefore, in order to perform this method, we must first establish the
(asymptotic) conditional distribution of:
$$\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix}$$

***Note:*** We have referred to our estimators of interest as
$\hat\beta_{X1}$ and $\hat\beta_{Y1}$, instead of simply $\hat\beta_{X}$
and $\hat\beta_{Y}$, to ensure clarity in our proof below. In the proof,
we will be dealing with linear regression in matrix form and
$\hat\beta_X$ will refer to a vector of length 2 in which the first
entry will be $\hat\beta_{X0}$ and the second entry will be
$\hat\beta_{X1}$, our estimator of interest.

<br>

**PROOF:**

***Useful property:*** Suppose that we have a random vector $\bf Z$ that
is partitioned into components $\bf X$ and $\bf Y$ that is realized from
a multivariate normal distribution with mean vector with corresponding
components $\bf \mu_X$ and $\bf \mu_Y$, and variance-covariance matrix
which has been partitioned into four parts as shown below:

$$\bf Z = \begin{pmatrix} \bf X \\ \bf Y \end{pmatrix} \sim N\left( \begin{pmatrix} \bf \mu_X \\ \bf \mu_Y \end{pmatrix}, \begin{pmatrix} \bf \sum_X & \bf \sum_{XY} \\ \bf \sum_{YX} & \bf \sum_{Y} \end{pmatrix} \right)$$

Here, $\bf \sum_{X}$ is the variance-covariance matrix for the random
vector $\bf X$. $\bf \sum_{Y}$ is the variance-covariance matrix for the
random vector $\bf Y$ and $\bf \sum_{YX}$ contains the covariances
between the elements of $\bf X$ and the corresponding elements of
$\bf Y$.

Then, the conditional distribution of $\bf Y$ given that $\bf X$ takes a
particular value $\bf x$ is also going to be a multivariate normal with
conditional expectation:

$$E(\bf Y | \bf X = \bf x) = \bf \mu_Y + \bf \sum_{YX} \bf \left(\sum_X\right)^{-1}(\bf x - \bf \mu_X)$$
The conditional variance-covariance matrix of $\bf Y$ given that
$\bf X = \bf x$ is equal to the variance-covariance matrix for $\bf Y$
minus the term that involves the covariances between $\bf X$ and $\bf Y$
and the variance-covariance matrix for $\bf X$:

$$\bf{var}(\bf Y | \bf X = \bf x) = \bf \sum_Y - \sum_{YX}\left(\sum_X\right)^{-1}\sum_{XY}$$

Therefore, if we let
$\bf X = \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix}$
and
$\bf Y = \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix}$,
we can then use the above to obtain
$$E\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right) \text{ and    }\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right).$$

Thus, we now merely have to work out the values for the following:

i)  ${\bf \mu_Y} = E \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix}$

ii) ${\bf \mu_X} = E \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix}$

iii) ${\bf \sum_X} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right) & \text{var}\left(\hat \beta_{Y1}\right) \end{pmatrix}$

iv) ${\bf \sum_{XY}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1}, \hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{Y1}, \hat\beta_{X1_\pi}\right) & \text{cov}\left(\hat\beta_{Y1}, \hat \beta_{Y1_\pi}\right) \end{pmatrix}$

v)  ${\bf \sum_{YX}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1_\pi}, \hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{Y1_\pi}, \hat\beta_{X1}\right) & \text{cov}\left(\hat\beta_{Y1_\pi}, \hat \beta_{Y1}\right) \end{pmatrix}$

vi) ${\bf \sum_{Y}} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right) & \text{var}\left(\hat \beta_{Y1_\pi}\right) \end{pmatrix}$

<br>

[$\star \; E(\hat\beta_{X1})$ and
$\text{var}(\hat\beta_{X1})$:]{style="color: purple;"}

We will first work out ${\bf \mu_X}$ and ${\bf \sum_X}$. Let us assume
the following structural equations model linking genotype at a given
SNP, $G$, a continuous exposure, $X$ and a continuous outcome, $Y$.
Thus, for a randomly selected individual in the population, genotype,
exposure and outcome are causally linked via the following equations.
Note that $\varepsilon_X$, $\varepsilon_Y$ and $U$ are zero mean error
terms with finite variance.

$$X = \beta_X G + \varepsilon_X + \delta_{u,x}U$$
$$Y = \beta X + \varepsilon_Y + \delta_{u,y}U$$ Here, the terms
$\varepsilon_X$, $\varepsilon_Y$ and $U$ are zero mean error terms with
finite variance. It can be shown that
$\text{cov}(X,Y) = \beta \cdot \text{var}(X) + \delta_{u,x}\delta_{u,y}\cdot\text{var}(U)$
and thus, the slope coefficient for the regression of $Y$ on $X$ is only
unbiased for $\beta$ when there is no confounding, i.e.
$\delta_{u,x}\delta_{u,y}\cdot\text{var}(U) = 0$.

<br>

***Note:*** Consider linear regression in matrix form in which we have
$\bf{X} = \bf{G_X}\beta_X + \epsilon$. Then, it is well known that the
least squares solution for
${\bf \hat \beta_X} = \begin{pmatrix} \hat \beta_{X0} \\ \hat \beta_{X1} \\ \end{pmatrix}$
is ${\bf \hat \beta_X} = (\bf{G_X}^T\bf{G_X})^{-1}\bf{G_X}^T\bf{X}$.
This vector ${\bf \hat \beta_X}$ is normally distributed with mean
$(\bf{G_X}^T\bf{G_X})^{-1}(\bf{G_X}^T\bf{G_X})\beta = \beta$ and
covariance matrix $\sigma_X^2(\bf{G_X}^T\bf{G_X})^{-1}$ =
$(\bf{G_X}^T\bf{G_X})^{-1}$ if $\sigma_X^2$ is assumed to equal 1.
Similarly, if we have $\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$, then the
vector $\bf \hat \beta_Y$ is normally distributed with mean
$(\bf{G_Y}^T\bf{G_Y})^{-1}(\bf{G_Y}^T\bf{G_Y})\beta_Y = \beta_Y$ and
covariance matrix $\sigma_Y^2(\bf{G_Y}^T\bf{G_Y})^{-1}$ =
$(\bf{G_Y}^T\bf{G_Y})^{-1}$ if $\sigma_Y^2 = 1$.

<br>

Now, bearing in mind the above, suppose for each randomly sampled
individual in our large data set, we have measured their genotype for a
given SNP $i$. As well as genotype information, we have exposure values
for $N_X$ of these individuals and outcome data has been collected for
$N_Y$ of these individuals. It is possible that there could be
individuals for which we have both exposure and outcome values, i.e.
there may exist an overlap of individuals that are found in both the
exposure and outcome group. We denote the number of individuals that are
found in both as $N_{\text{overlap}}$. Clearly,
$N_{\text{overlap}} \le \text{min}\{N_X,N_Y\}$.

Focussing on the sample of $N_X$ individuals with measurements for
genotype and exposure, we denote the genotypes for the given SNP $i$ as
$G_{1},...,G_{N_X}$, with $G_j \in \{0,1,2\}$ referring to the genotype
of individual $j$ at that SNP. These genotypes for the first sample of
$N_X$ individuals can be organized in a matrix:

$$\bf{G_X} =\begin{pmatrix} 1 & G_{1} \\ 1 & G_{2} \\ \vdots & \vdots \\ 1 & G_{N_X} \end{pmatrix} = \begin{pmatrix}  \bf G_{\text{overlap}} \\ \bf G_{X_1} \end{pmatrix}$$
For this SNP $i$, an equation of the form
$\bf{X} = \bf{G_X}\beta_X + \epsilon$ is assumed in which $X_j$ is the
exposure value of individual $j$, $\bf{G_X}$ is defined as above and
$\sigma_X^2 = 1$. In order to obtain $\text{var}(\hat \beta_{X1})$ for
this SNP, we must obtain the covariance matrix of $\bf \hat \beta_X$ as
$\text{var}(\hat \beta_{X1})$ is equal to the bottom right entry of this
matrix. Thus, it follows that:

$$\bf{G_X}^T\bf{G_X} = \begin{pmatrix} N_X & \sum_{j=1}^{N_X} G_j \\ \sum_{j=1}^{N_X} G_j & \sum_{j=1}^{N_X} G_j^2  \end{pmatrix}$$

If we let $\text{maf}_i < 0.5$ denote the allele frequency for the
variant allele of SNP $i$ over the population and assume that the SNP is
in Hardy Weinberg equilibrium, then due to independent sampling, each
$G_j$ is binomially distributed with $E(G_j) = 2\text{maf}_i$ and
$E(G_j^2) = (E(G_j))^2 + \text{var}(G_j) = 4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)$.
By the law of large numbers, this gives:

$$\bf{G_X}^T\bf{G_X} \sim \begin{pmatrix} N_X & N_X \cdot E(G_j)\\ N_X \cdot E(G_j) & N_X \cdot E(G_j^2)  \end{pmatrix} = \begin{pmatrix} N_X & N_X \cdot 2\text{maf}_i \\ N_X \cdot 2\text{maf}_i & N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))  \end{pmatrix}$$

Now we wish to obtain $(\bf{G_X}^T\bf{G_X})^{-1}$ and extract the bottom
right entry of the resulting matrix. In the above matrix, we let
$a = N_X$, $b = N_X \cdot 2\text{maf}_i$, $c = N_X \cdot 2\text{maf}_i$
and $d = N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))$.
Then, it can easily be shown that for SNP $i$, assuming
$\sigma_X^2 = 1$, we have:
$$\text{var}(\hat \beta_{X1}) \sim \frac{a}{ad-bc} = \frac{N_X}{(N_X)(N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))) - (N_X \cdot 2\text{maf}_i)^2} = \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

[$\star \; E(\hat\beta_{Y1})$ and
$\text{var}(\hat\beta_{Y1})$:]{style="color: purple;"}

In a similar fashion, for a given SNP $i$, if an equation of the form
$\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$ is assumed in which $Y_j$ is the
outcome value of individual $j, j = 1,..., N_Y$, then $\bf{G_Y}$ can be
defined as:
$$\bf{G_Y} =\begin{pmatrix} 1 & G_{1} \\ 1 & G_{2} \\ \vdots & \vdots \\ 1 & G_{N_Y} \end{pmatrix} = \begin{pmatrix}  \bf G_{\text{overlap}} \\ \bf G_{Y_1} \end{pmatrix}$$

***Note:*** In the above matrix $\bf G_Y$, the entries in the second
column from $G_1$ to $G_{N_{\text{overlap}}}$ will be identical to those
in the same positions of the second column of $\bf G_X$ as these entries
represent the genotypes of individuals who have had both their outcome
and exposure values measured. However, we expect the entries in the
second column of $\bf G_Y$ from $G_{N_{\text{overlap}}+1}$ to $G_{N_Y}$
to differ as these entries represent the other individuals who have only
had their value for the outcome measured. These identical parts of
$\bf G_X$ and $\bf G_Y$ are represented by the matrix
$\bf G_{\text{overlap}}$.

With $\sigma_Y^2 = 1$, then $\text{var}(\hat\beta_{Y1})$ is easily
obtained as:
$$\text{var}(\hat \beta_{Y1}) \sim \frac{1}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

[$\star \; \text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$:]{style="color: purple;"}

The aim is now to obtain the covariance of these regression coefficients
for each SNP, i.e. $\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$. From
above and using the two equations we have constructed
$\bf{X} = \bf{G_X}\beta_X + \epsilon$ and
$\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$, we know that the estimated
regression coefficient vectors corresponding to the SNP-exposure and
SNP-outcome regressions are:
$${\bf \hat \beta_X} = (\bf{G_X}^T\bf{G_X})^{-1}\bf{G_X}^T\bf{X}$$
$${\bf \hat \beta_Y} = (\bf{G_Y}^T\bf{G_Y})^{-1}\bf{G_Y}^T\bf{Y}$$

It can be easily shown that
$N_X({\bf G_X}^T{\bf G_X})^{-1} \sim N_Y(\bf{G_Y}^T\bf{G_Y})^{-1}$ as:
$$({\bf G_X}^T{\bf G_X})^{-1} \sim \frac{1}{N_X} \begin{pmatrix} 1 & 2\text{maf}_i \\  2\text{maf}_i &  4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)  \end{pmatrix} ^{-1} \text{and } ({\bf G_Y}^T{\bf G_Y})^{-1} \sim \frac{1}{N_Y} \begin{pmatrix} 1 & 2\text{maf}_i \\  2\text{maf}_i &  4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)  \end{pmatrix} ^{-1}.$$
Therefore, letting ${\bf C} \sim N_X(\bf{G_X}^T\bf{G_X})^{-1}$ and
${\bf C} \sim N_Y(\bf{G_Y}^T\bf{G_Y})^{-1}$, it follows that:
$${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \textbf{cov}\left(\frac{\bf{C}}{N_X}{\bf G_X^T}{\bf X}, \frac{\bf C}{N_Y}{\bf G_Y^T}{\bf Y}\right) = \frac{1}{N_X N_Y} \textbf{cov}\left({\bf C} {\bf G}_{\text{overlap}}^T {\bf X}_{\text{overlap}}, {\bf C} {\bf G}_{\text{overlap}}^T {\bf Y}_{\text{overlap}}\right)$$
giving
$${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \frac{\bf C {\bf G}_{\text{overlap}}^T \text{cov}( {\bf X}_{\text{overlap}},  {\bf Y}_{\text{overlap}}) {\bf G}_{\text{overlap} }{\bf C}^T }{N_X N_Y}$$

Here, ${\bf X}_{\text{overlap}}$ and ${\bf Y}_{\text{overlap}}$ denote
the first $N_{\text{overlap}}$ elements of $\bf X$ and $\bf Y$. Noting
that
$\textbf{cov}( {\bf X}_{\text{overlap}}, {\bf Y}_{\text{overlap}}) = \text{cov}(X,Y) {\bf I}_{N_\text{overlap}}$,
where ${\bf I}_{N_\text{overlap}}$ is the
${N_\text{overlap}} \times {N_\text{overlap}}$ identity matrix,
${\bf G_{\text{overlap}}}^T{\bf G_{\text{overlap}}} \sim N_\text{overlap}{\bf C}^{-1}$
and ${\bf C} = {\bf C}^T$, it follows that:
$${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \frac{N_{\text{overlap}}\text{cov}(X,Y)}{N_XN_Y} {\bf C}$$

As stated above, we are interested in
$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$ which is the bottom right
entry of the matrix $\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)$. As the
bottom right entry of $\bf C$ is
$\frac{1}{2\text{maf}_i(1-\text{maf}_i)}$ and letting
$\text{cov}(X,Y) = \text{cor}(X,Y) \sqrt{\text{var}(X)\text{var}(Y)} = \rho$
in which $\text{cor}(X,Y) = \rho$ and
$\text{var}(X) = \text{var}(Y) = 1$, we have:
$$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1}) \sim \frac{N_{\text{overlap}}\rho}{N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

Therefore, putting all of the theoretical results together, we have the
following expressions for ${\bf \mu_X}$ and ${\bf \sum_X}$:

ii) ${\bf \mu_X} = E \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} = \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix}$

iii) ${\bf \sum_X} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right) & \text{var}\left(\hat \beta_{Y1}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)} \begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

<br>

[$\star \; E(\hat\beta_{X1_\pi})$, $\text{var}(\hat\beta_{X1_\pi})$ and
$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1})$:]{style="color: purple;"}

Now, suppose we split the full data set into two fractions $\pi$ and
$1-\pi$. Thus, in the first fraction (sub sample), we would have
***approximately*** $\pi N_X$ individuals who have an exposure
measurement, $\pi N_Y$ who have had their outcome measured and
$\pi N_{\text{overlap}}$ individuals who have values available for both
exposure and outcome. We can write the genotype matrix of those that
have had their exposure measured in this first sub sample, denoted by
$G_{X\pi}$, as:
$$\bf G_{X\pi} = \begin{pmatrix} \bf G_{\text{overlap}{\pi}} \\ \bf G_{X_1\pi} \end{pmatrix}$$

Based on this genotype matrix, and denoting the $(\pi N_X) \times 1$
exposure vector of this sub sample by $\bf X_{\pi}$, the estimated
regression coefficient vector corresponding to this SNP-exposure
regression in the sub sample is:
$${\bf \hat \beta_{X_\pi}} = ({\bf G_{X\pi}^T G_{X\pi}})^{-1}{\bf G_{X\pi}^TX_\pi} \sim \frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_\pi}$$

We saw above that
$\text{var}\left(\hat \beta_{X1}\right) \sim \frac{1}{N_X \cdot 2\text{maf}_i(1-\text{maf}_i)}$,
and thus, it is clear to see then that we must have
$\text{var}\left(\hat \beta_{X1_{\pi}}\right) \sim \frac{1}{\pi N_X \cdot 2\text{maf}_i(1-\text{maf}_i)}$.

Now, let us consider
$\textbf{cov}\left(\bf \hat\beta_{X_\pi}, \hat\beta_X\right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_\pi}, \frac{\bf C}{N_X}{\bf G_{X}^TX}\right)$.
As $\bf G_{X\pi}$ and $\bf X_\pi$ are clearly subsets of $\bf G_X$ and
$\bf X$, respectively, then we get:
$$\textbf{cov}\left({\bf \hat\beta_{X_\pi}, \hat\beta_{X}} \right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_{\pi}}, \frac{\bf C}{N_X}{\bf G_{X\pi}^TX_{\pi}} \right) = \frac{\bf CG_{X \pi}^T \text{var}(\bf X_\pi){\bf G_{X \pi}C^T}}{\pi N_X \cdot N_X}$$
This expression can be simplified in a similar manner to previously,
noting that ${\bf G_{X\pi}^TG_{X\pi}} \sim \pi N_X \cdot {\bf C}^{-1}$:
$$\textbf{cov}({\bf\hat\beta_{X_\pi}, \hat\beta_X}) \sim \frac{\pi N_X \cdot  {\bf C}}{\pi N_X \cdot N_X} = \frac{\bf C}{N_X}$$
We then obtain:
$$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1}) \sim \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$$
In addition, from this set-up, we can easily state that
$E(\hat\beta_{X1_\pi}) = \beta_{X1}$.

<br>

[$\star \; E(\hat\beta_{Y1_\pi})$, $\text{var}(\hat\beta_{Y1_\pi})$ and
$\text{cov}(\hat\beta_{Y1_\pi}, \hat\beta_{Y1})$:]{style="color: purple;"}

Now, due to the way we have partitioned the original data set, we can
write the genotype matrix for the outcome-SNP regression in the first
sub sample, denoted by $\bf G_{Y\pi}$, in the form:
$$\bf G_{Y\pi} = \begin{pmatrix}  \bf G_{\text{overlap}{\pi}} \\ \bf G_{Y_1\pi} \end{pmatrix} $$

Similar to when we considered the full data set, $\bf G_{Y\pi}$ and
$\bf G_{X\pi}$ share their first $\pi \times N_{\text{overlap}}$ rows,
represented by the matrix $\bf G_{\text{overlap}\pi}$. This matrix,
$\bf G_{\text{overlap}\pi}$, contains the genotypes of the individuals
who are contained in the first sub sample and have had both their
outcome and exposure values measured. The estimated regression
coefficient vector corresponding to the SNP-outcome regression in the
sub sample is:
$${\bf \hat \beta_{Y_\pi}} = ({\bf G_{Y\pi}^T G_{Y\pi}})^{-1}{\bf G_{Y\pi}^TY_\pi} \sim \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_\pi}$$
Therefore, following the same process as we used above in order to
obtain $\text{var}\left(\hat \beta_{X1_{\pi}}\right)$ and
$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1})$, it is easy to see that
we should obtain
$\text{var}\left(\hat \beta_{Y1_{\pi}}\right) \sim \frac{1}{\pi N_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$
and
$\text{cov}(\hat\beta_{Y1_\pi}, \hat\beta_{Y1}) \sim \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$.
Also, we get $E(\hat\beta_{Y1_\pi}) = \beta_{Y1}$.

<br>

[$\star \; \text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi})$:]{style="color: purple;"}

We next consider $\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi})$.
We saw that $\bf G_{X\pi}$ can be partitioned as
$\bf G_{X\pi} = \begin{pmatrix} \bf G_{\text{overlap}{\pi}} \\ \bf G_{X_1\pi} \end{pmatrix}$
and similarly, $\bf G_{Y\pi}$ can be partitioned as
$\bf G_{Y\pi} = \begin{pmatrix} \bf G_{\text{overlap}{\pi}} \\ \bf G_{Y_1\pi} \end{pmatrix}$.
Therefore, we get:
$$\textbf{cov}\left({\bf\hat\beta_{X_\pi}, \hat\beta_{Y_\pi} }\right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_{\pi}}, \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_{\pi}} \right) = \frac{\bf CG_{overlap \pi}^T \text{cov}(\bf X_{overlap\pi}, Y_{overlap\pi}){\bf G_{overlap \pi}C^T}}{\pi N_X \cdot \pi N_Y}$$
This simplifies, giving:
$$\textbf{cov}({\bf \hat\beta_{X_\pi}, \hat\beta_{Y_\pi}}) \sim \frac{\pi N_{\text{overlap}}\text{cov}(X,Y)}{\pi N_X \cdot \pi N_Y} {\bf C}$$
and subsequently:
$$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi}) \sim \frac{N_{\text{overlap}}\rho}{\pi N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

[$\star \; \text{cov}(\hat\beta_{X1}, \hat\beta_{Y1_\pi})$ and
$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1})$:]{style="color: purple;"}

Now, let us find $\textbf{cov}({\bf\hat\beta_{X}, \hat\beta_{Y_\pi}})$
and $\textbf{cov}({\bf\hat\beta_{X_\pi}, \hat\beta_{Y}})$. Using our
expressions for $\bf \hat\beta_X$ and $\bf \hat\beta_{Y_\pi}$ detailed
above and knowing that the genotype matrices $\bf G_{X}$ and
$\bf G_{Y_\pi}$ share genotype information of $\pi N_{\text{overlap}}$
individuals which is contained in $G_{\text{overlap}\pi}$, we get:
$$\textbf{cov}\left({\bf \hat\beta_{X}, \hat\beta_{Y_\pi}}\right) \sim \textbf{cov}\left(\frac{\bf C}{N_X}{\bf G_{X}^TX}, \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_{\pi}} \right) = \frac{\bf CG_{overlap \pi}^T \text{cov}(\bf X_{overlap\pi}, Y_{overlap\pi}){\bf G_{overlap \pi}C^T}}{ N_X \cdot \pi N_Y}$$

It follows that:
$$\textbf{cov}({\bf\hat\beta_{X}, \hat\beta_{Y_\pi}}) \sim \frac{ \pi N_{\text{overlap}}\text{cov}(X,Y)}{N_X \cdot \pi N_Y} {\bf C}$$
and this gives:
$$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1_\pi}) \sim \frac{N_{\text{overlap}}\rho}{ N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

In a very similar fashion, using our expressions for
$\bf \hat\beta_{X_\pi}$ and $\bf \hat\beta_{Y}$ detailed above, we get:
$$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1}) \sim \frac{N_{\text{overlap}}\rho}{ N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

Therefore, we finally have the following results for $\bf \mu_Y$,
${\bf \sum_{XY}}$, ${\bf \sum_{YX}}$ and ${\bf \sum_{Y}}$:

i)  ${\bf \mu_Y} = E \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} = \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix}$

ii) ${\bf \sum_{XY}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1}, \hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{Y1}, \hat\beta_{X1_\pi}\right) & \text{cov}\left(\hat\beta_{Y1}, \hat \beta_{Y1_\pi}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

iii) ${\bf \sum_{YX}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1_\pi}, \hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{Y1_\pi}, \hat\beta_{X1}\right) & \text{cov}\left(\hat\beta_{Y1_\pi}, \hat \beta_{Y1}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

iv) ${\bf \sum_{Y}} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right) & \text{var}\left(\hat \beta_{Y1_\pi}\right) \end{pmatrix} = \frac{1}{\pi} \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

Therefore, it is clear to see that
${\bf \sum_{X}} = {\bf \sum_{XY}} = {\bf \sum_{YX}}$ and
$\pi \cdot {\bf \sum_{Y}} = {\bf \sum_{X}}$.

Then, using the identities, we can establish
$E\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right)$
and
$\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right)$
as follows:

-   $E\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right) = \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix} + \bf \sum_{X} \bf \left(\sum_X\right)^{-1} \left( \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} - \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix} \right) = \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix}$

-   $\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right) = \frac{1}{\pi} {\bf \sum_{X}} - {\bf \sum_{X}} \left( {\bf \sum_{X}} \right)^{-1}{\bf \sum_{X}} = \left( \frac{1-\pi}{\pi}\right) {\bf \sum_{X}} = \left( \frac{1-\pi}{\pi}\right) \cdot \textbf{var}\left( \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right)$

Therefore, we finally have obtained the required conditional
distribution:

$$ \left( \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} \right) \sim N\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}, \left( \frac{1-\pi}{\pi}\right) \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}\right)$$

<br> 

***Note:*** The above expression holds in the case that $\text{var}(X) = \text{var}(Y) = 1$, as stated above. Let us briefly consider how we could rewrite this expression if it is ***not*** the case that $\text{var}(X) = \text{var}(Y) = 1$. Firstly, our expression would now have to include $\text{var}(X)$ and $\text{var(Y)}$ as follows:

$$ \left( \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} \right) \sim N\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}, \left( \frac{1-\pi}{\pi}\right) \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{\text{var}(X)}{N_X} & \frac{N_{\text{overlap}}\rho\sqrt{\text{var}(X)\text{var}(Y)}}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho\sqrt{\text{var}(X)\text{var}(Y)}}{N_XN_Y} & \frac{\text{var}(Y)}{N_Y} \end{pmatrix}\right)$$

However, it is possible to write this in another form in which we make use of the fact that we already have estimates for $\text{se}\left(\hat \beta_{X1}\right)$ and $\text{se}\left(\hat \beta_{Y1}\right)$. We note the following:

i. As $\text{var}(\hat \beta_{X1}) \sim \frac{\text{var}(X)}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$, then $\left(\text{se}(\hat \beta_{X1})\right)^2 \sim \frac{\text{var}(X)}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$.

ii. As $\text{var}(\hat \beta_{Y1}) \sim \frac{\text{var}(Y)}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}$, then $\left(\text{se}(\hat \beta_{Y1})\right)^2 \sim \frac{\text{var}(Y)}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}$.

iii. $\text{se}\left(\hat \beta_{X1}\right) \cdot \text{se}\left(\hat \beta_{Y1}\right) = \sqrt{\frac{\text{var}(X)}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}}\cdot \sqrt{\frac{\text{var}(Y)}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)} \frac{\sqrt{\text{var}(X)\text{var}(Y)}}{\sqrt{N_X N_Y}}$

Putting all of the above together, we obtain an alternative form of the required conditional distribution:

$$ \left( \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} \right) \sim N\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}, \left( \frac{1-\pi}{\pi}\right) \begin{pmatrix} \left(\text{se}\left(\hat \beta_{X1}\right)\right)^2 & \text{se}\left(\hat \beta_{X1}\right) \text{se}\left(\hat \beta_{Y1}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}\\ \text{se}\left(\hat \beta_{X1}\right) \text{se}\left(\hat \beta_{Y1}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}} & \left(\text{se}\left(\hat \beta_{Y1}\right)\right)^2 \end{pmatrix}\right)$$


<br><br>


**NEXT STEP:**

Ideally, we would like to be able to use our method when only summary statistics are available, i.e. we only have information on $\hat\beta_{X_i}$, $\hat\beta_{Y_i}$, $\text{se}(\hat\beta_{X_i})$ and $\text{se}(\hat\beta_{Y_i})$ for each SNP $i$. Therefore, in the equation above, we would need to estimate a value for $\frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}$. 

[$\star$]{style="color: blue;"} **Note:** From here, we will denote $\hat\beta_{X1}$ for SNP $i$ simply by $\hat\beta_{X_i}$ and similarly, $\hat\beta_{Y1}$ for SNP $i$ will be represented by $\hat\beta_{Y_i}$. 

In a similar form to the expression above, we assume for each SNP $i$ that $\hat\beta_{X_i}$ and $\hat\beta_{Y_i}$ follow the following bivariate normal distribution: 

$$  \begin{pmatrix} \hat \beta_{X_i} \\ \hat \beta_{Y_i} \end{pmatrix} \sim N\left( \begin{pmatrix} \beta_{X_i}  \\ \beta_{Y_i} \end{pmatrix}, \begin{pmatrix} \left(\text{se}\left(\hat \beta_{X_i}\right)\right)^2 & \text{se}\left(\hat \beta_{X_i}\right) \text{se}\left(\hat \beta_{Y_i}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}\\ \text{se}\left(\hat \beta_{X_i}\right) \text{se}\left(\hat \beta_{Y_i}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}} & \left(\text{se}\left(\hat \beta_{Y_i}\right)\right)^2 \end{pmatrix}\right)$$


For convenience, let us represent $\frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}$ by $\lambda$ as this is the value we are interested in estimating. Thus $\lambda$ is essentially equivalent to the correlation between $\hat\beta_{X_i}$ and $\hat\beta_{Y_i}$ for each SNP $i$. In addition, we let $\text{se}(\hat\beta_{X_i}) \approx \sigma_{X_i}$ and $\text{se}(\hat\beta_{Y_i}) \approx \sigma_{Y_i}$. The bivariate normal distribution then takes the simpler form of: 

$$  \begin{pmatrix} \hat \beta_{X_i} \\ \hat \beta_{Y_i} \end{pmatrix} \sim N\left( \begin{pmatrix} \beta_{X_i}  \\ \beta_{Y_i} \end{pmatrix}, \begin{pmatrix} \left(\sigma_{X_i}\right)^2 & \sigma_{X_i} \sigma_{Y_i} \lambda \\ \sigma_{X_i} \sigma_{Y_i} \lambda & \left(\sigma_{Y_i}\right)^2 \end{pmatrix}\right)$$

Given this, the corresponding ***probability density function*** for each SNP $i$ can be written as:

$$f\left( \hat \beta_{X_i}, \hat \beta_{Y_i} \right) = \frac{1}{2\pi\sqrt{1-\lambda^2}\sigma_{X_i}\sigma_{Y_i}}e^{ -\frac{1}{2(1-\lambda^2)} \left[ \left( \frac{\hat \beta_{X_i} - \beta_{X_i}}{\sigma_{X_i}}\right)^2 - 2\lambda\left(\frac{\hat \beta_{X_i} - \beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat \beta_{Y_i} - \beta_{Y_i}}{\sigma_{Y_i}}\right) + \left( \frac{\hat \beta_{Y_i} - \beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]}$$


Thus, we can form a likelihood function for $\lambda$ for a set of $n$ SNPs, $\left(\hat\beta_{X_i}, \hat\beta_{Y_i}, \sigma_{X_i}, \sigma_{Y_i}\right)$, $i=1,...,n$:

$$
\begin{aligned}
L\left( \lambda | \bf{\hat\beta_X, \hat\beta_Y, \sigma_X, \sigma_Y} \right) &= \Pi_{i=1}^{n}f\left( \hat \beta_{X_i}, \hat \beta_{Y_i} \right) \\ &= \left( \frac{1}{2\pi\sqrt{1-\lambda^2}} \right)^n \cdot\Pi_{i=1}^{n}\left(\frac{1}{\sigma_{X_i}\sigma_{Y_i}}\right)\cdot e^{-\frac{1}{2(1-\lambda^2)}\left[ \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)^2 -2\lambda\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] + \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]}
\end{aligned}
$$


where $\hat\beta_X = (\hat\beta_{X_1},...,\hat\beta_{X_n}), \hat\beta_Y = (\hat\beta_{Y_1},...,\hat\beta_{Y_n}), \sigma_X = (\sigma_{X_1},...,\sigma_{X_n})$ and $\sigma_Y = (\sigma_{Y_1},...,\sigma_{Y_n})$, the observed values for each SNP.


The ***log-likelihood function*** for $\lambda$,  $\text{log}L(\lambda) = l(\lambda)$ can then be derived as follows:

$$
l\left( \lambda \right) = -n \text{log}(2\pi) - \frac{n}{2}\text{log}(1-\lambda^2) - \text{log}(\Pi_{i=1}^{n}\left(\sigma_{X_i}\sigma_{Y_i}\right)) - \frac{1}{2(1-\lambda^2)}\left[ \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)^2 -2\lambda\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] + \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]
$$


[$\star$]{style="color: blue;"} **Assumption:** If assume that $\beta_{X_i} = 0$ and $\beta_{Y_i} = 0$ for $i=1,...,n$, we can then denote the following constants by letters to simplify our derivations:

- $\sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)^2 = \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)^2 = A$
- $\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] = \sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] = B$
- $\sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)^2 = \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)^2 = C$


In order to find the ***maximum likelihood estimate*** (MLE) for $\lambda$, we must first obtain an expression for $\frac{d}{d\lambda}l(\lambda)$ and then solve equal to zero. Using our defined constants $A$, $B$ and $C$ above, we proceed as follows:


$$
\begin{aligned}
\frac{d}{d\lambda}l\left( \lambda \right) &= 
\frac{d}{d\lambda}\left[ -n \text{log}(2\pi) - \frac{n}{2}\text{log}(1-\lambda^2) - \text{log}\left(\Pi_{i=1}^{n}\left(\sigma_{X_i}\sigma_{Y_i}\right)\right) - \frac{1}{2(1-\lambda^2)}\left(A - 2\lambda B + C\right)\right] \\ &=  - \frac{n}{2}\frac{d}{d\lambda}\left[\text{log}(1-\lambda^2) \right] - \frac{1}{2}\frac{d}{d\lambda}\left[ \frac{A}{1-\lambda^2} - \frac{2\lambda B}{1-\lambda^2} +\frac{C}{1-\lambda^2}\right] \\ &= -\frac{n}{2}\cdot\left[-\frac{2\lambda}{1-\lambda^2}\right] - \frac{1}{2}\cdot\left[ \frac{2\lambda A}{\left(1-\lambda^2\right)^2} - \frac{2\left(1+\lambda^2\right)B}{\left(1-\lambda^2\right)^2} +  \frac{2\lambda C}{\left(1-\lambda^2\right)^2}\right] \\ &= \frac{n\lambda}{\left(1-\lambda^2\right)^2}\cdot \left[ \left(1-\lambda^2\right) - \frac{A}{n} + \frac{1-\lambda^2}{\lambda}\frac{B}{n} - \frac{C}{n}\right] \\ &= \frac{n\lambda}{\left(1-\lambda^2\right)^2}\cdot \left[ \left(1-\lambda^2\right) - \tilde{A} + \frac{1-\lambda^2}{\lambda}\tilde{B} - \tilde{C}\right]
\end{aligned}
$$

in which we have let $\tilde{A} = \frac{A}{n}, \tilde{B} = \frac{B}{n}$ and $\tilde{C} = \frac{C}{n}$.

Solving the above equation equal to zero gives the following: 

$$
\begin{aligned}
\frac{d}{d\lambda}l\left( \lambda \right) = 0 &\implies \frac{n\lambda}{\left(1-\lambda^2\right)^2}\cdot \left[ \left(1-\lambda^2\right) - \tilde{A} + \frac{1-\lambda^2}{\lambda}\tilde{B} - \tilde{C}\right] = 0 \\ &\implies \left[ \left(1-\lambda^2\right) - \tilde{A} + \frac{1-\lambda^2}{\lambda}\tilde{B} - \tilde{C}\right] = 0 \\ &\implies \tilde{A} - \frac{1-\lambda^2}{\lambda}\tilde{B} + \tilde{C} = 1 - \lambda^2 \\ &\implies \lambda^3 - \tilde{B}\lambda^2 + \left(\tilde{A} + \tilde{C} - 1\right)\lambda - \tilde{B} = 0
\end{aligned}
$$

[$\star$]{style="color: blue;"} **Cubic equation for $\lambda$?**



