---
title: "Removing Winner's Curse bias in two-sample Mendelian randomisation with summary
  data"
author: "Amanda Forde"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

Source:
<https://github.com/amandaforde/winnerscurse_MR/MR_SimSS_derivation.Rmd>

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(dplyr)
library(TwoSampleMR)
library(ggplot2)
library(patchwork)
library(ggpubr)
library(gridExtra)
library(tidyr)
library(RColorBrewer)
col <- brewer.pal(8,"Dark2")
```

<br>

## Proposed Method Derivation

If we randomly split the full data set into two fractions $\pi$ and
$1-\pi$, conditional on the full $\beta_{X1}$ and $\beta_{Y1}$
estimators, $\hat\beta_{X1}$ and $\hat\beta_{Y1}$, it is possible to
simulate values for $\hat \beta_{X1_\pi}$ and $\hat \beta_{Y1_\pi}$, the
estimators in the first fraction of the full sample. Thus, we have
$\hat\beta_{X1} = \pi\hat \beta_{X1_\pi} + (1-\pi)\hat \beta_{X1_{1-\pi}}$
and similarly,
$\hat\beta_{Y1} = \pi\hat \beta_{Y1_\pi} + (1-\pi)\hat \beta_{Y1_{1-\pi}}$,
in which $\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ are the
estimators in the second fraction of the sample. With the simulated
values for $\hat \beta_{X1_\pi}$ and $\hat \beta_{Y1_\pi}$, values for
$\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ can be easily
obtained using the equations above. This provides us with an alternative
method which removes the problem of *Winner's Curse* using repeated
randomisation and selection of SNPs using $\hat \beta_{X1_\pi}$. The IVW
method, or indeed, any MR method of choice, is then fitted using
$\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ and these IVW
estimators are averaged over differing repeated runs.

Therefore, in order to perform this method, we must first establish the
(asymptotic) conditional distribution of:
$$\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix}$$

[$\star$]{style="color: blue;"} **Note:** We have referred to our
estimators of interest as $\hat\beta_{X1}$ and $\hat\beta_{Y1}$, instead
of simply $\hat\beta_{X}$ and $\hat\beta_{Y}$, to ensure clarity in our
proof below. In the proof, we will be dealing with linear regression in
matrix form and $\hat\beta_X$ will refer to a vector of length 2 in
which the first entry will be $\hat\beta_{X0}$ and the second entry will
be $\hat\beta_{X1}$, our estimator of interest.

[$\star$]{style="color: blue;"} **Note:** It is important to mention
that the relationships,
$\hat\beta_{X1} = \pi\hat \beta_{X1_\pi} + (1-\pi)\hat \beta_{X1_{1-\pi}}$
and
$\hat\beta_{Y1} = \pi\hat \beta_{Y1_\pi} + (1-\pi)\hat \beta_{Y1_{1-\pi}}$,
mentioned above, are in fact only ***approximate***, to order
$\frac{1}{N_X}$ and $\frac{1}{N_Y}$ respectively where $N_X$ and $N_Y$
are the sample sizes of the exposure and outcome data sets, and hold
because maximum likelihood estimates are ***asymptotically linear***.

<br>

**PROOF:**

***Useful property:*** Suppose that we have a random vector $\bf Z$ that
is partitioned into components $\bf X$ and $\bf Y$ that is realized from
a multivariate normal distribution with mean vector with corresponding
components $\bf \mu_X$ and $\bf \mu_Y$, and variance-covariance matrix
which has been partitioned into four parts as shown below:

$$\bf Z = \begin{pmatrix} \bf X \\ \bf Y \end{pmatrix} \sim N\left( \begin{pmatrix} \bf \mu_X \\ \bf \mu_Y \end{pmatrix}, \begin{pmatrix} \bf \sum_X & \bf \sum_{XY} \\ \bf \sum_{YX} & \bf \sum_{Y} \end{pmatrix} \right)$$

Here, $\bf \sum_{X}$ is the variance-covariance matrix for the random
vector $\bf X$. $\bf \sum_{Y}$ is the variance-covariance matrix for the
random vector $\bf Y$ and $\bf \sum_{YX}$ contains the covariances
between the elements of $\bf X$ and the corresponding elements of
$\bf Y$.

Then, the conditional distribution of $\bf Y$ given that $\bf X$ takes a
particular value $\bf x$ is also going to be a multivariate normal with
conditional expectation:

$$E(\bf Y | \bf X = \bf x) = \bf \mu_Y + \bf \sum_{YX} \bf \left(\sum_X\right)^{-1}(\bf x - \bf \mu_X)$$
The conditional variance-covariance matrix of $\bf Y$ given that
$\bf X = \bf x$ is equal to the variance-covariance matrix for $\bf Y$
minus the term that involves the covariances between $\bf X$ and $\bf Y$
and the variance-covariance matrix for $\bf X$:

$$\bf{var}(\bf Y | \bf X = \bf x) = \bf \sum_Y - \sum_{YX}\left(\sum_X\right)^{-1}\sum_{XY}$$

Therefore, if we let
$\bf X = \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix}$
and
$\bf Y = \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix}$,
we can then use the above to obtain
$$E\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right) \text{ and    }\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right).$$

Thus, we now merely have to work out the values for the following:

i)  ${\bf \mu_Y} = E \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix}$

ii) ${\bf \mu_X} = E \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix}$

iii) ${\bf \sum_X} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right) & \text{var}\left(\hat \beta_{Y1}\right) \end{pmatrix}$

iv) ${\bf \sum_{XY}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1}, \hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{Y1}, \hat\beta_{X1_\pi}\right) & \text{cov}\left(\hat\beta_{Y1}, \hat \beta_{Y1_\pi}\right) \end{pmatrix}$

v)  ${\bf \sum_{YX}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1_\pi}, \hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{Y1_\pi}, \hat\beta_{X1}\right) & \text{cov}\left(\hat\beta_{Y1_\pi}, \hat \beta_{Y1}\right) \end{pmatrix}$

vi) ${\bf \sum_{Y}} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right) & \text{var}\left(\hat \beta_{Y1_\pi}\right) \end{pmatrix}$

<br>

[$\star \; E(\hat\beta_{X1})$ and
$\text{var}(\hat\beta_{X1})$:]{style="color: purple;"}

We will first work out ${\bf \mu_X}$ and ${\bf \sum_X}$. Let us assume
the following structural equations model linking genotype at a given
SNP, $G$, a continuous exposure, $X$ and a continuous outcome, $Y$.
Thus, for a randomly selected individual in the population, genotype,
exposure and outcome are causally linked via the following equations.
Note that $\varepsilon_X$, $\varepsilon_Y$ and $U$ are zero mean error
terms with finite variance.

$$X = \beta_X G + \varepsilon_X + \delta_{u,x}U$$
$$Y = \beta X + \varepsilon_Y + \delta_{u,y}U$$ Here, the terms
$\varepsilon_X$, $\varepsilon_Y$ and $U$ are zero mean error terms with
finite variance. It can be shown that
$\text{cov}(X,Y) = \beta \cdot \text{var}(X) + \delta_{u,x}\delta_{u,y}\cdot\text{var}(U)$
and thus, the slope coefficient for the regression of $Y$ on $X$ is only
unbiased for $\beta$ when there is no confounding, i.e.
$\delta_{u,x}\delta_{u,y}\cdot\text{var}(U) = 0$.

<br>

***Note:*** Consider linear regression in matrix form in which we have
$\bf{X} = \bf{G_X}\beta_X + \epsilon$. Then, it is well known that the
least squares solution for
${\bf \hat \beta_X} = \begin{pmatrix} \hat \beta_{X0} \\ \hat \beta_{X1} \\ \end{pmatrix}$
is ${\bf \hat \beta_X} = (\bf{G_X}^T\bf{G_X})^{-1}\bf{G_X}^T\bf{X}$.
This vector ${\bf \hat \beta_X}$ is normally distributed with mean
$(\bf{G_X}^T\bf{G_X})^{-1}(\bf{G_X}^T\bf{G_X})\beta = \beta$ and
covariance matrix $\sigma_X^2(\bf{G_X}^T\bf{G_X})^{-1}$ =
$(\bf{G_X}^T\bf{G_X})^{-1}$ if $\sigma_X^2$ is assumed to equal 1.
Similarly, if we have $\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$, then the
vector $\bf \hat \beta_Y$ is normally distributed with mean
$(\bf{G_Y}^T\bf{G_Y})^{-1}(\bf{G_Y}^T\bf{G_Y})\beta_Y = \beta_Y$ and
covariance matrix $\sigma_Y^2(\bf{G_Y}^T\bf{G_Y})^{-1}$ =
$(\bf{G_Y}^T\bf{G_Y})^{-1}$ if $\sigma_Y^2 = 1$.

<br>

Now, bearing in mind the above, suppose for each randomly sampled
individual in our large data set, we have measured their genotype for a
given SNP $i$. As well as genotype information, we have exposure values
for $N_X$ of these individuals and outcome data has been collected for
$N_Y$ of these individuals. It is possible that there could be
individuals for which we have both exposure and outcome values, i.e.
there may exist an overlap of individuals that are found in both the
exposure and outcome group. We denote the number of individuals that are
found in both as $N_{\text{overlap}}$. Clearly,
$N_{\text{overlap}} \le \text{min}\{N_X,N_Y\}$.

Focussing on the sample of $N_X$ individuals with measurements for
genotype and exposure, we denote the genotypes for the given SNP $i$ as
$G_{1},...,G_{N_X}$, with $G_j \in \{0,1,2\}$ referring to the genotype
of individual $j$ at that SNP. These genotypes for the first sample of
$N_X$ individuals can be organized in a matrix:

$$\bf{G_X} =\begin{pmatrix} 1 & G_{1} \\ 1 & G_{2} \\ \vdots & \vdots \\ 1 & G_{N_X} \end{pmatrix} = \begin{pmatrix}  \bf G_{\text{overlap}} \\ \bf G_{X_1} \end{pmatrix}$$
For this SNP $i$, an equation of the form
$\bf{X} = \bf{G_X}\beta_X + \epsilon$ is assumed in which $X_j$ is the
exposure value of individual $j$, $\bf{G_X}$ is defined as above and
$\sigma_X^2 = 1$. In order to obtain $\text{var}(\hat \beta_{X1})$ for
this SNP, we must obtain the covariance matrix of $\bf \hat \beta_X$ as
$\text{var}(\hat \beta_{X1})$ is equal to the bottom right entry of this
matrix. Thus, it follows that:

$$\bf{G_X}^T\bf{G_X} = \begin{pmatrix} N_X & \sum_{j=1}^{N_X} G_j \\ \sum_{j=1}^{N_X} G_j & \sum_{j=1}^{N_X} G_j^2  \end{pmatrix}$$

If we let $\text{maf}_i < 0.5$ denote the allele frequency for the
variant allele of SNP $i$ over the population and assume that the SNP is
in Hardy Weinberg equilibrium, then due to independent sampling, each
$G_j$ is binomially distributed with $E(G_j) = 2\text{maf}_i$ and
$E(G_j^2) = (E(G_j))^2 + \text{var}(G_j) = 4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)$.
By the law of large numbers, this gives:

$$\bf{G_X}^T\bf{G_X} \sim \begin{pmatrix} N_X & N_X \cdot E(G_j)\\ N_X \cdot E(G_j) & N_X \cdot E(G_j^2)  \end{pmatrix} = \begin{pmatrix} N_X & N_X \cdot 2\text{maf}_i \\ N_X \cdot 2\text{maf}_i & N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))  \end{pmatrix}$$

Now we wish to obtain $(\bf{G_X}^T\bf{G_X})^{-1}$ and extract the bottom
right entry of the resulting matrix. In the above matrix, we let
$a = N_X$, $b = N_X \cdot 2\text{maf}_i$, $c = N_X \cdot 2\text{maf}_i$
and $d = N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))$.
Then, it can easily be shown that for SNP $i$, assuming
$\sigma_X^2 = 1$, we have:
$$\text{var}(\hat \beta_{X1}) \sim \frac{a}{ad-bc} = \frac{N_X}{(N_X)(N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))) - (N_X \cdot 2\text{maf}_i)^2} = \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

[$\star \; E(\hat\beta_{Y1})$ and
$\text{var}(\hat\beta_{Y1})$:]{style="color: purple;"}

In a similar fashion, for a given SNP $i$, if an equation of the form
$\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$ is assumed in which $Y_j$ is the
outcome value of individual $j, j = 1,..., N_Y$, then $\bf{G_Y}$ can be
defined as:
$$\bf{G_Y} =\begin{pmatrix} 1 & G_{1} \\ 1 & G_{2} \\ \vdots & \vdots \\ 1 & G_{N_Y} \end{pmatrix} = \begin{pmatrix}  \bf G_{\text{overlap}} \\ \bf G_{Y_1} \end{pmatrix}$$

***Note:*** In the above matrix $\bf G_Y$, the entries in the second
column from $G_1$ to $G_{N_{\text{overlap}}}$ will be identical to those
in the same positions of the second column of $\bf G_X$ as these entries
represent the genotypes of individuals who have had both their outcome
and exposure values measured. However, we expect the entries in the
second column of $\bf G_Y$ from $G_{N_{\text{overlap}}+1}$ to $G_{N_Y}$
to differ as these entries represent the other individuals who have only
had their value for the outcome measured. These identical parts of
$\bf G_X$ and $\bf G_Y$ are represented by the matrix
$\bf G_{\text{overlap}}$.

With $\sigma_Y^2 = 1$, then $\text{var}(\hat\beta_{Y1})$ is easily
obtained as:
$$\text{var}(\hat \beta_{Y1}) \sim \frac{1}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

[$\star \; \text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$:]{style="color: purple;"}

The aim is now to obtain the covariance of these regression coefficients
for each SNP, i.e. $\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$. From
above and using the two equations we have constructed
$\bf{X} = \bf{G_X}\beta_X + \epsilon$ and
$\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$, we know that the estimated
regression coefficient vectors corresponding to the SNP-exposure and
SNP-outcome regressions are:
$${\bf \hat \beta_X} = (\bf{G_X}^T\bf{G_X})^{-1}\bf{G_X}^T\bf{X}$$
$${\bf \hat \beta_Y} = (\bf{G_Y}^T\bf{G_Y})^{-1}\bf{G_Y}^T\bf{Y}$$

It can be easily shown that
$N_X({\bf G_X}^T{\bf G_X})^{-1} \sim N_Y(\bf{G_Y}^T\bf{G_Y})^{-1}$ as:
$$({\bf G_X}^T{\bf G_X})^{-1} \sim \frac{1}{N_X} \begin{pmatrix} 1 & 2\text{maf}_i \\  2\text{maf}_i &  4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)  \end{pmatrix} ^{-1} \text{and } ({\bf G_Y}^T{\bf G_Y})^{-1} \sim \frac{1}{N_Y} \begin{pmatrix} 1 & 2\text{maf}_i \\  2\text{maf}_i &  4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)  \end{pmatrix} ^{-1}.$$
Therefore, letting ${\bf C} \sim N_X(\bf{G_X}^T\bf{G_X})^{-1}$ and
${\bf C} \sim N_Y(\bf{G_Y}^T\bf{G_Y})^{-1}$, it follows that:
$${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \textbf{cov}\left(\frac{\bf{C}}{N_X}{\bf G_X^T}{\bf X}, \frac{\bf C}{N_Y}{\bf G_Y^T}{\bf Y}\right) = \frac{1}{N_X N_Y} \textbf{cov}\left({\bf C} {\bf G}_{\text{overlap}}^T {\bf X}_{\text{overlap}}, {\bf C} {\bf G}_{\text{overlap}}^T {\bf Y}_{\text{overlap}}\right)$$
giving
$${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \frac{\bf C {\bf G}_{\text{overlap}}^T \text{cov}( {\bf X}_{\text{overlap}},  {\bf Y}_{\text{overlap}}) {\bf G}_{\text{overlap} }{\bf C}^T }{N_X N_Y}$$

Here, ${\bf X}_{\text{overlap}}$ and ${\bf Y}_{\text{overlap}}$ denote
the first $N_{\text{overlap}}$ elements of $\bf X$ and $\bf Y$. Noting
that
$\textbf{cov}( {\bf X}_{\text{overlap}}, {\bf Y}_{\text{overlap}}) = \text{cov}(X,Y) {\bf I}_{N_\text{overlap}}$,
where ${\bf I}_{N_\text{overlap}}$ is the
${N_\text{overlap}} \times {N_\text{overlap}}$ identity matrix,
${\bf G_{\text{overlap}}}^T{\bf G_{\text{overlap}}} \sim N_\text{overlap}{\bf C}^{-1}$
and ${\bf C} = {\bf C}^T$, it follows that:
$${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \frac{N_{\text{overlap}}\text{cov}(X,Y)}{N_XN_Y} {\bf C}$$

As stated above, we are interested in
$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$ which is the bottom right
entry of the matrix $\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)$. As the
bottom right entry of $\bf C$ is
$\frac{1}{2\text{maf}_i(1-\text{maf}_i)}$ and letting
$\text{cov}(X,Y) = \text{cor}(X,Y) \sqrt{\text{var}(X)\text{var}(Y)} = \rho$
in which $\text{cor}(X,Y) = \rho$ and
$\text{var}(X) = \text{var}(Y) = 1$, we have:
$$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1}) \sim \frac{N_{\text{overlap}}\rho}{N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

Therefore, putting all of the theoretical results together, we have the
following expressions for ${\bf \mu_X}$ and ${\bf \sum_X}$:

ii) ${\bf \mu_X} = E \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} = \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix}$

iii) ${\bf \sum_X} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right) & \text{var}\left(\hat \beta_{Y1}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)} \begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

<br>

[$\star \; E(\hat\beta_{X1_\pi})$, $\text{var}(\hat\beta_{X1_\pi})$ and
$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1})$:]{style="color: purple;"}

Now, suppose we split the full data set into two fractions $\pi$ and
$1-\pi$. Thus, in the first fraction (sub sample), we would have
***approximately*** $\pi N_X$ individuals who have an exposure
measurement, $\pi N_Y$ who have had their outcome measured and
$\pi N_{\text{overlap}}$ individuals who have values available for both
exposure and outcome. We can write the genotype matrix of those that
have had their exposure measured in this first sub sample, denoted by
$G_{X\pi}$, as:
$$\bf G_{X\pi} = \begin{pmatrix} \bf G_{\text{overlap}{\pi}} \\ \bf G_{X_1\pi} \end{pmatrix}$$

Based on this genotype matrix, and denoting the $(\pi N_X) \times 1$
exposure vector of this sub sample by $\bf X_{\pi}$, the estimated
regression coefficient vector corresponding to this SNP-exposure
regression in the sub sample is:
$${\bf \hat \beta_{X_\pi}} = ({\bf G_{X\pi}^T G_{X\pi}})^{-1}{\bf G_{X\pi}^TX_\pi} \sim \frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_\pi}$$

We saw above that
$\text{var}\left(\hat \beta_{X1}\right) \sim \frac{1}{N_X \cdot 2\text{maf}_i(1-\text{maf}_i)}$,
and thus, it is clear to see then that we must have
$\text{var}\left(\hat \beta_{X1_{\pi}}\right) \sim \frac{1}{\pi N_X \cdot 2\text{maf}_i(1-\text{maf}_i)}$.

Now, let us consider
$\textbf{cov}\left(\bf \hat\beta_{X_\pi}, \hat\beta_X\right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_\pi}, \frac{\bf C}{N_X}{\bf G_{X}^TX}\right)$.
As $\bf G_{X\pi}$ and $\bf X_\pi$ are clearly subsets of $\bf G_X$ and
$\bf X$, respectively, then we get:
$$\textbf{cov}\left({\bf \hat\beta_{X_\pi}, \hat\beta_{X}} \right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_{\pi}}, \frac{\bf C}{N_X}{\bf G_{X\pi}^TX_{\pi}} \right) = \frac{\bf CG_{X \pi}^T \text{var}(\bf X_\pi){\bf G_{X \pi}C^T}}{\pi N_X \cdot N_X}$$
This expression can be simplified in a similar manner to previously,
noting that ${\bf G_{X\pi}^TG_{X\pi}} \sim \pi N_X \cdot {\bf C}^{-1}$:
$$\textbf{cov}({\bf\hat\beta_{X_\pi}, \hat\beta_X}) \sim \frac{\pi N_X \cdot  {\bf C}}{\pi N_X \cdot N_X} = \frac{\bf C}{N_X}$$
We then obtain:
$$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1}) \sim \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$$
In addition, from this set-up, we can easily state that
$E(\hat\beta_{X1_\pi}) = \beta_{X1}$.

<br>

[$\star \; E(\hat\beta_{Y1_\pi})$, $\text{var}(\hat\beta_{Y1_\pi})$ and
$\text{cov}(\hat\beta_{Y1_\pi}, \hat\beta_{Y1})$:]{style="color: purple;"}

Now, due to the way we have partitioned the original data set, we can
write the genotype matrix for the outcome-SNP regression in the first
sub sample, denoted by $\bf G_{Y\pi}$, in the form:
$$\bf G_{Y\pi} = \begin{pmatrix}  \bf G_{\text{overlap}{\pi}} \\ \bf G_{Y_1\pi} \end{pmatrix} $$

Similar to when we considered the full data set, $\bf G_{Y\pi}$ and
$\bf G_{X\pi}$ share their first $\pi \times N_{\text{overlap}}$ rows,
represented by the matrix $\bf G_{\text{overlap}\pi}$. This matrix,
$\bf G_{\text{overlap}\pi}$, contains the genotypes of the individuals
who are contained in the first sub sample and have had both their
outcome and exposure values measured. The estimated regression
coefficient vector corresponding to the SNP-outcome regression in the
sub sample is:
$${\bf \hat \beta_{Y_\pi}} = ({\bf G_{Y\pi}^T G_{Y\pi}})^{-1}{\bf G_{Y\pi}^TY_\pi} \sim \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_\pi}$$
Therefore, following the same process as we used above in order to
obtain $\text{var}\left(\hat \beta_{X1_{\pi}}\right)$ and
$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1})$, it is easy to see that
we should obtain
$\text{var}\left(\hat \beta_{Y1_{\pi}}\right) \sim \frac{1}{\pi N_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$
and
$\text{cov}(\hat\beta_{Y1_\pi}, \hat\beta_{Y1}) \sim \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$.
Also, we get $E(\hat\beta_{Y1_\pi}) = \beta_{Y1}$.

<br>

[$\star \; \text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi})$:]{style="color: purple;"}

We next consider $\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi})$.
We saw that $\bf G_{X\pi}$ can be partitioned as
$\bf G_{X\pi} = \begin{pmatrix} \bf G_{\text{overlap}{\pi}} \\ \bf G_{X_1\pi} \end{pmatrix}$
and similarly, $\bf G_{Y\pi}$ can be partitioned as
$\bf G_{Y\pi} = \begin{pmatrix} \bf G_{\text{overlap}{\pi}} \\ \bf G_{Y_1\pi} \end{pmatrix}$.
Therefore, we get:
$$\textbf{cov}\left({\bf\hat\beta_{X_\pi}, \hat\beta_{Y_\pi} }\right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_{\pi}}, \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_{\pi}} \right) = \frac{\bf CG_{overlap \pi}^T \text{cov}(\bf X_{overlap\pi}, Y_{overlap\pi}){\bf G_{overlap \pi}C^T}}{\pi N_X \cdot \pi N_Y}$$
This simplifies, giving:
$$\textbf{cov}({\bf \hat\beta_{X_\pi}, \hat\beta_{Y_\pi}}) \sim \frac{\pi N_{\text{overlap}}\text{cov}(X,Y)}{\pi N_X \cdot \pi N_Y} {\bf C}$$
and subsequently:
$$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi}) \sim \frac{N_{\text{overlap}}\rho}{\pi N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

[$\star \; \text{cov}(\hat\beta_{X1}, \hat\beta_{Y1_\pi})$ and
$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1})$:]{style="color: purple;"}

Now, let us find $\textbf{cov}({\bf\hat\beta_{X}, \hat\beta_{Y_\pi}})$
and $\textbf{cov}({\bf\hat\beta_{X_\pi}, \hat\beta_{Y}})$. Using our
expressions for $\bf \hat\beta_X$ and $\bf \hat\beta_{Y_\pi}$ detailed
above and knowing that the genotype matrices $\bf G_{X}$ and
$\bf G_{Y_\pi}$ share genotype information of $\pi N_{\text{overlap}}$
individuals which is contained in $G_{\text{overlap}\pi}$, we get:
$$\textbf{cov}\left({\bf \hat\beta_{X}, \hat\beta_{Y_\pi}}\right) \sim \textbf{cov}\left(\frac{\bf C}{N_X}{\bf G_{X}^TX}, \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_{\pi}} \right) = \frac{\bf CG_{overlap \pi}^T \text{cov}(\bf X_{overlap\pi}, Y_{overlap\pi}){\bf G_{overlap \pi}C^T}}{ N_X \cdot \pi N_Y}$$

It follows that:
$$\textbf{cov}({\bf\hat\beta_{X}, \hat\beta_{Y_\pi}}) \sim \frac{ \pi N_{\text{overlap}}\text{cov}(X,Y)}{N_X \cdot \pi N_Y} {\bf C}$$
and this gives:
$$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1_\pi}) \sim \frac{N_{\text{overlap}}\rho}{ N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

In a very similar fashion, using our expressions for
$\bf \hat\beta_{X_\pi}$ and $\bf \hat\beta_{Y}$ detailed above, we get:
$$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1}) \sim \frac{N_{\text{overlap}}\rho}{ N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

Therefore, we finally have the following results for $\bf \mu_Y$,
${\bf \sum_{XY}}$, ${\bf \sum_{YX}}$ and ${\bf \sum_{Y}}$:

i)  ${\bf \mu_Y} = E \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} = \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix}$

ii) ${\bf \sum_{XY}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1}, \hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{Y1}, \hat\beta_{X1_\pi}\right) & \text{cov}\left(\hat\beta_{Y1}, \hat \beta_{Y1_\pi}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

iii) ${\bf \sum_{YX}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1_\pi}, \hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{Y1_\pi}, \hat\beta_{X1}\right) & \text{cov}\left(\hat\beta_{Y1_\pi}, \hat \beta_{Y1}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

iv) ${\bf \sum_{Y}} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right) & \text{var}\left(\hat \beta_{Y1_\pi}\right) \end{pmatrix} = \frac{1}{\pi} \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

Therefore, it is clear to see that
${\bf \sum_{X}} = {\bf \sum_{XY}} = {\bf \sum_{YX}}$ and
$\pi \cdot {\bf \sum_{Y}} = {\bf \sum_{X}}$.

Then, using the identities, we can establish
$E\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right)$
and
$\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right)$
as follows:

-   $E\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right) = \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix} + \bf \sum_{X} \bf \left(\sum_X\right)^{-1} \left( \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} - \begin{pmatrix} \beta_{X1} \\ \beta_{Y1} \end{pmatrix} \right) = \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix}$

-   $\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right) = \frac{1}{\pi} {\bf \sum_{X}} - {\bf \sum_{X}} \left( {\bf \sum_{X}} \right)^{-1}{\bf \sum_{X}} = \left( \frac{1-\pi}{\pi}\right) {\bf \sum_{X}} = \left( \frac{1-\pi}{\pi}\right) \cdot \textbf{var}\left( \begin{pmatrix} \hat\beta_{X1} \\ \hat\beta_{Y1} \end{pmatrix} \right)$

Therefore, we finally have obtained the required conditional
distribution:

$$ \left( \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} \right) \sim N\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}, \left( \frac{1-\pi}{\pi}\right) \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}\right)$$

<br>

***Note:*** The above expression holds in the case that
$\text{var}(X) = \text{var}(Y) = 1$, as stated above. Let us briefly
consider how we could rewrite this expression if it is ***not*** the
case that $\text{var}(X) = \text{var}(Y) = 1$. Firstly, our expression
would now have to include $\text{var}(X)$ and $\text{var(Y)}$ as
follows:

$$ \left( \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} \right) \sim N\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}, \left( \frac{1-\pi}{\pi}\right) \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{\text{var}(X)}{N_X} & \frac{N_{\text{overlap}}\rho\sqrt{\text{var}(X)\text{var}(Y)}}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho\sqrt{\text{var}(X)\text{var}(Y)}}{N_XN_Y} & \frac{\text{var}(Y)}{N_Y} \end{pmatrix}\right)$$

However, it is possible to write this in another form in which we make
use of the fact that we already have estimates for
$\text{se}\left(\hat \beta_{X1}\right)$ and
$\text{se}\left(\hat \beta_{Y1}\right)$. We note the following:

i.  As
    $\text{var}(\hat \beta_{X1}) \sim \frac{\text{var}(X)}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$,
    then
    $\left(\text{se}(\hat \beta_{X1})\right)^2 \sim \frac{\text{var}(X)}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$.

ii. As
    $\text{var}(\hat \beta_{Y1}) \sim \frac{\text{var}(Y)}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}$,
    then
    $\left(\text{se}(\hat \beta_{Y1})\right)^2 \sim \frac{\text{var}(Y)}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}$.

iii. $\text{se}\left(\hat \beta_{X1}\right) \cdot \text{se}\left(\hat \beta_{Y1}\right) = \sqrt{\frac{\text{var}(X)}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}}\cdot \sqrt{\frac{\text{var}(Y)}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)} \frac{\sqrt{\text{var}(X)\text{var}(Y)}}{\sqrt{N_X N_Y}}$

Putting all of the above together, we obtain an alternative form of the
required conditional distribution:

$$ \left( \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} \right) \sim N\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}, \left( \frac{1-\pi}{\pi}\right) \begin{pmatrix} \left(\text{se}\left(\hat \beta_{X1}\right)\right)^2 & \text{se}\left(\hat \beta_{X1}\right) \text{se}\left(\hat \beta_{Y1}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}\\ \text{se}\left(\hat \beta_{X1}\right) \text{se}\left(\hat \beta_{Y1}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}} & \left(\text{se}\left(\hat \beta_{Y1}\right)\right)^2 \end{pmatrix}\right)$$

<br><br>

### Estimating the Causal Effect 

For a given data set, we can
easily simulate $\hat\beta_{X1_\pi}$ and $\hat\beta_{Y1_\pi}$ for each SNP using the above conditional distribution and our known values of $\hat\beta_{X1}$ and $\hat\beta_{Y1}$. We
then obtain $\widehat{\text{se}(\hat\beta_{X1_\pi})}$ by approximating
it with
$\sqrt{\text{var}(\hat\beta_{X1_\pi})} \sim \sqrt{\frac{1}{\pi}\cdot (\text{se}(\hat\beta_{X1}))^2}$,
and subsequently, obtain the corresponding $p$-value for
$\hat\beta_{X1_\pi}$ for each SNP. Any SNPs who have a $p$-value less
than $5 \times 10^{-8}$, or another threshold of choice, are then **selected**.

Then, as mentioned in the opening paragraph of this document, in order to avoid *Winner's Curse*, we obtain the estimates
corresponding to the other $1-\pi$ fraction of the data set, i.e.
$\hat\beta_{X1_{1-\pi}}$, $\widehat{\text{se}(\hat\beta_{X1_{1-\pi}})}$,
$\hat\beta_{Y1_{1-\pi}}$ and
$\widehat{\text{se}(\hat\beta_{Y1_{1-\pi}})}$, using:

-   $\hat\beta_{X1_{1-\pi}} = \frac{\hat\beta_{X1} - \pi\hat\beta_{X1_\pi}}{1-\pi}$

-   $\hat\beta_{Y1_{1-\pi}} = \frac{\hat\beta_{Y1} - \pi\hat\beta_{Y1_\pi}}{1-\pi}$

-   $\widehat{\text{se}(\hat\beta_{X1_{1-\pi}})} \sim \sqrt{\frac{1}{1-\pi}\cdot (\text{se}(\hat\beta_{X1}))^2}$

-   $\widehat{\text{se}(\hat\beta_{Y1_{1-\pi}})} \sim \sqrt{\frac{1}{1-\pi}\cdot (\text{se}(\hat\beta_{Y1}))^2}$

The MR method of choice can then be applied to these estimates to obtain an estimate for the causal effect. For example, $\hat\beta_{X1_{1-\pi}}$, $\widehat{\text{se}(\hat\beta_{X1_{1-\pi}})}$,
$\hat\beta_{Y1_{1-\pi}}$ and
$\widehat{\text{se}(\hat\beta_{Y1_{1-\pi}})}$, can be inputted into the `mr` function from the R
package `TwoSampleMR`, with a specified MR method, such as `"mr_ivw"` or `"mr_raps"`. The idea is that our method will
incorporate **several iterations** of this described process and then obtain
an **average** of all results in order to provide an estimate for
$\hat\beta$, the causal effect of the exposure on the outcome, which
does not suffer from *Winner's Curse* bias.

[$\star$]{style="color: blue;"} **Note:** If we let $\hat\beta^{(i)}$ be the estimate for the causal
effect obtained at iteration $i$, then our final estimate for the **causal
effect** of the exposure on the outcome is denoted by
$\overline{\hat\beta} = \sum_{i=1}^{N_{\text{iter}}} \hat\beta^{(i)}$, the average of the estimates
obtained at each iteration.

Our next task is to obtain a suitable value for
$\text{se}\left( \overline{\hat\beta} \right)$, the **standard error** of
our causal effect estimate. In order to do so, let us consider
$\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right)^2$
which can be easily computed from our set of results. Now,

$$\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right)^2 \approx E\left[ \sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right) ^2 \right] = \sum_{i=1}^{N_{\text{iter}}} \left[E\left( \left(\hat\beta^{(i)} \right)^2 \right)\right] - N_{\text{iter}}\cdot E\left( \left( \overline{\hat\beta}\right)^2 \right)$$
Using the well-known identity $E(X^2) = \text{var}(X) + (E(X))^2$, we
get:
$$\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right) \approx \sum_{i=1}^{N_{\text{iter}}} \left[ \text{var}\left(\hat\beta^{(i)}\right) + \left(E\left(\hat\beta^{(i)}\right)\right)^2 \right] - N_{\text{iter}}\cdot \left[ \text{var}\left(\overline{\hat\beta}\right) + \left(E\left(\overline{\hat\beta}\right)\right)^2\right]$$

As$E\left( \hat\beta^{(i)} \right) = E\left( \overline{\hat\beta} \right)$
for $i=1,...,N_{\text{iter}}$, we have:
$$\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right) \approx \sum_{i=1}^{N_{\text{iter}}} \left[ \text{var}\left(\hat\beta^{(i)}\right)\right] - N_{\text{iter}}\cdot \left[ \text{var}\left(\overline{\hat\beta}\right)\right]$$
This finally gives us an expression for
$\text{se}\left( \overline{\hat\beta} \right)$ as follows:

$$\text{se}\left( \overline{\hat\beta} \right) = \sqrt{\frac{\sum_{i=1}^{N_{\text{iter}}} \left[ \left( \text{se}\left(\hat\beta^{(i)}\right) \right) ^2\right] - \sum_{i=1}^{N_{\text{iter}}}\left[ \hat\beta^{(i)} - \overline{\hat\beta} \right]}{N_{\text{iter}}}}.$$

<br><br>

### Estimating Correlation

Ideally, we would like to be able to use our method when only summary
statistics are available, i.e. we only have information on
$\hat\beta_{X_i}$, $\hat\beta_{Y_i}$, $\text{se}(\hat\beta_{X_i})$ and
$\text{se}(\hat\beta_{Y_i})$ for each SNP $i$. Therefore, in the
equation above, we would need to estimate a value for
$\frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}$.

[$\star$]{style="color: blue;"} **Note:** From here, we will denote
$\hat\beta_{X1}$ for SNP $i$ simply by $\hat\beta_{X_i}$ and similarly,
$\hat\beta_{Y1}$ for SNP $i$ will be represented by $\hat\beta_{Y_i}$.

In a similar form to the expression above, we assume for each SNP $i$
that $\hat\beta_{X_i}$ and $\hat\beta_{Y_i}$ follow the following
bivariate normal distribution:

$$  \begin{pmatrix} \hat \beta_{X_i} \\ \hat \beta_{Y_i} \end{pmatrix} \sim N\left( \begin{pmatrix} \beta_{X_i}  \\ \beta_{Y_i} \end{pmatrix}, \begin{pmatrix} \left(\text{se}\left(\hat \beta_{X_i}\right)\right)^2 & \text{se}\left(\hat \beta_{X_i}\right) \text{se}\left(\hat \beta_{Y_i}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}\\ \text{se}\left(\hat \beta_{X_i}\right) \text{se}\left(\hat \beta_{Y_i}\right) \frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}} & \left(\text{se}\left(\hat \beta_{Y_i}\right)\right)^2 \end{pmatrix}\right)$$

For convenience, let us represent
$\frac{N_{\text{overlap}}\rho}{\sqrt{N_XN_Y}}$ by $\lambda$ as this is
the value we are interested in estimating. Thus $\lambda$ is essentially
equivalent to the correlation between $\hat\beta_{X_i}$ and
$\hat\beta_{Y_i}$ for each SNP $i$. In addition, we let
$\text{se}(\hat\beta_{X_i}) \approx \sigma_{X_i}$ and
$\text{se}(\hat\beta_{Y_i}) \approx \sigma_{Y_i}$. The bivariate normal
distribution then takes the simpler form of:

$$  \begin{pmatrix} \hat \beta_{X_i} \\ \hat \beta_{Y_i} \end{pmatrix} \sim N\left( \begin{pmatrix} \beta_{X_i}  \\ \beta_{Y_i} \end{pmatrix}, \begin{pmatrix} \left(\sigma_{X_i}\right)^2 & \sigma_{X_i} \sigma_{Y_i} \lambda \\ \sigma_{X_i} \sigma_{Y_i} \lambda & \left(\sigma_{Y_i}\right)^2 \end{pmatrix}\right)$$

Given this, the corresponding ***probability density function*** for
each SNP $i$ can be written as:

$$f\left( \hat \beta_{X_i}, \hat \beta_{Y_i} \right) = \frac{1}{2\pi\sqrt{1-\lambda^2}\sigma_{X_i}\sigma_{Y_i}}e^{ -\frac{1}{2(1-\lambda^2)} \left[ \left( \frac{\hat \beta_{X_i} - \beta_{X_i}}{\sigma_{X_i}}\right)^2 - 2\lambda\left(\frac{\hat \beta_{X_i} - \beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat \beta_{Y_i} - \beta_{Y_i}}{\sigma_{Y_i}}\right) + \left( \frac{\hat \beta_{Y_i} - \beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]}$$

Thus, we can form a likelihood function for $\lambda$ for a set of $n$
SNPs,
$\left(\hat\beta_{X_i}, \hat\beta_{Y_i}, \sigma_{X_i}, \sigma_{Y_i}\right)$,
$i=1,...,n$:

$$
\begin{aligned}
L\left( \lambda | \bf{\hat\beta_X, \hat\beta_Y, \sigma_X, \sigma_Y} \right) &= \Pi_{i=1}^{n}f\left( \hat \beta_{X_i}, \hat \beta_{Y_i} \right) \\ &= \left( \frac{1}{2\pi\sqrt{1-\lambda^2}} \right)^n \cdot\Pi_{i=1}^{n}\left(\frac{1}{\sigma_{X_i}\sigma_{Y_i}}\right)\cdot e^{-\frac{1}{2(1-\lambda^2)}\left[ \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)^2 -2\lambda\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] + \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]}
\end{aligned}
$$

where
$\hat\beta_X = (\hat\beta_{X_1},...,\hat\beta_{X_n}), \hat\beta_Y = (\hat\beta_{Y_1},...,\hat\beta_{Y_n}), \sigma_X = (\sigma_{X_1},...,\sigma_{X_n})$
and $\sigma_Y = (\sigma_{Y_1},...,\sigma_{Y_n})$, the observed values
for each SNP.

The ***log-likelihood function*** for $\lambda$,
$\text{log}L(\lambda) = l(\lambda)$ can then be derived as follows:

$$
l\left( \lambda \right) = -n \text{log}(2\pi) - \frac{n}{2}\text{log}(1-\lambda^2) - \text{log}(\Pi_{i=1}^{n}\left(\sigma_{X_i}\sigma_{Y_i}\right)) - \frac{1}{2(1-\lambda^2)}\left[ \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)^2 -2\lambda\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] + \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]
$$

[$\star$]{style="color: blue;"} **Assumption:** If assume that
$\beta_{X_i} = 0$ and $\beta_{Y_i} = 0$ for $i=1,...,n$, we can then
denote the following constants by letters to simplify our derivations:

-   $\sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)^2 = \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)^2 = A$
-   $\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}-\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] = \sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] = B$
-   $\sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}-\beta_{Y_i}}{\sigma_{Y_i}}\right)^2 = \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)^2 = C$

In order to find the ***maximum likelihood estimate*** (MLE) for
$\lambda$, we must first obtain an expression for
$\frac{d}{d\lambda}l(\lambda)$ and then solve equal to zero. Using our
defined constants $A$, $B$ and $C$ above, we proceed as follows:

$$
\begin{aligned}
\frac{d}{d\lambda}l\left( \lambda \right) &= 
\frac{d}{d\lambda}\left[ -n \text{log}(2\pi) - \frac{n}{2}\text{log}(1-\lambda^2) - \text{log}\left(\Pi_{i=1}^{n}\left(\sigma_{X_i}\sigma_{Y_i}\right)\right) - \frac{1}{2(1-\lambda^2)}\left(A - 2\lambda B + C\right)\right] \\ &=  - \frac{n}{2}\frac{d}{d\lambda}\left[\text{log}(1-\lambda^2) \right] - \frac{1}{2}\frac{d}{d\lambda}\left[ \frac{A}{1-\lambda^2} - \frac{2\lambda B}{1-\lambda^2} +\frac{C}{1-\lambda^2}\right] \\ &= -\frac{n}{2}\cdot\left[-\frac{2\lambda}{1-\lambda^2}\right] - \frac{1}{2}\cdot\left[ \frac{2\lambda A}{\left(1-\lambda^2\right)^2} - \frac{2\left(1+\lambda^2\right)B}{\left(1-\lambda^2\right)^2} +  \frac{2\lambda C}{\left(1-\lambda^2\right)^2}\right] \\ &= \frac{n\lambda}{\left(1-\lambda^2\right)^2}\cdot \left[ \left(1-\lambda^2\right) - \frac{A}{n} + \frac{1-\lambda^2}{\lambda}\frac{B}{n} - \frac{C}{n}\right] \\ &= \frac{n\lambda}{\left(1-\lambda^2\right)^2}\cdot \left[ \left(1-\lambda^2\right) - \tilde{A} + \frac{1-\lambda^2}{\lambda}\tilde{B} - \tilde{C}\right]
\end{aligned}
$$

in which we have let $\tilde{A} = \frac{A}{n}, \tilde{B} = \frac{B}{n}$
and $\tilde{C} = \frac{C}{n}$.

Solving the above equation equal to zero gives the following:

$$
\begin{aligned}
\frac{d}{d\lambda}l\left( \lambda \right) = 0 &\implies \frac{n\lambda}{\left(1-\lambda^2\right)^2}\cdot \left[ \left(1-\lambda^2\right) - \tilde{A} + \frac{1-\lambda^2}{\lambda}\tilde{B} - \tilde{C}\right] = 0 \\ &\implies \left[ \left(1-\lambda^2\right) - \tilde{A} + \frac{1-\lambda^2}{\lambda}\tilde{B} - \tilde{C}\right] = 0 \\ &\implies \tilde{A} - \frac{1-\lambda^2}{\lambda}\tilde{B} + \tilde{C} = 1 - \lambda^2 \\ &\implies \lambda^3 - \tilde{B}\lambda^2 + \left(\tilde{A} + \tilde{C} - 1\right)\lambda - \tilde{B} = 0
\end{aligned}
$$

[$\star$]{style="color: blue;"} **Cubic equation for** $\lambda$?

In the following example, we will use our work above to obtain an
estimate for $\lambda$ using simulated data. First, we establish a
function to simulate data, `sim_mr_stats`.

```{r}
sim_mr_stats <- function(n_snps, prop_effect, h2, frac_overlap, n_x, n_y, cor_xy, beta_xy){
  n_overlap <- frac_overlap*min(n_x, n_y)
  maf <- runif(n_snps, 0.01, 0.5)
  effect_snps <- n_snps*prop_effect
  index <- sample(1:n_snps, ceiling(effect_snps), replace=FALSE) # random sampling
  beta_gx <- rep(0,n_snps)
  beta_gx[index] <- rnorm(length(index),0,1)
  var_x <- sum(2*maf*(1-maf)*beta_gx^2)/h2
  if(var_x != 0){beta_gx <- beta_gx/sqrt(var_x)} # scaling to represent an exposure with variance 1
  beta_gy <- beta_gx * beta_xy

  var_gx <- 1/(n_x*2*maf*(1-maf)) # var(X)=1
  var_gy <- 1/(n_y*2*maf*(1-maf)) # var(Y)=1
  cov_gx_gy <- ((n_overlap*cor_xy)/(n_x*n_y))*(1/(2*maf*(1-maf)))
  # create covariance matrix for each SNP
  cov_array <- array(dim=c(2, 2, n_snps))
  cov_array[1,1,] <- var_gx
  cov_array[2,1,] <- cov_gx_gy
  cov_array[1,2,] <- cov_array[2,1,]
  cov_array[2,2,] <- var_gy

  summary_stats <- apply(cov_array, 3, function(x){MASS::mvrnorm(n=1, mu=c(0,0), Sigma=x)})
  summary_stats <- t(summary_stats + rbind(beta_gx, beta_gy))

  data <- tibble(
    SNP = 1:n_snps,
    beta.exposure = summary_stats[,1],
    beta.outcome = summary_stats[,2],
    se.exposure = sqrt(var_gx),
    se.outcome = sqrt(var_gy),
    true.exposure = beta_gx,
    true.outcome = beta_gy
  )
  return(data)
}

```

We now use the function, `mr_sim_stats`, to simulate sets of summary
statistics with different proportions of effect SNPs. We anticipate that
solving the above cubic equation for $\lambda$ or alternatively,
optimising the ***log-likelihood function*** for $\lambda$, will become
less accurate at estimating $\lambda$ as the proportion of effect SNPs
increases. This is due to the increased violation of our **assumption**
that $\beta_{X_i} = 0$ and $\beta_{Y_i} = 0$ for $i=1,...,n$. Note that
as the roots of the cubic equation can be both real and imaginary, we
will focus on using the function `optimise` to optimise the
***log-likelihood function*** rather than solving the cubic equation. In
all examples below, the fraction of overlap is 1 between the exposure
and outcome samples and therefore, the true value for $\lambda$ is equal
to the correlation between the exposure, $X$ and outcome, $Y$ which has
been set to 0.6.

```{r}
set.seed(1998)
## prop_effect = 0
data <- sim_mr_stats(10^5,0,0.4,1,50000,50000,0.6,0.3) 
n <- nrow(data)
B <- sum((data$beta.exposure/data$se.exposure)*(data$beta.outcome/data$se.outcome))
A <- sum((data$beta.exposure/data$se.exposure)^2) 
C <- sum((data$beta.outcome/data$se.outcome)^2)
f <-function(x){-(n/2)*(log(1-x^2)) - ((1)/(2*(1-x^2)))*(A-2*x*B+C)}
optimize(f,interval=c(-1,1),maximum=TRUE)$maximum

set.seed(1998)
## prop_effect = 0.01
data <- sim_mr_stats(10^5,0.01,0.4,1,50000,50000,0.6,0.3) 
n <- nrow(data)
B <- sum((data$beta.exposure/data$se.exposure)*(data$beta.outcome/data$se.outcome))
A <- sum((data$beta.exposure/data$se.exposure)^2) 
C <- sum((data$beta.outcome/data$se.outcome)^2)
f <-function(x){-(n/2)*(log(1-x^2)) - ((1)/(2*(1-x^2)))*(A-2*x*B+C)}
optimize(f,interval=c(-1,1),maximum=TRUE)$maximum

set.seed(1998)
## prop_effect = 0.1
data <- sim_mr_stats(10^5,0.1,0.4,1,50000,50000,0.6,0.3) 
n <- nrow(data)
B <- sum((data$beta.exposure/data$se.exposure)*(data$beta.outcome/data$se.outcome))
A <- sum((data$beta.exposure/data$se.exposure)^2) 
C <- sum((data$beta.outcome/data$se.outcome)^2)
## optimise log-likelihood
f <-function(x){-(n/2)*(log(1-x^2)) - ((1)/(2*(1-x^2)))*(A-2*x*B+C)}
optimize(f,interval=c(-1,1),maximum=TRUE)$maximum

set.seed(1998)
## prop_effect = 0.2
data <- sim_mr_stats(10^5,0.2,0.4,1,50000,50000,0.6,0.3) 
n <- nrow(data)
B <- sum((data$beta.exposure/data$se.exposure)*(data$beta.outcome/data$se.outcome))
A <- sum((data$beta.exposure/data$se.exposure)^2) 
C <- sum((data$beta.outcome/data$se.outcome)^2)
## optimise log-likelihood
f <-function(x){-(n/2)*(log(1-x^2)) - ((1)/(2*(1-x^2)))*(A-2*x*B+C)}
optimize(f,interval=c(-1,1),maximum=TRUE)$maximum
```

It is clear to see that, as anticipated, as more non-null effect SNPs
are included in our calculations, bias is introduced into our estimate
of $\lambda$. Therefore, in order to counteract this, we must select
SNPs based on their $z$-values and perform our calculations with just
these SNPs. We repeat one of the examples above but just with those SNPs
in which their absolute $z$-values for both exposure and outcome are
less than 0.5. It can be seen below that the correlation estimate is now
extremely upward biased, indicating that our log-likelihood expression
must then take into account this selection process.

```{r}
## prop_effect = 0.1
set.seed(1998)
data <- sim_mr_stats(10^5,0.1,0.4,1,50000,50000,0.6,0.3) 
data <- data[abs(data$beta.exposure/data$se.exposure) < 0.5 & abs(data$beta.outcome/data$se.outcome) < 0.5,]
n <- nrow(data)
B <- sum((data$beta.exposure/data$se.exposure)*(data$beta.outcome/data$se.outcome))
A <- sum((data$beta.exposure/data$se.exposure)^2) 
C <- sum((data$beta.outcome/data$se.outcome)^2)
## optimise log-likelihood
f <-function(x){-(n/2)*(log(1-x^2)) - ((1)/(2*(1-x^2)))*(A-2*x*B+C)}
optimize(f,interval=c(-1,1),maximum=TRUE)$maximum
```

[$\star$]{style="color: blue;"} **Question of interest:** Why has the
value here increased towards 1, rather than decreased as we might have
expected by restricting the region?

Therefore, we must now work on establishing this new ***conditional
log-likelihood*** expression. As we have chosen to select SNPs with
absolute $z$-scores for both exposure and outcome that are less than
0.5, we first consider obtaining an expression for the probability of
each SNP having absolute $z$-scores for both exposure and outcome less
than 0.5. Continuing to assume that $\beta_{X_i}$ and $\beta_{Y_i}$ are
both equal to 0 and $\sigma_{X_i}$ and $\sigma_{Y_i}$ are known, this
probability for a single SNP is given by the following cumulative
distribution function:

$$f\left( |z_{X_i}| < 0.5, |z_{Y_i}| < 0.5 \right) = \frac{1}{2\pi\sqrt{1-\lambda^2}\sigma_{X_i}\sigma_{Y_i}} \int^{0.5}_{-0.5} \int^{0.5}_{-0.5} e^{ -\frac{1}{2(1-\lambda^2)} \left[ \left( z_{X_i}\right)^2 - 2\lambda\left(z_{X_i}\right)\left(z_{Y_i}\right) + \left( z_{Y_i}\right)^2\right]} dz_{X_i} dz_{Y_i}$$

Using the above expression, we can then establish our desired
conditional likelihood, i.e. the likelihood of $\lambda$ given that only
SNPs with both absolute $z$-scores less than 0.5 have been selected. We
note the first two terms of the numerator and denominator cancel each
other out in the expression, leaving only the exponential terms.

$$
\begin{aligned}
L\left( \lambda \right) &= \frac{\Pi_{i=1}^{n}f\left( \hat \beta_{X_i}, \hat \beta_{Y_i} \right)}{\left[ f\left( |z_{X_i}| < 0.5, |z_{Y_i}| < 0.5 \right)\right]^n} \\ &=  \frac{e^{-\frac{1}{2(1-\lambda^2)}\left[ \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)^2 -2\lambda\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] + \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]}}{\left[ \int^{0.5}_{-0.5} \int^{0.5}_{-0.5} e^{ -\frac{1}{2(1-\lambda^2)} \left[ \left( z_{X_i}\right)^2 - 2\lambda\left(z_{X_i}\right)\left(z_{Y_i}\right) + \left( z_{Y_i}\right)^2\right]} dz_{X_i} dz_{Y_i}\right]^n}
\end{aligned}
$$

The ***conditional log-likelihood*** is then found to be:

$$
l\left( \lambda \right) = {-\frac{1}{2(1-\lambda^2)}\left[ \sum_{i=1}^{n}\left( \frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)^2 -2\lambda\sum_{i=1}^{n}\left[\left(\frac{\hat\beta_{X_i}}{\sigma_{X_i}}\right)\left(\frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)\right] + \sum_{i=1}^{n}\left( \frac{\hat\beta_{Y_i}}{\sigma_{Y_i}}\right)^2\right]}- n \cdot \log \left[ \int^{0.5}_{-0.5} \int^{0.5}_{-0.5} e^{ -\frac{1}{2(1-\lambda^2)} \left[ \left( z_{X_i}\right)^2 - 2\lambda\left(z_{X_i}\right)\left(z_{Y_i}\right) + \left( z_{Y_i}\right)^2\right]} dz_{X_i} dz_{Y_i}\right]
$$

We now simulate data in a similar manner to above and consider
estimating $\lambda$ by optimizing this conditional log-likelihood
function. The inclusion of the integral in this expression makes it much
more complex to work with than the previous function. However, we will
attempt to define the function and subsequently optimize it using the
`pracma` package, as shown below.

```{r}
library(pracma)
set.seed(1998)
data <- sim_mr_stats(10^5,0.1,0.4,1,50000,50000,0.6,0.3) 
data <- data[abs(data$beta.exposure/data$se.exposure) < 0.5 & abs(data$beta.outcome/data$se.outcome) < 0.5,]
n <- nrow(data)
B <- sum((data$beta.exposure/data$se.exposure)*(data$beta.outcome/data$se.outcome))
A <- sum((data$beta.exposure/data$se.exposure)^2) 
C <- sum((data$beta.outcome/data$se.outcome)^2)

fun_obj <- function(lambda){
 - ((1)/(2*(1-lambda^2)))*(A-2*lambda*B+C) - n*log((pracma::integral2(function(x,y) 
  exp((-(1)/(2*(1-lambda^2)))*(x^2-2*lambda*x*y+y^2)),
  xmin = -0.5,xmax = 0.5, ymin = -0.5, ymax = 0.5)$Q))
} 

optimize(fun_obj, interval=c(-1,1),maximum=TRUE)$maximum
```

We can see that our approach seems to have performed reasonably well
here. There is a small improvement in the estimate over using all the
SNPs while there is a considerable improvement compared to the instance
in which we selected SNPs and used the original non-conditional
expression.

#### Simulations

However, the above is only one example and thus, in order to test our
estimation approach properly, we will perform a brief simulation study.
The following code was run:

```{r, eval=FALSE}
library(parallel)
## Total number of simulations:
tot_sim <- 10
## Set of parameters:
sim_params <- expand.grid(
  sim = c(1:tot_sim),
  h2 = c(0.2,0.4,0.6),
  prop_effect = c(0.01,0.05,0.1),
  cor_xy = c(-0.1,0.1,0.3,0.5)
)

set.seed(1998)
run_sim <- function(h2,prop_effect,cor_xy,sim){
  data_full <- sim_mr_stats(n_snps=10^6, prop_effect, h2, frac_overlap=1, n_x=50000, n_y=50000, cor_xy, beta_xy=0.3)
  n <- nrow(data_full)
  B <- sum((data_full$beta.exposure/data_full$se.exposure)*(data_full$beta.outcome/data_full$se.outcome))
  A <- sum((data_full$beta.exposure/data_full$se.exposure)^2) 
  C <- sum((data_full$beta.outcome/data_full$se.outcome)^2)
  ## optimise log-likelihood
  f <-function(x){-(n/2)*(log(1-x^2)) - ((1)/(2*(1-x^2)))*(A-2*x*B+C)}
  est_cor_full <- optimize(f,interval=c(-1,1),maximum=TRUE)$maximum
  
  data <- data_full[abs(data_full$beta.exposure/data_full$se.exposure) < 0.5 & abs(data_full$beta.outcome/data_full$se.outcome) < 0.5,]
  n <- nrow(data)
  B <- sum((data$beta.exposure/data$se.exposure)*(data$beta.outcome/data$se.outcome))
  A <- sum((data$beta.exposure/data$se.exposure)^2)
  C <- sum((data$beta.outcome/data$se.outcome)^2)
  ## optimise conditional log-likelihood
  fun_obj <- function(lambda){
    - ((1)/(2*(1-lambda^2)))*(A-2*lambda*B+C) - n*log((pracma::integral2(function(x,y)
      exp((-(1)/(2*(1-lambda^2)))*(x^2-2*lambda*x*y+y^2)),
      xmin = -0.5,xmax = 0.5, ymin = -0.5, ymax = 0.5)$Q))
  }
  est_cor <- optimize(fun_obj, interval=c(-1,1),maximum=TRUE)$maximum
  
  results <- list(params = c(h2,prop_effect,cor_xy), est_cor=c(est_cor,est_cor_full), data = data_full)
  return(results)
}

res <- mclapply(1:nrow(sim_params), function(i){
  do.call(run_sim, args=as.list(sim_params[i,]))}, mc.cores=1)
```

```{r,include=FALSE}
res <- read.csv("cor_deriv_sims.csv")

res$sim_cor <- c(rep(c(1:90),4))
res_new <- rbind(res,res)
res_new$true_cor <- res_new$actual_cor
res_new$est_cor <- c(res$est_cor,res$est_cor_full)
res_new$fun <- c(rep("CL",360),rep("L",360))
res <- res_new
```

The number of SNPs is fixed at 1,000,000 and the sample size is fixed at
50,000 with full sample overlap. The causal effect of the exposure on
the outcome remains at **0.3** for each simulation. The heritability
takes values $h^2 \in \{0.2,0.4,0.6\}$, the proportion of effect SNPs
takes values $\pi \in \{0.01,0.05,0.1\}$ and the true correlation
between the exposure and outcome is varied as follows:
$\text{cor}_{XY} \in \{-0.1,0.1,0.3,0.5\}$. This factorial design study
is repeated **10** times. In this small set of simulations, we consider
two options for estimating the correlation:

i)  `fun: L`: optimizing the *log-likelihood* function with the full set
    of SNPs

ii) `fun: CL`: optimizing the *conditional log-likelihood* function with
    a selected subset of SNPs in which both absolute $z$-scores of every
    SNP in the subset are less than 0.5

The above results are organised in a dataframe and we attempt to
illustrate as follows:

```{r, fig.width=10}
library("RColorBrewer")
col <- brewer.pal(8,"GnBu")
ggplot(res, aes(x=sim_cor, y=est_cor,color=as.factor(h2),shape=as.factor(prop_effect))) + geom_point(size=2) + facet_grid(fun~true_cor, labeller="label_both")+ ylab("Estimated Correlation") + xlab("Simulation") + scale_color_manual(values=c(col[4],col[6],col[8])) +
  geom_hline(aes(yintercept = true_cor), linetype=1, size=0.5) + 
  guides(color = guide_legend(title = "Heritability"), shape= guide_legend(title = "Polygenicity")) 
```

[$\star$]{style="color: blue;"} **Note:** In the above grid, `true_cor`
represents the true correlation value. Furthermore, the bottom half of
the grid, `fun: L`, depicts the estimates obtained by optimizing the
*log-likelihood* function with the full set of SNPs while the top half,
`fun: CL`, depicts the estimated values computed by optimizing the
*conditional log-likelihood* function with a selected subset of SNPs in
which both absolute $z$-scores of every SNP in the subset are less than
0.5.

In addition to this illustration, we also compute the average estimated
correlation, `mean` as well as the root mean square error, `rmse`, for
each true correlation value.

```{r,include=FALSE}
res <-res_new[1:360,]
```

```{r}
rmse.L <- function(cor){sqrt(sum((res[res$true_cor==cor,]$est_cor_full - res[res$true_cor==cor,]$true_cor)^2)/nrow(res[res$true_cor==cor,]))}
rmse.CL <- function(cor){sqrt(sum((res[res$true_cor==cor,]$est_cor - res[res$true_cor==cor,]$true_cor)^2)/nrow(res[res$true_cor==cor,]))}
mean.L <- function(cor){sum(res[res$true_cor==cor,]$est_cor_full)/nrow(res[res$true_cor==cor,])}
mean.CL <- function(cor){sum(res[res$true_cor==cor,]$est_cor)/nrow(res[res$true_cor==cor,])}
rmse <- data.frame(true_cor = c(-0.1,0.1,0.3,0.5),
                   mean.L = c(mean.L(-0.1),mean.L(0.1),mean.L(0.3),mean.L(0.5)),
                   mean.CL = c(mean.CL(-0.1),mean.CL(0.1),mean.CL(0.3),mean.CL(0.5)),
                   rmse.L = c(rmse.L(-0.1),rmse.L(0.1),rmse.L(0.3),rmse.L(0.5)),
                   rmse.CL = c(rmse.CL(-0.1),rmse.CL(0.1),rmse.CL(0.3),rmse.CL(0.5)))
rmse
```

The above results give us an indication that both methods seem to be
largely *unbiased* as the averages for each are very close to the
corresponding true correlation values. However, the much larger `rmse`
values for the second approach suggest that it has a much greater
variance. This large variance is also evident in the above plot.

The above simulation study is repeated with the exact same parameters but with the **sample size increased to 1,000,000**. The results can be illustrated as follows.


```{r,include=FALSE}
res <- read.csv("cor_deriv_sims_2.csv")

res$sim_cor <- c(rep(c(1:90),4))
res_new <- rbind(res,res)
res_new$true_cor <- res_new$actual_cor
res_new$est_cor <- c(res$est_cor,res$est_cor_full)
res_new$fun <- c(rep("CL",360),rep("L",360))
res <- res_new
```

```{r, fig.width=10}
library("RColorBrewer")
col <- brewer.pal(8,"GnBu")
ggplot(res, aes(x=sim_cor, y=est_cor,color=as.factor(h2),shape=as.factor(prop_effect))) + geom_point(size=2) + facet_grid(fun~true_cor, labeller="label_both")+ ylab("Estimated Correlation") + xlab("Simulation") + scale_color_manual(values=c(col[4],col[6],col[8])) +
  geom_hline(aes(yintercept = true_cor), linetype=1, size=0.5) + 
  guides(color = guide_legend(title = "Heritability"), shape= guide_legend(title = "Polygenicity")) 
```
```{r,include=FALSE}
res <-res_new[1:360,]
```


In addition, we compute the average estimated correlation as well as the root mean square error for each true correlation value. 

```{r}
rmse.L <- function(cor){sqrt(sum((res[res$true_cor==cor,]$est_cor_full - res[res$true_cor==cor,]$true_cor)^2)/nrow(res[res$true_cor==cor,]))}
rmse.CL <- function(cor){sqrt(sum((res[res$true_cor==cor,]$est_cor - res[res$true_cor==cor,]$true_cor)^2)/nrow(res[res$true_cor==cor,]))}
mean.L <- function(cor){sum(res[res$true_cor==cor,]$est_cor_full)/nrow(res[res$true_cor==cor,])}
mean.CL <- function(cor){sum(res[res$true_cor==cor,]$est_cor)/nrow(res[res$true_cor==cor,])}
rmse <- data.frame(true_cor = c(-0.1,0.1,0.3,0.5),
                   mean.L = c(mean.L(-0.1),mean.L(0.1),mean.L(0.3),mean.L(0.5)),
                   mean.CL = c(mean.CL(-0.1),mean.CL(0.1),mean.CL(0.3),mean.CL(0.5)),
                   rmse.L = c(rmse.L(-0.1),rmse.L(0.1),rmse.L(0.3),rmse.L(0.5)),
                   rmse.CL = c(rmse.CL(-0.1),rmse.CL(0.1),rmse.CL(0.3),rmse.CL(0.5)))
rmse
```

[$\star$]{style="color: blue;"} **Note:** These new set of results in which we have increased the sample size show us that the *log-likelihood* approach is clearly ***biased*** while the *conditional log-likelihood* remains ***unbiased*** with a high degree of variance.


[$\star$]{style="color: blue;"} **Conclusions?**


<br>

#### Real Data

We also decided to test our approach for estimating the correlation, or
$\lambda$, on real data. The steps followed are provided in detail
below. As it is estimating $\lambda$ that is of interest here, rather
than estimating the causal effect, we simply considered two quantitative
traits, BMI and height. We first randomly obtained a subset of 100,000
individuals which were common to both BMI and height UKBB datasets. This
provided us with fully overlapping datasets, which means that
$\hat \lambda = \hat \rho_{XY}$, in which $\rho_{XY}$ is the correlation
between the exposure and the outcome. Therefore, the individual level
data for these 100,000 individuals allow us to obtain $\hat\rho_{XY}$.
Then, by carrying out GWASs for both datasets and obtaining summary
statistics, we can use these summary statistics to obtain $\hat \lambda$
and test our approach. We note that we must use our approach on
independent SNPs and therefore, we will also consider pruning our full
set of SNPs to obtain a smaller set of approximately independent SNPs.
It is then the summary statistics of this smaller set of SNPs that will
be used to estimate $\lambda$.

[$\star$]{style="color: blue;"} **Note:** We had considered clumping the
SNPs as opposed to pruning, as it is clumping that seems to be more
often used in MR practices. However, clumping functionality is not
available on PLINK 2.0 and thus, pruning was deemed the quicker easier
choice.

**Step 1: Obtain fully overlapping sample data set**

*Code:*
`Rscript sample_HB.R height4plink.txt bmi4plink.txt height_cor.txt bmi_cor.txt cor_HB.txt`

-   *Inputs:* height4plink.txt, bmi4plink.txt
-   *Outputs:* height_cor.txt, bmi_cor.txt, cor_HB.txt

***R scripts:***

-   [**sample_HB.R**](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/R/sample_HB.R)
    reads in both height4plink.txt and bmi4plink.txt. It first removes
    any individuals which do not have available information for both
    traits and then, obtains a set of individuals which are common to
    both datasets. From this set, using `set.seed(1998)`, a random set
    of 100,000 individuals are chosen. The outputted data sets,
    height_cor.txt and bmi_cor.txt contain height and BMI information,
    respectively, of the same 100,000 individuals, ordered by ID number.
    Using the function `cor()`, the correlation between height and BMI
    for these individuals is obtained and can be found in cor_HB.txt.

[$\star$]{style="color: blue;"} **Note:** The estimate for $\rho_{XY}$,
which was obtained using the sample of 100,000 UKBB individuals, was
found to be **-0.0168766255879653**.

**Step 2: Performing two GWASs**

*Code:* `sbatch parallel_height_cor.sh` `sbatch parallel_bmi_cor.sh`

-   *Inputs:* height_cor.txt, bmi_cor.txt, height_cor.sh, bmi_cor.sh,
    height_cor.txt, bmi_cor.txt, \*\_qcd.pgen, \*\_qcd.psam,
    \*\_qcd.pvar
-   *Outputs:* height_res\_\*.PHENO1.glm.linear,
    bmi_res\_\*.PHENO1.glm.linear for each chromosome

***Shell scripts:***

-   [height_cor.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/height_cor.sh)
    and
    [bmi_cor.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/bmi_cor.sh)
    first combine the genotype information, \*\_qcd.pgen, \*\_qcd.psam,
    \*\_qcd.pvar, with the height/BMI data sets,
    height_cor.txt/bmi_cor.txt, to make \*\_qcd_height.bed,
    \*\_qcd_height.bim, \*\_qcd_height.fam and \*\_qcd_bmi.bed,
    \*qcd_bmi.bim, \*qcd_bmi.fam files for the specified chromosome.
    They then use these files and perform an association analysis with
    the results outputted in height_res\*.PHENO1.glm.linear and
    bmi_res\*.PHENO1.glm.linear for the chromsome.

-   [parallel_height_cor.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/parallel_height_cor_prune.sh)
    and
    [parallel_bmi_cor.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/parallel_bmi_cor_prune.sh)
    run height_cor.sh and bmi_cor.sh, respectively, on each chromosome
    from 1 to 22 in parallel.

**Step 3: Combining results**

*Code:* `sbatch summary_stats_height_cor.sh`
`sbatch summary_stats_bmi_cor.sh`

-   *Inputs:* summary_stats_height_cor.R, summary_stats_bmi_cor.R,
    height_res\_\*.PHENO1.glm.linear, bmi_res\_\*.PHENO1.glm.linear for
    each chromosome
-   *Outputs:* summary_stats_height_cor.txt, summary_stats_bmi_cor.txt

***R scripts:***

-   [summary_stats_height_cor.R](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/R/summary_stats_height_cor.R)
    and
    [summary_stats_bmi_cor.R](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/R/summary_stats_bmi_cor.R)
    combine together the summary statistics obtained from performing an
    association analysis on each chromosome, with respect to height/BMI,
    and output data sets with 5 columns: chromosome, position, rsID,
    estimated effect size, standard error of estimated effect size.

***Shell scripts:***

-   [summary_stats_height_cor.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/summary_stats_height_cor.sh)
    and
    [summary_stats_bmi_cor.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/summary_stats_bmi_cor.sh)
    run the R scripts summary_stats_height_cor.R and
    summary_stats_bmi_cor.R, inputting height_res\_\*.PHENO1.glm.linear
    and bmi_res\_\*.PHENO1.glm.linear of each chromsome and outputting
    summary_stats_height_cor.txt and summary_stats_bmi_cor.txt.

**Step 4: Obtaining pruned set of SNPs**

*Code:* `sbatch parallel_bmi_cor_prune.sh`
`sbatch parallel_height_cor_prune.sh` `sbatch join_pruned_bmi.sh`
`sbatch join_pruned_height.sh` `sbatch summary_stats_prune.sh`

-   *Inputs:* bmi_cor_prune.sh, height_cor_prune.sh, bmi_cor.txt,
    height_cor.txt, \*\_qcd.pgen, \*\_qcd.psam, \*\_qcd.pvar,
    prune_join.R, summary_stats_prune.R, summary_stats_height_cor.txt,
    summary_stats_bmi_cor.txt
-   *Outputs:* summary_stats_height_prune.txt,
    summary_stats_bmi_prune.txt

***R scripts:***

-   [prune_join.R](https://github.com/amandaforde/winnerscurse_MR/tree/main/real_data_scripts/R/prune_join.R)
    combines the lists of pruned SNPs together from the 22 chromosomes
    to form a single list.

-   [summary_stats_prune.R](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/R/summary_stats_prune.R)
    reads in the list of pruned SNPs and a data set of summary
    statistics, i.e. summary_stats_height_cor.txt or
    summary_stats_bmi_cor.txt. It then creates a subset of this summary
    statistics data set which contains summary statistics of only those
    SNPs contained in the list of pruned SNPs.

***Shell scripts:***

-   [bmi_cor_prune.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/bmi_cor_prune.sh)
    and
    [height_cor_prune.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/height_cor_prune.sh)
    first combine the genotype information, \*\_qcd.pgen, \*\_qcd.psam,
    \*\_qcd.pvar, with the BMI/height data set,
    bmi_cor.txt/height_cor.txt, to make \*\_qcd_bmi.bed,
    \*\_qcd_bmi.bim, \*\_qcd_bmi.fam and \*\_qcd_height.bed,
    \*qcd_height.bim, \*qcd_height.fam files for the specified
    chromosome. They then use these files and employ the command
    `--indep-pairwise 50 5 0.5` for pruning. This means that pruning
    occurs by first calculating LD between each pair of SNPs in a window
    of 50 SNPs. If an LD value greater than 0.5 is observed, then one
    SNP out of this pair is removed. The window is shifted 5 SNPs
    forward and the process is repeated. A set of pruned SNPs are
    outputted and contained in the files bmi_prune\*.prune.in and
    height_prune\*.prune.in for each chromosome.

-   [parallel_bmi_cor_prune.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/parallel_bmi_cor_prune.sh)
    and
    [parallel_height_cor_prune.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/parallel_height_cor_prune.sh)
    run bmi_cor_prune.sh and height_cor_prune.sh, respectively, on each
    chromosome from 1 to 22 in parallel.

-   [join_pruned_bmi.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/join_pruned_bmi.sh)
    and
    [join_pruned_height.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/join_pruned_height.sh)
    run the R script prune_join.R, inputting bmi_prune\_\*.prune.in and
    height_prune\_\*.prune.in of each chromosome and outputting
    pruned_SNPS_bmi.txt and pruned_SNPS_height.txt, which both contain a
    list of 1,590,043 SNPs.

-   [summary_stats_prune.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/summary_stats_prune.sh)
    runs the R script summary_stats_prune.R twice, first inputting
    pruned_SNPs_bmi.txt and summary_stats_bmi_cor.txt and outputting
    summary_stats_bmi_prune.txt. The second time, pruned_SNPs_height.txt
    and summary_stats_height_cor.txt are inputted, and
    summary_stats_height_prune.txt is outputted.

**Step 5: Estimating correlation**

*Code:* `sbatch cor_check.sh`

-   *Inputs:* cor_check.R, summary_stats_bmi_prune.txt,
    summary_stats_height_prune.txt
-   *Outputs:* est_cor.txt

***R scripts:***

-   [**cor_check.R**](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/shell/cor_check.R) first reads in the summary statistics for the pruned set of SNPs for both height and BMI, summary_stats_height_prune.txt and summary_stats_bmi_prune.txt. It then ensures that any rows with missing information for the estimated effect size are removed from each data set and that both data sets contain summary statistics for the exact same SNPs. Following this, our first approach for estimating the correlation, or $\lambda$, using summary statistics is considered. The entire set of SNPs is used and the *log-likelihood* function is optimized to obtain an estimate. Second, a subset of SNPs is obtained which all have absolute $z$-scores for both height and BMI less than 0.5. The *conditional log-likelihood* function is then optimized to obtain a second estimate. The two estimates are outputted in est_cor.txt.

***Shell scripts:***

-   [cor_check.sh](https://github.com/amandaforde/winnerscurse_MR/blob/main/real_data_scripts/R/cor_check.sh)
    runs the R script cor_check.R, inputting summary_stats_bmi_prune.txt
    and summary_stats_height_prune.txt and outputting est_cor.txt.

[$\star$]{style="color: blue;"} **Note:** The estimate for $\lambda$
obtained using the first version of our approach, in which the
*log-likelihood* function is optimized using the full set of SNPs, was
**-0.086906563122001**, while the estimate obtained by optimizing the
*conditional log-likelihood* function with a selected subset of SNPs, in
which both absolute $z$-scores of every SNP in the subset are less than
0.5, was **-0.093659068071487**.


**Step 5a: Estimating correlation with unpruned SNPs**

*Code:* `sbatch cor_check.sh`

-   *Inputs:* cor_check.R, summary_stats_bmi_cor.txt,
    summary_stats_height_cor.txt
-   *Outputs:* est_cor_full.txt

Here, we use the same R script as above but now apply it to our original unpruned sets of summary statistics.

[$\star$]{style="color: blue;"} **Note:** The estimate for $\lambda$
obtained using the first version of our approach, in which the
*log-likelihood* function is optimized using the full set of SNPs, was
**-0.0861423439755467**, while the estimate obtained by optimizing the
*conditional log-likelihood* function with a selected subset of SNPs, in
which both absolute $z$-scores of every SNP in the subset are less than
0.5, was [**-0.0352981656737371**]{style="color: darkred;"}.

Even though the SNPs used here are no longer independent, we can see that we have potentially stabilised the *conditional log-likelihood* estimate further as it has moved closer to the expected value while the *log-likelihood* remains at a similar value as before.


**Step 5b: Estimating correlation with increased $z$-statistic threshold**

*Code:* `sbatch cor_check.sh`

-   *Inputs:* cor_check.R, summary_stats_bmi_prune.txt,
    summary_stats_height_prune.txt
-   *Outputs:* est_cor_1_prune.txt

Here, we use the same R script as above applied to the pruned set of SNPs, but in which the subset of SNPs used by the *conditional log-likelihood* function all have absolute $z$-scores for both height and BMI less than **1**.

[$\star$]{style="color: blue;"} **Note:** The estimate for $\lambda$
obtained using the first version of our approach, in which the
*log-likelihood* function is optimized using the full set of SNPs, was
**-0.0869065653122001**, while the estimate obtained by optimizing the
*conditional log-likelihood* function with a selected subset of SNPs, in
which both absolute $z$-scores of every SNP in the subset are less than
1, was **-0.0836582509286098**.

*Code:* `sbatch cor_check.sh`

-   *Inputs:* cor_check.R, summary_stats_bmi_cor.txt,
    summary_stats_height_cor.txt
-   *Outputs:* est_cor_1_unprune.txt

Again, we use the same R script as above applied to the original unpruned set of SNPs, but in which the subset of SNPs used by the *conditional log-likelihood* function all have absolute $z$-scores for both height and BMI less than **1**.

[$\star$]{style="color: blue;"} **Note:** The estimate for $\lambda$
obtained using the first version of our approach, in which the
*log-likelihood* function is optimized using the full set of SNPs, was
**-0.0861423439755467**, while the estimate obtained by optimizing the
*conditional log-likelihood* function with a selected subset of SNPs, in
which both absolute $z$-scores of every SNP in the subset are less than
1, was **-0.077674560083575**.


[$\star$]{style="color: blue;"} **Conclusions?**

<br>


### Reducing SNP numbers 

It is mentioned in our section regarding estimating the causal effect that our proposed method involves repeatedly simulating values for $\hat\beta_{X_{\pi}}$ and $\hat\beta_{Y_{\pi}}$ and then selecting SNPs using $\hat\beta_{X_{\pi}}$. In order to reduce the standard error of our causal effect estimate, this repeated simulation of $\hat\beta_{X_{\pi}}$ and $\hat\beta_{Y_{\pi}}$ values occurs several times, ideally at the very minimum $N_{\text{iter}} = 100$ times. Naturally, with a large data set of SNPs, this repeated simulation process is very **computationally intensive**. Of course, there are many SNPs that will never be selected across the $N_{\text{iter}}$ iterations. 

Thus, in order to improve the computational intensity of our approach, we propose that a **subset of SNPs** should be first obtained from the large data set of SNPs and it is just then the SNPs in this subset that will be used in our method. We suggest that selection of a SNP into this subset will be based on its probability to be selected on a given iteration, i.e. the probability that the simulated value $\left| \hat{z}_{X_{\pi}} \right| = \left| \frac{\hat\beta_{X_{\pi}}}{\widehat{\text{se}(\hat\beta_{X_{\pi}})}} \right|$ is less than `qnorm((5e-8)/2, lower.tail=FALSE)` $\approx 5.45$, in which we have chosen that selection at each iteration is based on the $p$-value threshold of $5 \times 10^{-8}$. Therefore, we must now derive an expression for the probability of a SNP to be selected on a given iteration. 

Firstly, let us note that from our work detailed above, we know that $\sqrt{\text{var}\left( \hat\beta_{X_\pi}\right)} \sim \sqrt{\frac{1}{\pi}\cdot \text{var}\left( \hat\beta_{X}\right)}$ which allows us to approximate $\widehat{\text{se}\left( \hat\beta_{X_\pi}\right)} \sim \sqrt{\frac{1}{\pi}\cdot \left(\text{se}\left( \hat\beta_{X}\right)\right)^2}$. We can then define $\hat{z}_{X_{\pi}}$ as $\hat{z}_{X_{\pi}}=\frac{\hat\beta_{X_{\pi}}}{\sqrt{\frac{1}{\pi}\cdot \left(\text{se}\left( \hat\beta_{X}\right)\right)^2}}$.

Now, from above, we also have the following $E(\hat\beta_{X_\pi} | \hat\beta_{X}) = \hat\beta_{X}$ and $\text{var}\left( \hat\beta_{X_\pi} | \hat\beta_{X} \right) \sim \frac{1-\pi}{\pi}\cdot \text{var}\left( \hat\beta_{X}\right)$ which gives $\left(\text{se}\left( \hat\beta_{X_\pi} | \hat\beta_{X}\right)\right)^2 \sim \frac{1-\pi}{\pi}\cdot \left(\text{se}\left( \hat\beta_{X}\right)\right)^2$. Therefore, we can write the following: 

$$\left( \hat\beta_{X_\pi} | \hat\beta_{X} \right) \sim N \left( \hat\beta_X, \left(\frac{1-\pi}{\pi}\right)\cdot(\text{se}(\hat\beta_{X}))^2 \right)$$


We are in fact interested in the distribution of $\hat{z}_{X_{\pi}}$. However, as this is simply equal to $\hat\beta_{X_\pi}$ multiplied by the scalar $\frac{1}{\sqrt{\frac{1}{\pi}\cdot \left(\text{se}\left( \hat\beta_{X}\right)\right)^2}}$, we can use the distribution above to form the following:

$$\left( \frac{\hat\beta_{X_\pi}}{\sqrt{\frac{1}{\pi}\cdot \left(\text{se}\left( \hat\beta_{X}\right)\right)^2}} \middle| \hat\beta_{X} \right) = \left( \hat{z}_{X_{\pi}} | \hat\beta_{X} \right) \sim N \left( \frac{\hat\beta_X}{\sqrt{\frac{1}{\pi}\cdot \left(\text{se}\left( \hat\beta_{X}\right)\right)^2}}, \frac{\left(\frac{1-\pi}{\pi}\right)\cdot(\text{se}(\hat\beta_{X}))^2}{\frac{1}{\pi}\cdot \left(\text{se}\left( \hat\beta_{X}\right)\right)^2} \right)$$

Tidying the above up and defining $\hat{z}_{X} = \frac{\hat\beta_{X}}{\sqrt{\left(\frac{1}{\pi}\right)\cdot(\text{se}(\hat\beta_{X}))^2}}$, we get:

$$\left( \hat{z}_{X_{\pi}} | \hat\beta_{X} \right) \sim N \left( \hat{z}_{X}, 1-\pi \right)$$

Therefore, using this result, the probability of a SNP to be selected on a given iteration, $f(\left| \hat{z}_{X_{\pi}} \right| > 5.45)$ can be easily derived as follows, in which $\Phi$ is the cumulative distribution function of a standard normal: 

$$f(\left| \hat{z}_{X_{\pi}} \right| > 5.45 ) = \Phi\left(\frac{- 5.45 + \hat{z}_{X}}{\sqrt{1-\pi}}\right) + \Phi\left(\frac{- 5.45 - \hat{z}_{X}}{\sqrt{1-\pi}}\right)$$

[$\star$]{style="color: blue;"} **Note:** Recall that the value of $\hat{z}_X$ is known for each SNP. Thus, $f(\left| \hat{z}_{X_{\pi}} \right| > 5.45)$ can be quickly worked out for each SNP using the above. 

Now that we have shown that we can easily work out the probability of each SNP to be selected on a given iteration, the question of interest turns to how might we use these probabilities to choose which SNPs to contain in the subset of SNPs that will be used in our method. We first considered that perhaps we could simply choose those SNPs in which this probability is greater than a chosen value. For example, choosing this value to be 0.01, SNPs which have $f(\left| \hat{z}_{X_{\pi}} \right| > 5.45) \ge 0.01$, would then be selected into the subset and used in our method. However, this approach could prove to be potentially problematic if we were presented with a data set which, for some reason, had thousands of SNPs with a probability of selection just below 1%. In the case where we use the entire set of SNPs in our method, many of these SNPs would actually turn out to be instruments while in this restricted version concerning only a subset of SNPs, these SNPs would all be excluded. 

Given the above, we consider a different approach in which we lower bound the probability that the restricted version and the full version will result in the same instruments, i.e. the same SNPs being selected. Upon working out $f(\left| \hat{z}_{X_{\pi}} \right| > 5.45)$ for each SNP, we then order all SNPs by these probabilities from smallest to largest. Following this ordering, we obtain the cumulative sum of these probabilities. Note that in this case, the largest cumulative sum is then equivalent to the expected number of instruments when the full set of SNPs is used in our method. We could then choose all SNPs for which the **cumulative probability is greater than 0.05** for our subset. This process should ensure that for a single iteration of our method, the number of instruments if the full set of SNPs is used and the number of instruments if merely the subset is used will be equal with probability at least 95%. The point where we subset the SNPs, i.e. 0.05 here, can also be interpreted as the expected difference in the number of instruments if the full set of SNPs is used and the number of instruments if the subset is used. Thus, this cumulative probability approach gives an extra probabilistic guarantee that the results obtained using the full set of SNPs and those obtained using the subset will be very similar in terms of the number of, and actual, instruments included.


[$\star$]{style="color: blue;"} **Example:** For example, if our data set of interest originally had 100,000 SNPs such as the example data set, `data`, below, applying our proposed method would result in simulating values for $\hat\beta_{X_\pi}$ and  $\hat\beta_{Y_\pi}$ for all of those SNPs a total of $N_{\text{iter}}$ times. Assuming that $\pi =0.5$ and the $p$-value threshold for selection is $5 \times 10^{-8}$, we could obtain the probability of selection for each SNP using our derivation above, namely `p.exposure1`. Following, this we can easily compute the cumulative sums by first ordering the SNPs and then using the function `cumsum`. Finally, all SNPs for which this cumulative sum is less than 0.05 are removed. It can be seen below that we are left 436 SNPs, which is clearly a great reduction in SNP numbers compared to the original 100,000. This means that computation time would be greatly reduced, allowing us to then increase the value of $N_{\text{iter}}$.


```{r}
## Simulate entire data set:
set.seed(1998)
data <- sim_mr_stats(n_snps=10^5,prop_effect=0.01,h2=0.4,frac_overlap=1,n_x=100000,n_y=100000,cor_xy=0.6,beta_xy=0.3) 

## Obtain probabilities of selection:
pi <- 0.5
thresh1 <- stats::qnorm((5e-8)/2, lower.tail=FALSE)
data$mu.exposure1 <- data$beta.exposure/sqrt(((1)/(pi))*(data$se.exposure)^2)
data$p.exposure1 <- pnorm((-thresh1 + data$mu.exposure1)/(sqrt(1-pi))) + pnorm((-thresh1 - data$mu.exposure1)/(sqrt(1-pi)))

## Obtain cumulative sum of probabilities:
data1 <- data[order(data$p.exposure1,decreasing=FALSE),]
data1$cumul.sum <- cumsum(data1$p.exposure1)

## Remove SNPs with cumulative sum less than 0.05:
data1 <- data1[-which(data1$cumul.sum < 0.05),]
nrow(data1) 
```


<br><br>

## Using `mr.simss`

We have created an R package, `mr.simss`, which allows us to implement the approach that we have proposed in this document. `mr.simss` can be installed as follows: 

```{r}
remotes::install_github("amandaforde/mr.simss")
library(mr.simss)
```


We can now briefly demonstrate how the functions in this package may be used in order to obtain MR causal effect estimates using GWAS summary statistics. Firstly, we will consider a fully overlapping toy data set in which the causal effect of the exposure on the outcome is **0.3**. We use the function created above, `sim_mr_stats` to do so and view the first six rows of our simulated data set.

```{r}
set.seed(1998)
data <- sim_mr_stats(n_snps=10^5,prop_effect=0.01,h2=0.4,frac_overlap=1,n_x=100000,n_y=100000,cor_xy=0.6,beta_xy=0.3) 
head(data)
```


Before we implement our method from `mr.simss`, we consider first using the `mr` function from the `TwoSampleMR` R package which applies the following well-known method, IVW, and obtains the MR causal effect estimate: 

```{r}
data_mr <- tibble::tibble(
      SNP = data$SNP,
      id.exposure="X",
      id.outcome="Y",
      exposure="X",
      outcome="Y",
      beta.exposure = data$beta.exposure,
      beta.outcome = data$beta.outcome,
      se.exposure = data$se.exposure,
      se.outcome = data$se.outcome,
      mr_keep=TRUE
    )

data_sig <- data_mr %>% dplyr::filter(2*(stats::pnorm(abs(data_mr$beta.exposure/data_mr$se.exposure), lower.tail=FALSE)) < 5e-8)
data_sig %>% TwoSampleMR::mr(data_mr,method_list=c("mr_ivw"))
```

We also consider applying the MR-RAPS method to this data set as follows:

```{r}
library(mr.raps)
results <- mr.raps::mr.raps(data_sig$beta.exposure,data_sig$beta.outcome,data_sig$se.exposure,data_sig$se.outcome)
data.frame(method="mr_raps", nsnp=nrow(data_sig), b=results$beta.hat, se=results$beta.se, pval=results$beta.p.value)
```

<br>

The **main function** in the package is `mr_simss` which executes the method, **MR-SimSS**. We can apply `mr_simss` directly to our simulated data set, assuming that we have knowledge of the number of samples in both outcome and exposure GWASs, the number of overlapping samples between the two GWASs, and the correlation between the exposure and the outcome. As the default setting of the parameter `n.iter`, `n.iter=1000` can be quite computationally intensive, we will use `n.iter=100` for demonstration purposes. Also, note that as we are aware that we are dealing with fully overlapping samples, it is advisable to use the 3 split approach. Therefore, we set `splits=3`.

```{r}
mr.simss::mr_simss(data,n.exposure=100000,n.outcome=100000,n.overlap=100000,cor.xy=0.6,n.iter=100,splits=3,parallel=TRUE)$summary
```


As mentioned above, for large values of `n.iter`, applying our proposed approach to the entire data set can be quite computationally intensive. We have discussed this in the section *"Reducing SNP numbers"* and described a way in which we could obtain a subset of SNPs and simply apply our method to just the SNPs in that subset, resulting in a great reduction in computational time. Changing the parameter `subset` in the `mr_simss` function from its default setting `subset=FALSE` to `subset=TRUE` allows us to use the **MR-SimSS** method with a smaller subset of SNPs. Note that the value for `sub.cut` determines a cut-off point for inclusion of SNPs into the subset based on cumulative probabilities. The default value is `sub.cut=0.05`. Further details regarding the manner in which SNPs are selected into the subset are provided in the section above. Due to the reduction in computational time by using `subset=TRUE`, this then allows us to increase `n.iter`.

```{r}
mr.simss::mr_simss(data,subset=TRUE,sub.cut=0.05,n.exposure=100000,n.outcome=100000,n.overlap=100000,cor.xy=0.6,n.iter=1000,splits=3,parallel=TRUE)$summary
```

Now, let us explicitly examine how much we save in computational time by using the subset approach for this example. We also note that our full data set here has only 100,000 SNPs. It is quite likely that a real data set would have more than this, meaning much greater computational time for only $N_{\text{iter}} = 100$ iterations of the method. Firstly, using the full data set of 100,000 SNPs with our method and carrying out 100 iterations takes about 4 minutes.

```{r}
start.time <- Sys.time()
full <- mr.simss::mr_simss(data,n.exposure=100000,n.outcome=100000,n.overlap=100000,cor.xy=0.6,n.iter=100,splits=3,parallel=TRUE)
end.time <- Sys.time()
round(end.time - start.time,2)
```

Following this, we can see that using the subset approach for 1000 iterations takes only 27.36 seconds, which is a reduction in computational time of approximately 88%, while the number of iterations have been increased 10-fold.  

```{r}
start.time <- Sys.time()
sub <- mr.simss::mr_simss(data,subset=TRUE,sub.cut=0.05,n.exposure=100000,n.outcome=100000,n.overlap=100000,cor.xy=0.6,n.iter=1000,splits=3,parallel=TRUE)
end.time <- Sys.time()
round(end.time - start.time,2)
```


In addition, we could have used the function `est_lambda` to first estimate *lambda* and then use this value with `mr_simss`. We demonstrate this as follows:

```{r}
lambda <- est_lambda(data)
mr.simss::mr_simss(data,subset=TRUE,sub.cut=0.05,est.lambda=TRUE,lambda.val=lambda,n.iter=1000,splits=3,parallel=TRUE)$summary
```


Note that in all our examples so far we have used the default MR method, IVW, with our approach. We now have a look at using MR-RAPS with our approach. MR-RAPS has the additional benefit that it also assists in eliminating weak instrument bias. 

```{r}
mr.simss::mr_simss(data,subset=TRUE,sub.cut=0.05,est.lambda=TRUE,lambda.val=lambda,n.iter=1000,splits=3,mr_method="mr_raps",parallel=TRUE)$summary
```

As our method works in combination with other MR methods, as shown here with IVW and MR-RAPS, it is also possible to use `mr_simss` with other pleiotropy robust approaches, such as weighted median and weighted mode. We demonstrate this below. However, in this instance, these two methods do not seem to perform as well MR-RAPS and IVW. 

```{r}
mr.simss::mr_simss(data,subset=TRUE,sub.cut=0.05,est.lambda=TRUE,lambda.val=lambda,n.iter=100,splits=3,mr_method="mr_weighted_median",parallel=TRUE)$summary
```

```{r}
mr.simss::mr_simss(data,subset=TRUE,sub.cut=0.05,est.lambda=TRUE,lambda.val=lambda,n.iter=100,splits=3,mr_method="mr_weighted_mode",parallel=TRUE)$summary
```

<br>

[$\star$]{style="color: blue;"} **Note:** Naturally, it is difficult to see if our approach,  **MR-SimSS**, is in fact performing well and obtaining *bias-reduced* causal effect estimates by illustrating its implementation with just one simulated data set. Therefore, the next step is thus to consider performing a **simulation study** which would incorporate simulating many different data sets and comparing our approach with other currently published methods. 


<br><br>

## Simulation study design

When designing our simulation study, there are several things to consider and keep in mind:

i) Firstly, we must decide on what **other methods** should we include in our study and thus, compare our proposed method against. 

ii) We must consider what **simulation parameters** should we adjust, e.g. sample size, fraction of overlap, true causal effect, heritability and polygenicity of exposure, correlation between exposure and outcome, and the total number of SNPs, and what values they should take.

iii) In addition, the current way in which we are creating our simulated data sets does not take into account any balanced or directional **pleiotropy**, a characteristic that is regularly seen in real data scenarios. 

[$\star$]{style="color: blue;"} **Note:** Above, we have discussed several variations of our method. Thus, when designing our simulation study, this gives rise to the following key considerations:

- [**Question:**]{style="color: Purple;"} *Should we use the full data set of SNPs with our method or obtain a* ***subset***, *as described above, and use only that smaller set of SNPs with the method? If we choose the subset approach, is the value of 0.05 most appropriate for `sub.cut`?* [**Discussion:**]{style="color: Purple;"} The benefit of using the smaller set of SNPs is that it will *significantly* reduce computation time and thus, allow us to increase the number of iterations, `n.iter`, to 1000, which has been seen, in brief experimentation, to produce a more stable estimate than `n.iter=100`. Furthermore, it is anticipated that an approach with a much smaller computational time would be of more interest to researchers. From above, 0.05 does seem appropriate for `sub.cut` although it could possibly be reduced further to 0.01, say. Indeed, this is perhaps an area for further investigation in which there could be other variants of the approach which differ in the means of obtaining a suitable cut-off point.  

- [**Question:**]{style="color: Purple;"} *Should we* ***estimate*** ***lambda*** *using `est_lambda()` or will we assume that the values of `n.exposure`, `n.outcome`, `n.overlap` and `cor.xy` are all known?* [**Discussion:**]{style="color: Purple;"} Of course, this is only of interest to use when the fraction of overlap is greater than 0, i.e. when we don't have two fully independent outcome and exposure GWAS data sets. The hesitation of using `est_lambda()` is due to the fact that the estimate of *lambda* it provides has been shown to have a high degree of variance, even if it is unbiased. We currently do not know how using a highly variable estimate of *lambda* in our method may affect our causal effect estimate. Perhaps the key question is 'do we generally expect researchers to know the number of samples in each GWAS, the number of overlapping samples and the correlation between the exposure and the outcome, or at least be able to estimate these values very well?' If the researcher has access to the individual-level data, these values would be very easily obtainable. Therefore, it is perhaps reasonable to test our approach using the default setting of `est.lambda=FALSE` with *known* values of `n.exposure`, `n.outcome`, `n.overlap` and `cor.xy`. That said, a certain avenue for further research would be to see how sensitive our method is to variable estimates of *lambda*. 

- [**Question:**]{style="color: Purple;"} *For all fractions of overlap, should we test both `splits=2` and `splits=3`?* [**Discussion:**]{style="color: Purple;"} An alternative here would be to suggest using `splits=2` only when the outcome and exposure GWASs have employed two independent non-overlapping samples. Otherwise, for any degree of overlap, `splits=3` may be considered as the safer option in order to avoid weak instrument bias in the direction of the observational association. However, it could be interesting to see in simulations how `splits=2` and `splits=3` perform in each overlap scenario.


- [**Question:**]{style="color: Purple;"} *Is it okay to simply leave the* ***splitting fractions at 50%***, *i.e. `pi=0.5` and `pi2=0.5`, or will further theoretical justification for the reason for choosing these fractions be required?* [**Discussion:**]{style="color: Purple;"} Again, this question presents another avenue for further work and exploration. However, it could be potentially difficult to come up with a single theoretical rule for these splitting fractions as it is likely that optimal fractions will change based on the genetic architectures of the exposure and outcome. We feel that it is perhaps more important to present our initial proposal of the method and demonstrate its effectiveness rather than obtaining optimal values for each parameter of our method. In practice, with individual-level data, in order to avoid *Winner's Curse*, it has been seen that researchers tend to split their entire data set in two and use half of the samples for selecting/discovering the instrument SNPs and the other half for estimating the SNP-exposure effect sizes. In fact, this idea of optimising discovery/replication sample splits for MR analysis is mentioned in the work of [Sadreev et al. (2021)](https://www.medrxiv.org/content/10.1101/2021.06.28.21259622v1) where they state that they "determined that splitting the UK Biobank sample into two sets of 50% each would provide the most flexibility". Therefore, this gives us at least some justification for choosing `pi=0.5`, which means that at each iteration, simulated values corresponding to half of the entire data set will be used for the purpose of SNP selection. Naturally, this does mean that as the sample size for selection is decreased compared to that of the full data set, we do expect less SNPs to be selected as instruments on a given iteration than if we were to use the full data set for selection. In the three split approach, we have chosen to continue to use half of the data set for selection. The reason for this is that we want to ensure that a good quantity of instruments are still being selected at each iteration. Of course, this means that smaller sample sizes are then used in estimating the SNP-exposure and SNP-outcome effect sizes, which can result in an increase in variance of our causal effect estimate. However, as we can increase the number of iterations of our approach as high as potentially $N_{\text{iter}} = 1000$ and our final causal effect estimate is the mean of the estimates at each iteration, this high number of iterations can reverse the increase in variance and allow us to obtain a much more stable final causal effect estimate. For these reasons, we feel that simply setting the values `pi=0.5` and `pi2=0.5` and using these defaults is appropriate for this stage of our work. 


- [**Question:**]{style="color: Purple;"} *Should we consider using a value greater than $5 \times 10^{-8}$ for the* ***selection threshold*** *or is it appropriate to simply leave it as `threshold=5e-8`?* [**Discussion:**]{style="color: Purple;"} The selection threshold is perhaps another parameter that deserves the same treatment as the splitting fractions above, in the sense that time could be dedicated to investigating if using a less stringent threshold would be more suitable, but for now it is perhaps more sensible to simply remaining using a default value. For that reason, we will continue to simply use the genome-wide threshold of $5 \times 10^{-8}$ for selection purposes in our proposed method. 

- [**Question:**]{style="color: Purple;"} *What* ***MR methods*** *should we consider to work in combination with our method?* [**Discussion:**]{style="color: Purple;"} As mentioned above, our proposed approach requires the use of other MR methods to work in conjunction with it. We first investigate using the IVW method with our approach as this is perhaps the simplest MR method, and much work has been done in the field to suggest improvements to this straightforward approach. However, we have seen that even though *Winner's Curse* bias will be removed from using our approach with IVW, it is possible that a certain amount of weak instrument bias will still exist, especially due to the simulated sample splitting procedure. As MR-RAPS is a well-known method that can alleviate this weak instrument bias, we have also considered using MR-RAPS with our approach. This combination has been seen to work really well in practice, especially in the case where we have a single fully overlapping data set and use `splits=3`. It is possible that other pleiotropy-robust approaches could be considered, however it is perhaps sensible to focus on IVW and MR-RAPS and to dedicate investigating the performance of these other MR methods in conjunction with our approach to another time for similar reasons as those mentioned above, e.g. time constraints etc.  

<br>

### Other methods

The first point mentioned above regarding the simulation study design suggested that we must decide on what other methods we should include in our study for comparison purposes. The following recently published papers can give us an idea regarding this: [*"Breaking the Winner's Curse in Mendelian Randomization: Rerandomized Inverse Variance Weighted Estimator"*](https://arxiv.org/pdf/2302.10470v1.pdf), [*"Bias correction for inverse variance weighting Mendelian randomization"*](https://onlinelibrary.wiley.com/doi/epdf/10.1002/gepi.22522) and [*"Incorporating discovery and replication GWAS into summary data Mendelian randomization studies: A review of current methods and a simple, general and powerful alternative"*](https://www.biorxiv.org/content/10.1101/2023.01.12.523708v2.full.pdf). 

Naturally, we will compare our method against just simply using **IVW** and **MR-RAPS** on the entire data set and selecting instruments based on this entire data set. 

[$\star$]{style="color: blue;"} **Question:** Should we also consider using two-sample/three-sample forms of IVW and MR-RAPS in our simulations? For example, suppose we simulated two fully independent data sets of summary statistics for exposure and outcome, with a sample size each of 100,000 say. If we were to consider three-sample IVW in this setting, would we then have to use our simulation approach to simulate a sample split of 50,000 and 50,000 and use the first split to select instruments and the second split for the SNP-exposure effect sizes? Can we just use the original simulated SNP-outcome effect sizes then? 


[$\star$]{style="color: blue;"} **Note:** It is also possible for us to consider returning to our previous work and applying some of the *Winner's Curse* correction methods discussed in [*"Review and further developments in statistical corrections for Winner's Curse in genetic association studies"*](https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1010546), such as the **empirical Bayes and bootstrap methods**, and using the resulting adjusted SNP-exposure association estimates in the IVW method. For example, the code for doing so together with an example which uses our above simulated data set is provided as follows.


```{r}
library(winnerscurse)

mr_ivw_EB <- function(data, threshold=5e-8){
	## obtain adjusted estimate
  data.exp <- data.frame(rsid=data$SNP,beta=data$beta.exposure,se=data$se.exposure)
  data.exp.EB <- empirical_bayes(data.exp)
  data.exp.EB <- data.exp.EB[order(data.exp.EB$rsid,decreasing=FALSE),]
  data$beta.exposure.EB <- data.exp.EB$beta_EB
  ## subset data set
  data_sig <- data %>% dplyr::filter(2*(stats::pnorm(abs(data$beta.exposure/data$se.exposure), lower.tail=FALSE)) < threshold)
  if(nrow(data_sig) < 3){return(NULL)}else{
  ivw.res <- summary(stats::lm(data_sig$beta.outcome ~ -1 + data_sig$beta.exposure.EB, weights = 1/data_sig$se.outcome^2))
	b <- ivw.res$coef[1,1]
	se <- ivw.res$coef[1,2]/min(1,ivw.res$sigma) 
	pval <- 2 * stats::pnorm(abs(b/se), lower.tail=FALSE)
	return(data.frame(method="mr_ivw_EB", nsnp=nrow(data_sig), b=b, se=se, pval=pval))
  }
}

mr_ivw_boot <- function(data, threshold=5e-8){
	## obtain adjusted estimate
  data.exp <- data.frame(rsid=data$SNP,beta=data$beta.exposure,se=data$se.exposure)
  data.exp.boot <- BR_ss(data.exp)
  data.exp.boot <- data.exp.boot[order(data.exp.boot$rsid,decreasing=FALSE),]
  data$beta.exposure.boot <- data.exp.boot$beta_BR_ss
  ## subset data set
  data_sig <- data %>% dplyr::filter(2*(stats::pnorm(abs(data$beta.exposure/data$se.exposure), lower.tail=FALSE)) < threshold)
  if(nrow(data_sig) < 3){return(NULL)}else{
  ivw.res <- summary(stats::lm(data_sig$beta.outcome ~ -1 + data_sig$beta.exposure.boot, weights = 1/data_sig$se.outcome^2))
	b <- ivw.res$coef[1,1]
	se <- ivw.res$coef[1,2]/min(1,ivw.res$sigma) 
	pval <- 2 * stats::pnorm(abs(b/se), lower.tail=FALSE)
	return(data.frame(method="mr_ivw_boot", nsnp=nrow(data_sig), b=b, se=se, pval=pval))
  }
}

```


```{r}
mr_ivw_EB(data,threshold=5e-8)
```


```{r}
mr_ivw_boot(data,threshold=5e-8)
```

<br>

Interestingly, the three papers mentioned above are all quite different in their approaches to performing a simulation study. Firstly,
[*"Breaking the Winner's Curse in Mendelian Randomization: Rerandomized Inverse Variance Weighted Estimator"*](https://arxiv.org/pdf/2302.10470v1.pdf) simulates suitable summary statistics and considers the proposed **RIVW** estimator together with classical two-/three-sample IVW, debiased IVW (dIVW) and two/three-sample RAPS estimator. All simulations involve two independent non-overlapping samples. 
In their three-sample MR analyses for IVW and MR-RAPS, they "generate a new independent exposure data with the same sample size for IV selection". This is different from the approach considered above for three-sample IVW. 

[$\star$]{style="color: blue;"} **Note:** Code for the RIVW method proposed in this paper does not seem to have been made available. 


Secondly, [*"Bias correction for inverse variance weighting Mendelian randomization"*](https://onlinelibrary.wiley.com/doi/epdf/10.1002/gepi.22522) 
proposed the **MRlap** method and compares it to RAPS, IVW, dIVW, weighted median and weighted mode. However, we note that their simulation approach makes use of genotypic data and thus, differs in our approach where we would just be simulating summary statistics. They essentially simulate individual-level data and subsequently perform a GWAS using BGENIE for each repetition of each simulation scenario. It appears that for most methods, instruments are selected using the same data set with a $p$-value threshold of $5 \times 10^{-8}$. It is mentioned that "using a third sample to select instruments and avoid winner's curse was out of the scope of the paper".

We note that in a section of their study they compare "corrected effects obtained using the full (overlapping) sample to IVW-based effects obtained by splitting it into two halves to avoid sample overlap." This is perhaps something we could consider in the case of full overlap in our simulations. Thus potentially in the case of full overlap we could consider one-sample, two-sample and three-sample IVW and in the case of no overlap we would just need to consider two-sample and three-sample IVW. We described briefly above how we might go about this. 

[$\star$]{style="color: blue;"} **Note:** Code for MRlap is available on GitHub. However, upon looking at the required inputs for the function `mrlap`, it may be difficult for us to include MRlap in our simulation study. 

Finally, [*"Incorporating discovery and replication GWAS into summary data Mendelian randomization studies: A review of current methods and a simple, general and powerful alternative"*](https://www.biorxiv.org/content/10.1101/2023.01.12.523708v2.full.pdf) considers **regression calibration with IVW** and compares it to naive IVW, 3-sample IVW, 3-sample MR-RAPS, MR-RAPS with ZP and MR-RAPS with UMVCUE. It is noted in this paper that MRlap requires "genome-wide information about SNP-exposure and SNP-outcome associations (not only instruments) to estimate the underlying parameters needed for the correction." However, it is also mentioned that "sample overlap...is not relevant to the scope of this paper". Due to the requirement of genome-wide information MRlap is omitted from the simulation study in this paper and only used with real data. As we intend to perform our simulation study in a similar manner by merely simulating summary statistics, we may also be **unable to include MRlap** in that part of our work. 

[$\star$]{style="color: blue;"} **Note:** Code for regression calibration is available on GitHub. Similar to our approach, regression calibration is used in collaboration with other MR methods such as IVW, simple/weighted median and simple/weighted mode.  

In this paper, three non-overlapping independent samples are generated in all simulations, i.e. discovery, replication and outcome. Regression calibration requires independent discovery and replication samples. If we were to include it in our study, we would need to consider splitting our original simulated samples in order to produce independent discovery and replication samples.


In two out of three of the above papers, the debiased IVW method, **dIVW**, is also considered. The authors of this method recommend using all available SNPs to estimate the causal effect. Code is available to implement the dIVW method from the `mr.divw` R package. However, if we use this package as follows and apply `mr.divw` to our previously simulated data set, we get the following result. It can be seen that the result is clearly extremely biased here and thus, further investigation is required here before considering using `mr.divw` in our simulation study. However, if we actually do select SNPs and apply the method to just those SNPs with $p$-values less than $5 \times 10^{-8}$, it can be seen that the result is much closer to what we might expect.


```{r}
library(mr.divw)

mr_divw <- function(data,sig_SNPs=FALSE,threshold=5e-8){
  if(sig_SNPs==TRUE){
     data_sig <- data %>% dplyr::filter(2*(stats::pnorm(abs(data$beta.exposure/data$se.exposure), lower.tail=FALSE)) < threshold)
     res.divw <- mr.divw(data_sig$beta.exposure, data_sig$beta.outcome, data_sig$se.exposure, data_sig$se.outcome, diagnostics=FALSE)
     return(data.frame(method="mr_divw", nsnp=res.divw$n.IV, b=res.divw$beta.hat, se=res.divw$beta.se, pval=2*(stats::pnorm(abs(res.divw$beta.hat/res.divw$beta.se), lower.tail=FALSE))))
  }else{
     res.divw <- mr.divw(data$beta.exposure, data$beta.outcome, data$se.exposure, data$se.outcome, diagnostics=FALSE)
     return(data.frame(method="mr_divw", nsnp=res.divw$n.IV, b=res.divw$beta.hat, se=res.divw$beta.se, pval=2*(stats::pnorm(abs(res.divw$beta.hat/res.divw$beta.se), lower.tail=FALSE))))
  }
}

mr_divw(data)
```


```{r}
mr_divw(data,sig_SNPs=TRUE)
```






