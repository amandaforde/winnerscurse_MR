---
title: "Removing Winner's Curse bias in two-sample Mendelian randomisation with summary
  data"
author: "Amanda Forde"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
---

Source: https://github.com/amandaforde/winnerscurse_MR/winnerscurse_PRSS.Rmd

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(dplyr)
library(TwoSampleMR)
library(ggplot2)
library(patchwork)
library(ggpubr)
library(gridExtra)
library(tidyr)
library(RColorBrewer)
col <- brewer.pal(8,"Dark2")
```


<br> 

## Proposed Method Derivation 

If we randomly split the full data set into two fractions $\pi$ and $1-\pi$, conditional on the full $\beta_{X1}$ and $\beta_{Y1}$ estimators, $\hat\beta_{X1}$ and $\hat\beta_{Y1}$, it is possible to simulate values for $\hat \beta_{X1_\pi}$ and $\hat \beta_{Y1_\pi}$, the estimators in the first fraction of the full sample. Thus, we have $\hat\beta_{X1} = \pi\hat \beta_{X1_\pi} + (1-\pi)\hat \beta_{X1_{1-\pi}}$ and similarly, $\hat\beta_{Y1} = \pi\hat \beta_{Y1_\pi} + (1-\pi)\hat \beta_{Y1_{1-\pi}}$, in which $\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ are the estimators in the second fraction of the sample. With the simulated values for $\hat \beta_{X1_\pi}$ and $\hat \beta_{Y1_\pi}$, values for $\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ can be easily obtained using the equations above. This provides us with an alternative method which removes the problem of *Winner's Curse* using repeated randomisation and selection of SNPs using $\hat \beta_{X1_\pi}$. The IVW method, or indeed, any MR method of choice, is then fitted using $\hat \beta_{X1_{1-\pi}}$ and $\hat \beta_{Y1_{1-\pi}}$ and these IVW estimators are averaged over differing repeated runs.

Therefore, in order to perform this method, we must first establish the (asymptotic) conditional distribution of: $$\begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix}$$


***Note:*** We have referred to our estimators of interest as $\hat\beta_{X1}$ and $\hat\beta_{Y1}$, instead of simply $\hat\beta_{X}$ and $\hat\beta_{Y}$, to ensure clarity in our proof below. In the proof, we will be dealing with linear regression in matrix form and $\hat\beta_X$ will refer to a vector of length 2 in which the first entry will be $\hat\beta_{X0}$ and the second entry will be $\hat\beta_{X1}$, our estimator of interest. 



<br>

**PROOF:**

***Useful property:*** Suppose that we have a random vector $\bf Z$ that is partitioned into components $\bf X$ and $\bf Y$ that is realized from a multivariate normal distribution with mean vector with corresponding components $\bf \mu_X$ and $\bf \mu_Y$, and variance-covariance matrix which has been partitioned into four parts as shown below: 

$$\bf Z = \begin{pmatrix} \bf X \\ \bf Y \end{pmatrix} \sim N\left( \begin{pmatrix} \bf \mu_X \\ \bf \mu_Y \end{pmatrix}, \begin{pmatrix} \bf \sum_X & \bf \sum_{XY} \\ \bf \sum_{YX} & \bf \sum_{Y} \end{pmatrix} \right)$$

Here, $\bf \sum_{X}$ is the variance-covariance matrix for the random vector $\bf X$. $\bf \sum_{Y}$ is the variance-covariance matrix for the random vector $\bf Y$ and $\bf \sum_{YX}$ contains the covariances between the elements of $\bf X$ and the corresponding elements of $\bf Y$. 

Then, the conditional distribution of $\bf Y$ given that $\bf X$ takes a particular value $\bf x$ is also going to be a multivariate normal with conditional expectation: 

$$E(\bf Y | \bf X = \bf x) = \bf \mu_Y + \bf \sum_{YX} \bf \left(\sum_X\right)^{-1}(\bf x - \bf \mu_X)$$
The conditional variance-covariance matrix of $\bf Y$ given that $\bf X = \bf x$ is equal to the variance-covariance matrix for $\bf Y$ minus the term that involves the covariances between $\bf X$ and $\bf Y$ and the variance-covariance matrix for $\bf X$: 

$$\bf{var}(\bf Y | \bf X = \bf x) = \bf \sum_Y - \sum_{YX}\left(\sum_X\right)^{-1}\sum_{XY}$$

Therefore, if we let $\bf X = \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}$ and $\bf Y = \begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix}$, we can then use the above to obtain $$E\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right) \text{ and    }\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right).$$

Thus, we now merely have to work out the values for the following: 

i) ${\bf \mu_Y} = E \begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix}$ 

ii) ${\bf \mu_X} = E \begin{pmatrix} \hat \beta_{X1}  \\ \hat \beta_{Y1} \end{pmatrix}$ 

iii) ${\bf \sum_X} =  \begin{pmatrix} \text{var}\left(\hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right) & \text{var}\left(\hat \beta_{Y1}\right) \end{pmatrix}$  

iv) ${\bf \sum_{XY}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1}, \hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{Y1}, \hat\beta_{X1_\pi}\right) & \text{cov}\left(\hat\beta_{Y1}, \hat \beta_{Y1_\pi}\right) \end{pmatrix}$  

v) ${\bf \sum_{YX}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1_\pi}, \hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{Y1_\pi}, \hat\beta_{X1}\right) & \text{cov}\left(\hat\beta_{Y1_\pi}, \hat \beta_{Y1}\right) \end{pmatrix}$

vi) ${\bf \sum_{Y}} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right) & \text{var}\left(\hat \beta_{Y1_\pi}\right) \end{pmatrix}$ 


<br>

<span style="color: purple;">**$\star \; E(\hat\beta_{X1})$ and $\text{var}(\hat\beta_{X1})$:**</span> 

We will first work out ${\bf \mu_X}$ and ${\bf \sum_X}$. Let us assume the following structural equations model linking genotype at a given SNP, $G$, a continuous exposure, $X$ and a continuous outcome, $Y$. Thus, for a randomly selected individual in the population, genotype, exposure and outcome are causally linked via the following equations. Note that $\varepsilon_X$, $\varepsilon_Y$ and $U$ are zero mean error terms with finite variance. 

$$X = \beta_X G + \varepsilon_X + \delta_{u,x}U$$
$$Y = \beta X + \varepsilon_Y + \delta_{u,y}U$$
Here, the terms $\varepsilon_X$, $\varepsilon_Y$ and $U$ are zero mean error terms with finite variance. It can be shown that $\text{cov}(X,Y) = \beta \cdot \text{var}(X) + \delta_{u,x}\delta_{u,y}\cdot\text{var}(U)$ and thus, the slope coefficient for the regression of $Y$ on $X$ is only unbiased for $\beta$ when there is no confounding, i.e. $\delta_{u,x}\delta_{u,y}\cdot\text{var}(U) = 0$. 


<br>

***Note:*** Consider linear regression in matrix form in which we have $\bf{X} = \bf{G_X}\beta_X + \epsilon$. Then, it is well known that the least squares solution for ${\bf \hat \beta_X} =  \begin{pmatrix} \hat \beta_{X0} \\ \hat \beta_{X1} \\ \end{pmatrix}$ is ${\bf \hat \beta_X} = (\bf{G_X}^T\bf{G_X})^{-1}\bf{G_X}^T\bf{X}$. This vector ${\bf \hat \beta_X}$ is normally distributed with mean $(\bf{G_X}^T\bf{G_X})^{-1}(\bf{G_X}^T\bf{G_X})\beta = \beta$ and covariance matrix $\sigma_X^2(\bf{G_X}^T\bf{G_X})^{-1}$ = $(\bf{G_X}^T\bf{G_X})^{-1}$ if $\sigma_X^2$ is assumed to equal 1. Similarly, if we have $\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$, then the vector $\bf \hat \beta_Y$ is normally distributed with mean $(\bf{G_Y}^T\bf{G_Y})^{-1}(\bf{G_Y}^T\bf{G_Y})\beta_Y = \beta_Y$ and covariance matrix $\sigma_Y^2(\bf{G_Y}^T\bf{G_Y})^{-1}$ = $(\bf{G_Y}^T\bf{G_Y})^{-1}$ if $\sigma_Y^2 = 1$. 

<br>


Now, bearing in mind the above, suppose for each randomly sampled individual in our large data set, we have measured their genotype for a given SNP $i$. As well as genotype information, we have exposure values for $N_X$ of these individuals and outcome data has been collected for $N_Y$ of these individuals. It is possible that there could be individuals for which we have both exposure and outcome values, i.e. there may exist an overlap of individuals that are found in both the exposure and outcome group. We denote the number of individuals that are found in both as $N_{\text{overlap}}$. Clearly, $N_{\text{overlap}} \le \text{min}\{N_X,N_Y\}$. 

Focussing on the sample of $N_X$ individuals with measurements for genotype and exposure, we denote the genotypes for the given SNP $i$ as $G_{1},...,G_{N_X}$, with $G_j \in \{0,1,2\}$ referring to the genotype of individual $j$ at that SNP. These genotypes for the first sample of $N_X$ individuals can be organized in a matrix: 

$$\bf{G_X} =\begin{pmatrix} 1 & G_{1} \\ 1 & G_{2} \\ \vdots & \vdots \\ 1 & G_{N_X} \end{pmatrix} = \begin{pmatrix}  \bf G_{\text{overlap}} \\ \bf G_{X_1} \end{pmatrix}$$
For this SNP $i$, an equation of the form $\bf{X} = \bf{G_X}\beta_X + \epsilon$ is assumed in which $X_j$ is the exposure value of individual $j$, $\bf{G_X}$ is defined as above and $\sigma_X^2 = 1$. In order to obtain $\text{var}(\hat \beta_{X1})$ for this SNP, we must obtain the covariance matrix of $\bf \hat \beta_X$ as $\text{var}(\hat \beta_{X1})$ is equal to the bottom right entry of this matrix. Thus, it follows that: 

$$\bf{G_X}^T\bf{G_X} = \begin{pmatrix} N_X & \sum_{j=1}^{N_X} G_j \\ \sum_{j=1}^{N_X} G_j & \sum_{j=1}^{N_X} G_j^2  \end{pmatrix}$$

If we let $\text{maf}_i < 0.5$ denote the allele frequency for the variant allele of SNP $i$ over the population and assume that the SNP is in Hardy Weinberg equilibrium, then due to independent sampling, each $G_j$ is binomially distributed with $E(G_j) = 2\text{maf}_i$ and $E(G_j^2) = (E(G_j))^2 + \text{var}(G_j) = 4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)$. By the law of large numbers, this gives: 

$$\bf{G_X}^T\bf{G_X} \sim \begin{pmatrix} N_X & N_X \cdot E(G_j)\\ N_X \cdot E(G_j) & N_X \cdot E(G_j^2)  \end{pmatrix} = \begin{pmatrix} N_X & N_X \cdot 2\text{maf}_i \\ N_X \cdot 2\text{maf}_i & N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))  \end{pmatrix}$$

Now we wish to obtain $(\bf{G_X}^T\bf{G_X})^{-1}$ and extract the bottom right entry of the resulting matrix. In the above matrix, we let $a = N_X$, $b = N_X \cdot 2\text{maf}_i$, $c = N_X \cdot 2\text{maf}_i$ and $d = N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))$. Then, it can easily be shown that for SNP $i$, assuming $\sigma_X^2 = 1$, we have: $$\text{var}(\hat \beta_{X1}) \sim \frac{a}{ad-bc} = \frac{N_X}{(N_X)(N_X \cdot (4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i))) - (N_X \cdot 2\text{maf}_i)^2} = \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$$ 


<br> 

<span style="color: purple;">**$\star \; E(\hat\beta_{Y1})$ and $\text{var}(\hat\beta_{Y1})$:**</span>

In a similar fashion, for a given SNP $i$, if an equation of the form $\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$ is assumed in which $Y_j$ is the outcome value of individual $j, j = 1,..., N_Y$, then $\bf{G_Y}$ can be defined as: $$\bf{G_Y} =\begin{pmatrix} 1 & G_{1} \\ 1 & G_{2} \\ \vdots & \vdots \\ 1 & G_{N_Y} \end{pmatrix} = \begin{pmatrix}  \bf G_{\text{overlap}} \\ \bf G_{Y_1} \end{pmatrix}$$

***Note:*** In the above matrix $\bf G_Y$, the entries in the second column from $G_1$ to $G_{N_{\text{overlap}}}$ will be identical to those in the same positions of the second column of $\bf G_X$ as these entries represent the genotypes of individuals who have had both their outcome and exposure values measured. However, we expect the entries in the second column of $\bf G_Y$ from $G_{N_{\text{overlap}}+1}$ to $G_{N_Y}$ to differ as these entries represent the other individuals who have only had their value for the outcome measured. These identical parts of $\bf G_X$ and $\bf G_Y$ are represented by the matrix $\bf G_{\text{overlap}}$.

With $\sigma_Y^2 = 1$, then $\text{var}(\hat\beta_{Y1})$ is easily obtained as: $$\text{var}(\hat \beta_{Y1}) \sim \frac{1}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}$$ 

<br>

<span style="color: purple;">**$\star \; \text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$:**</span>

The aim is now to obtain the covariance of these regression coefficients for each SNP, i.e. $\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$. From above and using the two equations we have constructed $\bf{X} = \bf{G_X}\beta_X + \epsilon$ and $\bf{Y} = \bf{G_Y}\beta_Y + \epsilon$, we know that the estimated regression coefficient vectors corresponding to the SNP-exposure and SNP-outcome regressions are: $${\bf \hat \beta_X} = (\bf{G_X}^T\bf{G_X})^{-1}\bf{G_X}^T\bf{X}$$
$${\bf \hat \beta_Y} = (\bf{G_Y}^T\bf{G_Y})^{-1}\bf{G_Y}^T\bf{Y}$$

It can be easily shown that $N_X({\bf G_X}^T{\bf G_X})^{-1} \sim N_Y(\bf{G_Y}^T\bf{G_Y})^{-1}$ as: $$({\bf G_X}^T{\bf G_X})^{-1} \sim \frac{1}{N_X} \begin{pmatrix} 1 & 2\text{maf}_i \\  2\text{maf}_i &  4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)  \end{pmatrix} ^{-1} \text{and } ({\bf G_Y}^T{\bf G_Y})^{-1} \sim \frac{1}{N_Y} \begin{pmatrix} 1 & 2\text{maf}_i \\  2\text{maf}_i &  4\text{maf}_i^2 + 2\text{maf}_i(1-\text{maf}_i)  \end{pmatrix} ^{-1}.$$ 
Therefore, letting ${\bf C} \sim N_X(\bf{G_X}^T\bf{G_X})^{-1}$ and ${\bf C} \sim N_Y(\bf{G_Y}^T\bf{G_Y})^{-1}$, it follows that: $${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \textbf{cov}\left(\frac{\bf{C}}{N_X}{\bf G_X^T}{\bf X}, \frac{\bf C}{N_Y}{\bf G_Y^T}{\bf Y}\right) = \frac{1}{N_X N_Y} \textbf{cov}\left({\bf C} {\bf G}_{\text{overlap}}^T {\bf X}_{\text{overlap}}, {\bf C} {\bf G}_{\text{overlap}}^T {\bf Y}_{\text{overlap}}\right)$$ giving $${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \frac{\bf C {\bf G}_{\text{overlap}}^T \text{cov}( {\bf X}_{\text{overlap}},  {\bf Y}_{\text{overlap}}) {\bf G}_{\text{overlap} }{\bf C}^T }{N_X N_Y}$$

Here, ${\bf X}_{\text{overlap}}$ and ${\bf Y}_{\text{overlap}}$ denote the first $N_{\text{overlap}}$ elements of $\bf X$ and $\bf Y$. Noting that $\textbf{cov}( {\bf X}_{\text{overlap}},  {\bf Y}_{\text{overlap}}) = \text{cov}(X,Y) {\bf I}_{N_\text{overlap}}$, where ${\bf I}_{N_\text{overlap}}$ is the ${N_\text{overlap}} \times {N_\text{overlap}}$ identity matrix, ${{\bf G}_{\text{overlap}}}^T{{\bf G}_{\text{overlap}}} \sim N_\text{overlap}{\bf C}^{-1}$ and ${\bf C} = {\bf C}^T$, it follows that: $${\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)} \sim \frac{N_{\text{overlap}}\text{cov}(X,Y)}{N_XN_Y} {\bf C}$$

As stated above, we are interested in $\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1})$ which is the bottom right entry of the matrix $\bf \textbf{cov}(\hat\beta_X, \hat\beta_Y)$. As the bottom right entry of $\bf C$ is $\frac{1}{2\text{maf}_i(1-\text{maf}_i)}$ and letting $\text{cov}(X,Y) = \text{cor}(X,Y) \sqrt{\text{var}(X)\text{var}(Y)} = \rho$ in which $\text{cor}(X,Y) = \rho$ and $\text{var}(X) = \text{var}(Y) = 1$, we have: $$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1}) \sim \frac{N_{\text{overlap}}\rho}{N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

Therefore, putting all of the theoretical results together, we have the following expressions for ${\bf \mu_X}$ and ${\bf \sum_X}$: 

ii) ${\bf \mu_X} = E \begin{pmatrix} \hat \beta_{X1}  \\ \hat \beta_{Y1} \end{pmatrix} = \begin{pmatrix} \beta_{X1}  \\ \beta_{Y1} \end{pmatrix}$ 

iii) ${\bf \sum_X} =  \begin{pmatrix} \text{var}\left(\hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1}\right) & \text{var}\left(\hat \beta_{Y1}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)} \begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y}  \end{pmatrix}$  



<br>

#### <span style="color: purple;">**$\star \; E(\hat\beta_{X1_\pi})$, $\text{var}(\hat\beta_{X1_\pi})$ and $\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1})$:**</span>

Now, suppose we split the full data set into two fractions $\pi$ and $1-\pi$. Thus, in the first fraction (sub sample), we would have ***approximately*** $\pi N_X$ individuals who have an exposure measurement, $\pi N_Y$ who have had their outcome measured and $\pi N_{\text{overlap}}$ individuals who have values available for both exposure and outcome. We can write the genotype matrix of those that have had their exposure measured in this first sub sample, denoted by $G_{X\pi}$, as: $$\bf G_{X\pi} = \begin{pmatrix} \bf G_{\text{overlap}{\pi}} \\ \bf G_{X_1\pi} \end{pmatrix}$$

Based on this genotype matrix, and denoting the $(\pi N_X) \times 1$ exposure vector of this sub sample by $\bf X_{\pi}$, the estimated regression coefficient vector corresponding to this SNP-exposure regression in the sub sample is: $${\bf \hat \beta_{X_\pi}} = ({\bf G_{X\pi}^T G_{X\pi}})^{-1}{\bf G_{X\pi}^TX_\pi} \sim \frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_\pi}$$

We saw above that $\text{var}\left(\hat \beta_{X1}\right) \sim \frac{1}{N_X \cdot 2\text{maf}_i(1-\text{maf}_i)}$, and thus, it is clear to see then that we must have $\text{var}\left(\hat \beta_{X1_{\pi}}\right) \sim \frac{1}{\pi N_X \cdot 2\text{maf}_i(1-\text{maf}_i)}$. 

Now, let us consider $\textbf{cov}\left(\bf \hat\beta_{X_\pi}, \hat\beta_X\right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_\pi}, \frac{\bf C}{N_X}{\bf G_{X}^TX}\right)$. As $\bf G_{X\pi}$  and $\bf X_\pi$ are clearly subsets of $\bf G_X$ and $\bf X$, respectively, then we get: $$\textbf{cov}\left({\bf \hat\beta_{X_\pi}, \hat\beta_{X}} \right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_{\pi}}, \frac{\bf C}{N_X}{\bf G_{X\pi}^TX_{\pi}} \right) = \frac{{\bf CG_{X \pi}^T} \text{var}(\bf X_\pi){\bf G_{X \pi}C^T}}{\pi N_X \cdot N_X}$$ This expression can be simplified in a similar manner to previously, noting that ${\bf G_{X\pi}^TG_{X\pi}} \sim \pi N_X \cdot {\bf C}^{-1}$: $$\textbf{cov}({\bf\hat\beta_{X_\pi}, \hat\beta_X}) \sim \frac{\pi N_X \cdot  {\bf C}}{\pi N_X \cdot N_X} = \frac{\bf C}{N_X}$$ We then obtain: $$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1}) \sim \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$$ In addition, from this set-up, we can easily state that $E(\hat\beta_{X1_\pi}) = \beta_{X1}$. 


<br> 

<span style="color: purple;">**$\star \; E(\hat\beta_{Y1_\pi})$, $\text{var}(\hat\beta_{Y1_\pi})$ and $\text{cov}(\hat\beta_{Y1_\pi}, \hat\beta_{Y1})$:**</span>

Now, due to the way we have partitioned the original data set, we can write the genotype matrix for the outcome-SNP regression in the first sub sample, denoted by $\bf G_{Y\pi}$, in the form: $$\bf G_{Y\pi} = \begin{pmatrix}  \bf G_{\text{overlap}{\pi}} \\ \bf G_{Y_1\pi} \end{pmatrix} $$

Similar to when we considered the full data set, $\bf G_{Y\pi}$ and $\bf G_{X\pi}$ share their first $\pi \times N_{\text{overlap}}$ rows, represented by the matrix $\bf G_{\text{overlap}\pi}$. This matrix, $\bf G_{\text{overlap}\pi}$, contains the genotypes of the individuals who are contained in the first sub sample and have had both their outcome and exposure values measured. The estimated regression coefficient vector corresponding to the SNP-outcome regression in the sub sample is: $${\bf \hat \beta_{Y_\pi}} = ({\bf G_{Y\pi}^T G_{Y\pi}})^{-1}{\bf G_{Y\pi}^TY_\pi} \sim \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_\pi}$$
Therefore, following the same process as we used above in order to obtain $\text{var}\left(\hat \beta_{X1_{\pi}}\right)$ and $\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{X1})$, it is easy to see that we should obtain $\text{var}\left(\hat \beta_{Y1_{\pi}}\right) \sim \frac{1}{\pi N_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$ and $\text{cov}(\hat\beta_{Y1_\pi}, \hat\beta_{Y1}) \sim \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}$. Also, we get $E(\hat\beta_{Y1_\pi}) = \beta_{Y1}$.

<br>

<span style="color: purple;">**$\star \; \text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi})$:**</span>

We next consider $\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi})$. We saw that $\bf G_{X\pi}$ can be partitioned as $\bf G_{X\pi} = \begin{pmatrix}  \bf G_{\text{overlap}{\pi}} \\ \bf G_{X_1\pi} \end{pmatrix}$ and similarly, $\bf G_{Y\pi}$ can be partitioned as $\bf G_{Y\pi} = \begin{pmatrix}  \bf G_{\text{overlap}{\pi}} \\ \bf G_{Y_1\pi} \end{pmatrix}$. Therefore, we get: $$\textbf{cov}\left({\bf\hat\beta_{X_\pi}, \hat\beta_{Y_\pi} }\right) \sim \textbf{cov}\left(\frac{\bf C}{\pi N_X}{\bf G_{X\pi}^TX_{\pi}}, \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_{\pi}} \right) = \frac{{\bf CG_{overlap \pi}^T} \text{cov}(\bf X_{overlap\pi}, Y_{overlap\pi}){\bf G_{overlap \pi}C^T}}{\pi N_X \cdot \pi N_Y}$$ This simplifies, giving: $$\textbf{cov}({\bf \hat\beta_{X_\pi}, \hat\beta_{Y_\pi}}) \sim \frac{\pi N_{\text{overlap}}\text{cov}(X,Y)}{\pi N_X \cdot \pi N_Y} {\bf C}$$ and subsequently: $$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1_\pi}) \sim \frac{N_{\text{overlap}}\rho}{\pi N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>

<span style="color: purple;">**$\star \; \text{cov}(\hat\beta_{X1}, \hat\beta_{Y1_\pi})$ and $\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1})$:**</span>

Now, let us find $\textbf{cov}({\bf\hat\beta_{X}, \hat\beta_{Y_\pi}})$ and $\textbf{cov}({\bf\hat\beta_{X_\pi}, \hat\beta_{Y}})$. Using our expressions for $\bf \hat\beta_X$ and $\bf \hat\beta_{Y_\pi}$ detailed above and knowing that the genotype matrices $\bf G_{X}$ and $\bf G_{Y_\pi}$ share genotype information of $\pi N_{\text{overlap}}$ individuals which is contained in $G_{\text{overlap}\pi}$, we get: $$\textbf{cov}\left({\bf \hat\beta_{X}, \hat\beta_{Y_\pi}}\right) \sim \textbf{cov}\left(\frac{\bf C}{N_X}{\bf G_{X}^TX}, \frac{\bf C}{\pi N_Y}{\bf G_{Y\pi}^TY_{\pi}} \right) = \frac{{\bf CG_{overlap \pi}^T} \text{cov}(\bf X_{overlap\pi}, Y_{overlap\pi}){\bf G_{overlap \pi}C^T}}{ N_X \cdot \pi N_Y}$$

It follows that: $$\textbf{cov}({\bf\hat\beta_{X}, \hat\beta_{Y_\pi}}) \sim \frac{ \pi N_{\text{overlap}}\text{cov}(X,Y)}{N_X \cdot \pi N_Y} {\bf C}$$ and this gives: $$\text{cov}(\hat\beta_{X1}, \hat\beta_{Y1_\pi}) \sim \frac{N_{\text{overlap}}\rho}{ N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

In a very similar fashion, using our expressions for $\bf \hat\beta_{X_\pi}$ and $\bf \hat\beta_{Y}$ detailed above, we get: $$\text{cov}(\hat\beta_{X1_\pi}, \hat\beta_{Y1}) \sim \frac{N_{\text{overlap}}\rho}{ N_XN_Y \cdot 2\text{maf}_i(1-\text{maf}_i)}$$

<br>


Therefore, we finally have the following results for $\bf \mu_Y$, ${\bf \sum_{XY}}$, ${\bf \sum_{YX}}$ and ${\bf \sum_{Y}}$: 

i) ${\bf \mu_Y} = E \begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} = \begin{pmatrix} \beta_{X1}  \\ \beta_{Y1} \end{pmatrix}$ 

iv) ${\bf \sum_{XY}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1}, \hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{Y1}, \hat\beta_{X1_\pi}\right) & \text{cov}\left(\hat\beta_{Y1}, \hat \beta_{Y1_\pi}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$  

v) ${\bf \sum_{YX}} = \begin{pmatrix} \text{cov}\left(\hat \beta_{X1_\pi}, \hat \beta_{X1}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1}\right)\\ \text{cov}\left(\hat \beta_{Y1_\pi}, \hat\beta_{X1}\right) & \text{cov}\left(\hat\beta_{Y1_\pi}, \hat \beta_{Y1}\right) \end{pmatrix} = \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$

vi) ${\bf \sum_{Y}} = \begin{pmatrix} \text{var}\left(\hat \beta_{X1_\pi}\right) & \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right)\\ \text{cov}\left(\hat \beta_{X1_\pi}, \hat\beta_{Y1_\pi}\right) & \text{var}\left(\hat \beta_{Y1_\pi}\right) \end{pmatrix} = \frac{1}{\pi} \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}$ 

Therefore, it is clear to see that ${\bf \sum_{X}} = {\bf \sum_{XY}} = {\bf \sum_{YX}}$ and $\pi \cdot {\bf \sum_{Y}} = {\bf \sum_{X}}$. 

Then, using the identities, we can establish $E\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right)$ and $\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right)$ as follows: 


- $E\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right) = \begin{pmatrix} \beta_{X1}  \\ \beta_{Y1} \end{pmatrix} + \bf \sum_{X} \bf \left(\sum_X\right)^{-1} \left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} - \begin{pmatrix} \beta_{X1}  \\ \beta_{Y1} \end{pmatrix} \right) = \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}$

- $\bf{var}\left(\begin{pmatrix} \hat \beta_{X1_\pi}  \\ \hat \beta_{Y1_\pi} \end{pmatrix} | \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right) = \frac{1}{\pi} {\bf \sum_{X}} - {\bf \sum_{X}} \left( {\bf \sum_{X}} \right)^{-1}{\bf \sum_{X}} = \left( \frac{1-\pi}{\pi}\right)  {\bf \sum_{X}} = \left( \frac{1-\pi}{\pi}\right) \cdot \textbf{var}\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix} \right)$

Therefore, we finally have obtained the required conditional distribution:

$$ \left( \begin{pmatrix} \hat \beta_{X1_\pi} \\ \hat \beta_{Y1_\pi} \end{pmatrix} \Bigl \lvert \begin{pmatrix} \hat \beta_{X1} \\ \hat \beta_{Y1} \end{pmatrix} \right) \sim N\left( \begin{pmatrix} \hat\beta_{X1}  \\ \hat\beta_{Y1} \end{pmatrix}, \left( \frac{1-\pi}{\pi}\right) \cdot \frac{1}{2\text{maf}_i(1-\text{maf}_i)}\begin{pmatrix} \frac{1}{N_X} & \frac{N_{\text{overlap}}\rho}{N_XN_Y}\\ \frac{N_{\text{overlap}}\rho}{N_XN_Y} & \frac{1}{N_Y} \end{pmatrix}\right)$$ 

<br>
<br>

## Example of Method Application


We will simulate a simple data set and show how we can use this method in order to obtain a causal estimate. We will first produce $\hat\beta_{X1_\pi}$ and $\hat\beta_{Y1_\pi}$ and the corresponding values for $\hat\beta_{X1_{1-\pi}}$ and $\hat\beta_{Y1_{1-\pi}}$ for each SNP.

***Note:*** In order to use the method described on a set of summary statistics, we also need to be provided values for the minor allele frequency of each SNP, $\text{maf}_i$, the number of individuals whose exposure has been measured, $N_X$, the number of individuals whose outcome has been measured, $N_Y$, the number of individuals overlapping between these two groups, $N_{\text{overlap}}$, as well as the correlation between the exposure and outcome, $\rho$.


First, let us simulate the summary statistics, in which we specify the number of SNPs, $N_{\text{snp}} = 5000$, the heritability of the exposure, $h^2 = 0.4$, the number of individuals in the exposure data set, $N_X = 100,000$, the number of individuals in the outcome data set, $N_Y = 100,000$, the number of overlapping individuals, $N_\text{overlap} = 0$, the correlation between the exposure and the outcome, $\rho = 0.6$, and the ***causal effect*** of the exposure on the outcome, $\beta = 0.3$. 

In order to do this, we establish a suitable function, `sim_mr_ss` which simulates MR summary statistics. This function requires specifications of the number of independent causal SNPs `n_snps`, the heritability of the exposure (the proportion of variance explained in the exposure by these causal SNPs) `h2`, the fraction of overlapping samples `frac_overlap`, the number of samples in the exposure GWAS `n_x` and the outcome GWAS `n_y`, the observed correlation between the exposure and the outcome, `cor_xy` and finally, the causal effect between the exposure and the outcome, `beta_xy`. The function, `sim_mr_ss`, returns a data frame with simulated summary statistics in a suitable form so that MR methods can be applied. Note that we define the fraction of overlapping samples is defined as the number of samples that overlap between the exposure and outcome samples divided by the total number of samples in the exposure/outcome GWASs, whichever has the smallest value.


```{r, message=FALSE, warning=FALSE}
sim_mr_ss <- function(n_snps, h2, frac_overlap, n_x, n_y, cor_xy, beta_xy){
  n_overlap <- frac_overlap*min(n_x, n_y)
  maf <- runif(n_snps, 0.01, 0.05)
  index <- sample(1:n_snps, ceiling(n_snps), replace=FALSE) # random sampling
  beta_gx <- 0
  beta_gx[index] <- rnorm(length(index),0,1)

  var_x <- sum(2*maf*(1-maf)*beta_gx^2)/h2
  beta_gx <- beta_gx/sqrt(var_x) # scaling to represent an exposure with variance 1
  beta_gy <- beta_gx * beta_xy

  var_gx <- 1/(n_x*2*maf*(1-maf)) # var(X)=1
  var_gy <- 1/(n_y*2*maf*(1-maf)) # var(Y)=1
  cov_gx_gy <- ((n_overlap*cor_xy)/(n_x*n_y))*(1/(2*maf*(1-maf)))

  # create covariance matrix for each SNP
  cov_array <- array(dim=c(2, 2, n_snps))
  cov_array[1,1,] <- var_gx
  cov_array[2,1,] <- cov_gx_gy
  cov_array[1,2,] <- cov_array[2,1,]
  cov_array[2,2,] <- var_gy

  summary_stats <- apply(cov_array, 3, function(x){MASS::mvrnorm(n=1, mu=c(0,0), Sigma=x)})
  summary_stats <- t(summary_stats + rbind(beta_gx, beta_gy))

  data <- tibble(
    SNP = 1:n_snps,
    id.exposure="X",
    id.outcome="Y",
    exposure="X",
    outcome="Y",
    beta.exposure = summary_stats[,1],
    beta.outcome = summary_stats[,2],
    se.exposure = sqrt(var_gx),
    se.outcome = sqrt(var_gy),
    N.exposure = n_x,
    N.outcome = n_y,
    N.overlap = n_overlap,
    fval.exposure = (beta.exposure/se.exposure)^2,
    fval.outcome = (beta.outcome/se.outcome)^2,
    pval.exposure = pf(fval.exposure, df1=1, df2=N.exposure-1, lower.tail=FALSE),
    pval.outcome = pf(fval.outcome, df1=1, df2=N.outcome-1, lower.tail=FALSE),
    eaf.exposure = maf,
    eaf.outcome = maf,
    correlation = cor_xy,
    true.exposure = beta_gx,
    true.outcome = beta_gy,
    mr_keep=TRUE
  )

  return(data)
}

set.seed(1998)
data <- sim_mr_ss(n_snps = 5000, h2 = 0.4, frac_overlap = 0.25, n_x = 100000, n_y = 100000, cor_xy = 0.6, beta_xy = 0.3)

## summary statistics
head(data[,1:6])
```

Using this simulated data set and specifying $\pi = 0.5$, we can simulate $\hat\beta_{X1_\pi}$ and $\hat\beta_{Y1_\pi}$ for each SNP. We then obtain $\widehat{\text{se}(\hat\beta_{X1_\pi})}$ by approximating it with $\sqrt{\text{var}(\hat\beta_{X1_\pi})} \sim \sqrt{\frac{1}{\pi}\cdot \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}}$, and subsequently, obtain the corresponding $p$-value for $\hat\beta_{X1_\pi}$ for each SNP. Any SNPs who have a $p$-value less than $5 \times 10^{-8}$ are then **selected**. 

Then, in order to avoid *Winner's Curse*, we obtain the estimates corresponding to the other $1-\pi$ fraction of the data set, i.e. $\hat\beta_{X1_{1-\pi}}$, $\widehat{\text{se}(\hat\beta_{X1_{1-\pi}})}$, $\hat\beta_{Y1_{1-\pi}}$ and $\widehat{\text{se}(\hat\beta_{Y1_{1-\pi}})}$, using: 

- $\hat\beta_{X1_{1-\pi}} = \frac{\hat\beta_{X1} - \pi\hat\beta_{X1_\pi}}{1-\pi}$

- $\hat\beta_{Y1_{1-\pi}} = \frac{\hat\beta_{Y1} - \pi\hat\beta_{Y1_\pi}}{1-\pi}$

- $\widehat{\text{se}(\hat\beta_{X1_{1-\pi}})} \sim \sqrt{\frac{1}{1-\pi}\cdot \frac{1}{N_X\cdot 2\text{maf}_i(1-\text{maf}_i)}}$

- $\widehat{\text{se}(\hat\beta_{Y1_{1-\pi}})} \sim \sqrt{\frac{1}{1-\pi}\cdot \frac{1}{N_Y\cdot 2\text{maf}_i(1-\text{maf}_i)}}$


These estimates are then inputted into the `mr` function from the R package `TwoSampleMR` and the results are shown below. Here, in this example, we will use the IVW method. The idea is that our method will incorporate several iterations of this described process and then obtain an average of all results in order to provide an estimate for $\hat\beta$, the causal effect of the exposure on the outcome, which does not suffer from *Winner's Curse* bias.


```{r, message=FALSE, warning=FALSE}
wc_debias <- function(data,pi=0.5,mr_method="mr_ivw", threshold=5e-8){
  data$maf <- data$eaf.exposure
  # create covariance matrix for the conditional distribution of each SNP
  cond_var_gx <- ((1-pi)/(pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf)))
  cond_var_gy <- ((1-pi)/(pi))*(1/(data$N.outcome[1]*2*data$maf*(1-data$maf)))
  cond_cov_gx_gy <- ((1-pi)/(pi))*(((data$N.overlap[1]*data$correlation[1])/(data$N.exposure[1]*data$N.outcome[1]))*(1/(2*data$maf*(1-data$maf))))

  cond_cov_array <- array(dim=c(2, 2, nrow(data)))
  cond_cov_array[1,1,] <- cond_var_gx
  cond_cov_array[2,1,] <- cond_cov_gx_gy
  cond_cov_array[1,2,] <- cond_cov_array[2,1,]
  cond_cov_array[2,2,] <- cond_var_gy

  summary_stats_sub <- apply(cond_cov_array, 3, function(x) {MASS::mvrnorm(n=1, mu=c(0,0), Sigma=x)})

  summary_stats_sub1 <- t(summary_stats_sub + rbind(data$beta.exposure, data$beta.outcome))
  colnames(summary_stats_sub1) <- c("beta.exposure.1", "beta.outcome.1")
  data <- cbind(data, summary_stats_sub1)

  se.exposure.1 <-  sqrt(((1)/(pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf))))
  pval.exposure.1 <- pf((data$beta.exposure.1/se.exposure.1)^2, df1=1, df2=(pi*data$N.exposure)-1, lower.tail=FALSE)

  data <- data %>% dplyr::filter(pval.exposure.1 < threshold)
  if(nrow(data) < 3){return(NULL)}else{
    beta.exposure.2 <- (data$beta.exposure - pi*data$beta.exposure.1)/(1-pi)
    beta.outcome.2 <- (data$beta.outcome - pi*data$beta.outcome.1)/(1-pi)
    se.exposure.2 <- sqrt(((1)/(1-pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf))))
    se.outcome.2 <- sqrt(((1)/(1-pi))*(1/(data$N.outcome[1]*2*data$maf*(1-data$maf))))

    data <- tibble(
      SNP = data$SNP,
      id.exposure="X",
      id.outcome="Y",
      exposure="X",
      outcome="Y",
      beta.exposure = beta.exposure.2,
      beta.outcome = beta.outcome.2,
      se.exposure = se.exposure.2,
      se.outcome = se.outcome.2,
      fval.exposure = (beta.exposure/se.exposure)^2,
      fval.outcome = (beta.outcome/se.outcome)^2,
      pval.exposure = pf(fval.exposure, df1=1, df2=data$N.exposure-1, lower.tail=FALSE),
      pval.outcome = pf(fval.outcome, df1=1, df2=data$N.outcome-1, lower.tail=FALSE),
      eaf.exposure = data$maf,
      eaf.outcome = data$maf,
      mr_keep=TRUE
    )

    results <- data %>%
      TwoSampleMR::mr(.,method_list=mr_method)
    return(results[,5:9])
  }
}


wc_debias(data)
```


Now, we will repeat the above for $N_{\text{iter}}= 100$ iterations, compute how long it takes to do so, and view the results.

```{r, message=FALSE, warning=FALSE}
set.seed(1998)
prss_res <- function(data,n.iter=100,pi=0.5,mr_method="mr_ivw", threshold=5e-8){
  results <- c()
  for (i in 1:n.iter){
    wc_remove <- wc_debias(data,pi,mr_method, threshold)
    if(is.null(wc_remove) == FALSE){results <- rbind(results,wc_remove)}
  }
  if(length(results) == 0){return(NULL)}else{
    return(results)
  }
}

start.time <- Sys.time()
results <- prss_res(data)
end.time <- Sys.time()
end.time - start.time
head(results)
```


Therefore, if we let $\hat\beta^{(i)}$ be the estimate for the causal effect obtained at iteration $i$, then our final estimate for the causal effect of the exposure on the outcome is denoted by $\overline{\hat\beta}$ = `mean(results$b)`, the average of the estimates obtained at each iteration. This gives <span style="color: purple;">**$\overline{\hat\beta} =$ `r mean(results$b)`**</span>. 

Our next task is to obtain a suitable value for $\text{se}\left( \overline{\hat\beta} \right)$, the standard error of our estimate of the causal effect. In order to do so, let us consider $\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right)^2$ which can be easily computed from our set of results. Now,

$$\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right)^2 \approx E\left[ \sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right) ^2 \right] = \sum_{i=1}^{N_{\text{iter}}} \left[E\left( \left(\hat\beta^{(i)} \right)^2 \right)\right] - N_{\text{iter}}\cdot E\left( \left( \overline{\hat\beta}\right)^2 \right)$$
Using the well-known identity $E(X^2) = \text{var}(X) + (E(X))^2$, we get: 
$$\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right) \approx \sum_{i=1}^{N_{\text{iter}}} \left[ \text{var}\left(\hat\beta^{(i)}\right) + \left(E\left(\hat\beta^{(i)}\right)\right)^2 \right] - N_{\text{iter}}\cdot \left[ \text{var}\left(\overline{\hat\beta}\right) + \left(E\left(\overline{\hat\beta}\right)\right)^2\right]$$

As $\hat\beta^{(i)}$, $i=1,...,N_{\text{iter}}$ and $\overline{\hat\beta}$ are considered unbiased estimates of $\beta$, the true causal effect, then $E\left( \hat\beta^{(i)} \right) = \beta$ for $i=1,...,N_{\text{iter}}$ and $E\left( \overline{\hat\beta} \right) = \beta$ and we have:
$$\sum_{i=1}^{N_{\text{iter}}}\left( \hat\beta^{(i)} - \overline{\hat\beta} \right) \approx \sum_{i=1}^{N_{\text{iter}}} \left[ \text{var}\left(\hat\beta^{(i)}\right)\right] - N_{\text{iter}}\cdot \left[ \text{var}\left(\overline{\hat\beta}\right)\right]$$
This finally gives us an expression for $\text{se}\left( \overline{\hat\beta} \right)$ as follows: 

$$\text{se}\left( \overline{\hat\beta} \right) = \sqrt{\frac{\sum_{i=1}^{N_{\text{iter}}} \left[ \left( \text{se}\left(\hat\beta^{(i)}\right) \right) ^2\right] - \sum_{i=1}^{N_{\text{iter}}}\left[ \hat\beta^{(i)} - \overline{\hat\beta} \right]}{N_{\text{iter}}}}$$
 

```{r, message=FALSE, warning=FALSE}
est_se <- sqrt((sum(results$se^2)-sum((results$b-mean(results$b))^2))/(nrow(results)))
```


Given the above, we get <span style="color: purple;">**$\text{se}\left(\overline{\hat\beta}\right) =$ `r est_se`**</span>. 

In order to summarise these results, we can use the following function, `prss_2` and obtain the average number of significant SNPs obtained on each iteration, `nsnp`, as well as our estimate of the causal effect, `b`, its standard error, `se`, and corresponding $p$-value, `pval`.

```{r, message=FALSE, warning=FALSE}
set.seed(1998)
prss_2 <- function(data,n.iter=100,pi=0.5,mr_method="mr_ivw", threshold=5e-8){
  results <- c()
  for (i in 1:n.iter){
    wc_remove <- wc_debias(data,pi,mr_method, threshold)
    if(is.null(wc_remove) == FALSE){results <- rbind(results,wc_remove)}
  }
  if(length(results) == 0){return(NULL)}else{
    est_se <- sqrt((sum(results$se^2)-sum((results$b-mean(results$b))^2))/(nrow(results)))
    summary <- data.frame(method=c(mr_method), nsnp = c(mean(results$nsnp)), b = c(mean(results$b)), se=c(est_se), pval=c(2*pnorm(mean(results$b)/est_se, lower.tail=FALSE)))
    return(summary)
  }
}

prss_2(data)
```

<br>

## Further Exploration of Method Application 

We have constructed a function above, namely `prss_2` which takes a set of GWAS summary statistics and simulates the idea of splitting the full sample in two. This approach designates one sample split to discovering the significant SNPs while the other sample split is used to estimate both SNP-exposure and SNP-outcome associations. 

This function, `prss_2`, first takes a data frame or data table which must contain the following columns: SNP identifier, `SNP`, the exposure association estimates, `beta.exposure`, the outcome association estimates, `beta.outcome`, the standard errors of both exposure and outcome association estimates, `se.exposure` and `se.outcome`, the minor allele frequencies in the exposure data set, `eaf.exposure`, the number of samples in the exposure GWAS, `N.exposure`, the number of samples in the outcome GWAS, `N.outcome`, the number of overlapping samples, `N.overlap` and the observed correlation between exposure and outcome, `correlation`. The next argument is `n.iter`, the number of iterations with the default being `n.iter=100`. The third argument, `pi` represents the fraction of total samples that the user wishes to include in the first split, the split that is used to discovery the significant SNPs. Its default value is `pi = 0.5`. The final parameter, `mr_method`, is used to specify the MR method the user wishes to use. A list of all available methods can be found by running the function `mr_method_list()` from the R package, `TwoSampleMR`. The default setting is `mr_method = mr_ivw`, which uses inverse variance weighted regression.

**Note:** In the function, `wc_debias`, detailed above, a restriction is included - in order for an MR method to be applied to the summary statistics of the second split, at least 3 SNPs must be deemed significant in the first split. The reason for this is due to the fact that MR methods, such as Egger regression and the weighted median approach, require the summary statistics of at least 3 SNPs in order to obtain an appropriate causal effect estimate. 

<br>

In our proposed approach above, we use `n.iter = 100`. We will now test if increasing the number of iterations will improve our causal effect estimate and reduce the associated standard error. The corresponding values of our naive approach using the inverse variance weighted method are represented by the dashed <span style="color: #388E3C;">green</span> lines. Our current value, `n.iter = 100`, is denoted by the dotted black vertical line. In addition, in the first plot, the true causal effect of the exposure on the outcome of 0.3 is shown by the thick black horizontal line. With respect to improving the causal effect estimate, the plots demonstrate that increasing the number of iterations above 100 will likely not provide much additional benefit. For the standard error of the causal effect estimate, increasing the number of iterations would possibly reduce it further but this is not certain and it will still be more than 0.004 greater than the standard error of the *naive* approach. Therefore, for now, we will continue to use `n.iter = 100` as first suggested.


```{r, message=FALSE, warning=FALSE}
set.seed(1998)
results_ivw <- data %>% dplyr::filter(pval.exposure < 5e-8) %>% 
  TwoSampleMR::mr(.,method_list=c("mr_ivw"))

results <- prss_res(data, n.iter=1000)
est_se <- sqrt((sum(results$se[1]^2)-sum((results$b[1]-mean(results$b[1]))^2))/(nrow(results[1,])))
summary <- data.frame(n.iter = 1, method = results$method[1], nsnp = results$nsnp[1], b = results$b[1], se = est_se, pval = (2*pnorm(mean(results$b[1])/est_se, lower.tail=FALSE))) 
for(i in 2:1000){
  est_se <- sqrt((sum(results$se[1:i]^2)-sum((results$b[1:i]-mean(results$b[1:i]))^2))/(nrow(results[1:i,])))
  summary_new <- data.frame(n.iter = i, method=results$method[i], nsnp = c(mean(results$nsnp[1:i])), b = c(mean(results$b[1:i])), se=c(est_se), pval=c(2*pnorm(mean(results$b[1:i])/est_se, lower.tail=FALSE)))
  summary <- rbind(summary,summary_new)
}

ggplot(summary,aes(x=n.iter,y=b)) + geom_point(size=2) + xlab("No. of iterations") +
  ylab("Causal effect estimate") + geom_hline(yintercept=results_ivw$b, colour="#388E3C", linetype="dashed", size=1) + 
  geom_hline(yintercept=0.3, size=1) +
  geom_vline(xintercept=100, linetype="dotted",size=1) 

ggplot(summary,aes(x=n.iter,y=se)) + geom_point(size=2) + xlab("No. of iterations") +
  ylab("Standard error of causal effect estimate") + geom_hline(yintercept=results_ivw$se, colour="#388E3C", linetype="dashed", size=1) + 
  geom_vline(xintercept=100, linetype="dotted",size=1)
    
```



<br> 

Before we apply other MR methods to our data set accompanied by our proposed method which attempts to remove *Winner's Curse* bias, we will endeavour to evaluate the extent of *Winner's Curse* bias present in this data set. The following function, `wc_summary` provides an indication of how much *Winner's Curse* is present in the significant SNPs of the original data set. It requires a data set in the form outputted by the first function, `sim_mr_ss`. It then outputs four different metrics, namely number of significant SNPs, the percentage of these that are overestimated, the percentage that are significantly overestimated and finally, the MSE of significant SNPs.

```{r}
wc_summary <- function(summary_data, filter=TRUE){
  if(filter == TRUE){
   summary_data_sig <- summary_data %>% dplyr::filter(pf((beta.exposure/se.exposure)^2, df1=1, df2=N.exposure-1, lower.tail=FALSE) < 5e-8)
  }else{
   summary_data_sig <- summary_data
  }
  n_sig <- nrow(summary_data_sig)  # no. of associated SNPs with exposure
  perc_bias <- (sum(abs(summary_data_sig$beta.exposure) > abs(summary_data_sig$true.exposure))/n_sig)*100  # percentage with exposure association overestimated
  perc_x <- (sum(abs(summary_data_sig$beta.exposure) > (abs(summary_data_sig$true.exposure) + 1.96*summary_data_sig$se.exposure))/n_sig)*100  # percentage with exposure association significantly overestimated
  mse <- mean((summary_data_sig$true.exposure-summary_data_sig$beta.exposure)^2)
  wc_test <- data.frame(Metrics=c("No. sig SNPs", "% overestimated", "% significantly overestimated", "MSE"), Quantities=c(round(n_sig,6), round(perc_bias,6), round(perc_x,6), round(mse,6)))
  return(wc_test)
}

wc_summary(data)
```

In the plot below, we have plotted $z$ on the x-axis and $\text{bias}$ on the y-axis, in which we define $z$ to be the $z$-statistic of the SNP-exposure association estimate, i.e. `z = beta.exposure/se.exposure`, and $\text{bias}$ to be the SNP-exposure association estimate minus its true value, i.e. `bias = beta.exposure - true.exposure`. The dots representing SNP association estimates which have been overestimated and deemed as significant at a threshold of $5 \times 10^{-8}$ are coloured in  <span style="color: navy;">navy</span>, while those that have been significantly overestimated at this threshold are coloured in <span style="color: blue;">blue</span>. The <span style="color: darkred;">dark red</span> dashed line represents the $5 \times 10^{-8}$ significance threshold. 

```{r}
data$z <- data$beta.exposure/data$se.exposure
data$bias <- data$beta.exposure - data$true.exposure
subout1 <- data[(abs(data$beta.exposure) > (abs(data$true.exposure))) & (abs(data$beta.exposure/data$se.exposure) > qnorm((5e-8)/2, lower.tail=FALSE)),]
subout <- data[(abs(data$beta.exposure) > (abs(data$true.exposure) + 1.96*data$se.exposure)) & (abs(data$beta.exposure/data$se.exposure) > qnorm((5e-8)/2, lower.tail=FALSE)),]
ggplot(data,aes(x=z,y=bias)) + geom_point(size=2) + geom_point(data=subout1, aes(x=z,y=bias), color='navy', size=2) + geom_point(data=subout, aes(x=z,y=bias), color='blue', size=2) + xlab("z") +
  ylab("bias") + geom_hline(yintercept=0) + 
  geom_vline(xintercept=qnorm((5e-8)/2, lower.tail=FALSE), colour="darkred", linetype="dashed",size=1) +
  geom_vline(xintercept=-qnorm((5e-8)/2, lower.tail=FALSE), colour="darkred", linetype="dashed",size=1) 

```
<br> 

Using this, we can do a **'sanity check'** of our method - is there similar evidence of *Winner's Curse* visible in our simulated second split or do we witness a reduction in these metrics? We explore an answer to this question by simulating a sample split once and computing the above metrics on the simulated set of summary statistics. 

```{r}
data$maf <- data$eaf.exposure
cond_var_gx <- ((1-pi)/(pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf)))
cond_var_gy <- ((1-pi)/(pi))*(1/(data$N.outcome[1]*2*data$maf*(1-data$maf)))
cond_cov_gx_gy <- ((1-pi)/(pi))*(((data$N.overlap[1]*data$correlation[1])/(data$N.exposure[1]*data$N.outcome[1]))*(1/(2*data$maf*(1-data$maf))))
cond_cov_array <- array(dim=c(2, 2, nrow(data)))
cond_cov_array[1,1,] <- cond_var_gx
cond_cov_array[2,1,] <- cond_cov_gx_gy
cond_cov_array[1,2,] <- cond_cov_array[2,1,]
cond_cov_array[2,2,] <- cond_var_gy
summary_stats_sub <- apply(cond_cov_array, 3, function(x) {MASS::mvrnorm(n=1, mu=c(0,0), Sigma=x)})
summary_stats_sub1 <- t(summary_stats_sub + rbind(data$beta.exposure, data$beta.outcome))
colnames(summary_stats_sub1) <- c("beta.exposure.1", "beta.outcome.1")
data_ss <- cbind(data, summary_stats_sub1)
se.exposure.1 <-  sqrt(((1)/(pi))*(1/(data_ss$N.exposure[1]*2*data_ss$maf*(1-data_ss$maf))))
pval.exposure.1 <- pf((data_ss$beta.exposure.1/se.exposure.1)^2, df1=1, df2=(pi*data_ss$N.exposure)-1, lower.tail=FALSE)
data_sig <- data_ss %>% dplyr::filter(pval.exposure.1 < 5e-8)
data_sig$beta.exposure <- (data_sig$beta.exposure - pi*data_sig$beta.exposure.1)/(1-pi)
data_sig$beta.outcome <- (data_sig$beta.outcome - pi*data_sig$beta.outcome.1)/(1-pi)
data_sig$se.exposure <- sqrt(((1)/(1-pi))*(1/(data_sig$N.exposure[1]*2*data_sig$maf*(1-data_sig$maf))))
data_sig$se.outcome <- sqrt(((1)/(1-pi))*(1/(data_sig$N.outcome[1]*2*data_sig$maf*(1-data_sig$maf))))
  
wc_summary(data_sig, filter=FALSE)
```

The result above is reassuring - out of these significant SNPs, none have been significantly overestimated and the percentage that are overestimated is much closer to 50% than previous. Another form of checking if our method is doing as it should, i.e. removing *Winner's Curse* bias, is computing the mean of the absolute original SNP-exposure association estimates, the mean of the absolute simulated SNP-exposure association estimates and the mean of the true association values among the SNPs that have been deemed significant on one iteration of our sample splitting approach. We anticipate that the mean of the absolute original SNP-exposure association estimates should be higher than the other two means and that the mean of the absolute simulated SNP-exposure association estimates should be closer to the mean of the true association values. 

```{r}
data_sig2 <- data_ss %>% dplyr::filter(pval.exposure.1 < 5e-8)
## original estimates
mean(abs(data_sig2$beta.exposure))  
## simulated estimates
mean(abs(data_sig$beta.exposure)) 
## true values
mean(abs(data_sig$true.exposure)) 
```
The trend that we anticipated has been observed - this is encouraging. Of course, we must keep in mind that this is only ***one*** simulation setting and only ***one*** iteration of our proposed simulated sample splitting approach. Thus, in order to thoroughly investigate method performance, a simulation study will be executed. However, before doing so, we will first briefly investigate the use of other MR approaches, such as MR Egger and the weighted median, alongside our proposed method and check if a 50/50 split between discovery and replication is the most suitable. 

### Use of other MR methods 

In this section, we first compare our proposed approach with the results of applying various MR methods that do not correct for Winner's Curse. We will focus on the following 3 MR methods: Inverse variance weighted regression, MR Egger and weighted median. Thus, we first write a function which will apply these methods along with our approach to a set of summary statistics. The results are outputted in a data frame which allows comparison between values, e.g. in the output below, the first row concerns inverse variance weighted regression in which columns 2-5 are the results of the *naive* approach and columns 6-9 are the results of our proposed `prss` approach. We keep in mind that we have set the true causal effect of the exposure on the outcome to 0.3.


```{r}
prss_comp <- function(summary_data, frac=0.5){
dat <- summary_data %>% dplyr::filter(pval.exposure < 5e-8) %>% 
  TwoSampleMR::mr(.,method_list=c("mr_ivw", "mr_egger_regression", "mr_weighted_median"))

prss_ivw <- prss_2(summary_data, pi = frac, mr_method = "mr_ivw")
prss_mregger <- prss_2(summary_data, pi = frac, mr_method = "mr_egger_regression")
prss_med <- prss_2(summary_data, pi = frac, mr_method = "mr_weighted_median")
prss_all <- rbind(prss_ivw, prss_mregger, prss_med)
prss_all <- prss_all %>%
        rename("nsnp_prss" = "nsnp",
               "b_prss" = "b",
               "se_prss" = "se",
               "pval_prss" = "pval")
results <- cbind(dat[,5:9], prss_all[,2:5])
return(results)
}

prss_comp(data)
```

Here, we see that with a 50/50 split for our simulated data set, our approach seems to have slightly improved IVW and weighted median. However, a notable disimprovement of the causal effect estimate is seen for MR Egger in which our approach has resulted in an over-reduction of the estimate. We will need to interrogate this observation further in our simulation study and investigate if the trend of *disagreement* between our approach and MR Egger is present. 


### Determining optimal split fraction

With the data set that we have simulated originally, we will now investigate what percentage discovery/replication split appears to work best. We will start at a 30/70 discovery/replication split and then, increase the percentage of the entire sample used for discovery. It is possible that the optimal split could differ depending on which MR method we wish to use. 

```{r}
set.seed(1998)
data <- sim_mr_ss(n_snps = 5000, h2 = 0.4, frac_overlap = 0.25, n_x = 100000, n_y = 100000, cor_xy = 0.6, beta_xy = 0.3)

disc_30 <- prss_comp(data, frac=0.3)
disc_40 <- prss_comp(data, frac=0.4)
disc_50 <- prss_comp(data, frac=0.5)
disc_60 <- prss_comp(data, frac=0.6)
disc_70 <- prss_comp(data, frac=0.7)
disc_80 <- prss_comp(data, frac=0.8)
disc_90 <- prss_comp(data, frac=0.9)

summary <- data.frame(Disc_split = c(rep(30,3),rep(40,3),rep(50,3),rep(60,3),rep(70,3),rep(80,3),rep(90,3)), Method = rep(disc_30$method,7), b_PRSS = c(disc_30$b_prss, disc_40$b_prss, disc_50$b_prss, disc_60$b_prss, disc_70$b_prss, disc_80$b_prss, disc_90$b_prss), se_PRSS = c(disc_30$se_prss, disc_40$se_prss, disc_50$se_prss, disc_60$se_prss, disc_70$se_prss, disc_80$se_prss, disc_90$se_prss))

ggplot(summary, aes(x = Disc_split, y= b_PRSS, color=Method, shape=Method)) + geom_point(size=3) + scale_color_manual(values=c(col[1],col[2],col[3])) +
  geom_errorbar(aes(ymin=b_PRSS-se_PRSS, ymax=b_PRSS+se_PRSS), width=3,
                 position=position_dodge(0.05)) +
    geom_line() + geom_hline(yintercept=0.3, color = "black", lwd=1) + 
labs(x = "% discovery split", y="Causal effect estimate (PRSS)") + geom_hline(yintercept=disc_30$b[1], linetype="dashed", lwd= 1, color = col[1]) + geom_hline(yintercept=disc_30$b[2], linetype="dashed", lwd = 1, color = col[2]) + 
geom_hline(yintercept=disc_30$b[3], linetype="dashed",lwd = 1, color = col[3])
```

We repeat the above process of varying the percentage of samples in the discovery split when there is no overlap between the exposure and outcome data sets, i.e. `frac_overlap = 0`.


```{r}
set.seed(1998)
data <- sim_mr_ss(n_snps = 5000, h2 = 0.4, frac_overlap = 0, n_x = 100000, n_y = 100000, cor_xy = 0.6, beta_xy = 0.3)

disc_30 <- prss_comp(data, frac=0.3)
disc_40 <- prss_comp(data, frac=0.4)
disc_50 <- prss_comp(data, frac=0.5)
disc_60 <- prss_comp(data, frac=0.6)
disc_70 <- prss_comp(data, frac=0.7)
disc_80 <- prss_comp(data, frac=0.8)
disc_90 <- prss_comp(data, frac=0.9)

summary <- data.frame(Disc_split = c(rep(30,3),rep(40,3),rep(50,3),rep(60,3),rep(70,3),rep(80,3),rep(90,3)), Method = rep(disc_30$method,7), b_PRSS = c(disc_30$b_prss, disc_40$b_prss, disc_50$b_prss, disc_60$b_prss, disc_70$b_prss, disc_80$b_prss, disc_90$b_prss), se_PRSS = c(disc_30$se_prss, disc_40$se_prss, disc_50$se_prss, disc_60$se_prss, disc_70$se_prss, disc_80$se_prss, disc_90$se_prss))

ggplot(summary, aes(x = Disc_split, y= b_PRSS, color=Method, shape=Method)) + geom_point(size=3) + scale_color_manual(values=c(col[1],col[2],col[3])) +
  geom_errorbar(aes(ymin=b_PRSS-se_PRSS, ymax=b_PRSS+se_PRSS), width=3,
                 position=position_dodge(0.05)) +
    geom_line() + geom_hline(yintercept=0.3, color = "black", lwd=1) + 
labs(x = "% discovery split", y="Causal effect estimate (PRSS)") + geom_hline(yintercept=disc_30$b[1], linetype="dashed", lwd= 1, color = col[1]) + geom_hline(yintercept=disc_30$b[2], linetype="dashed", lwd = 1, color = col[2]) + 
geom_hline(yintercept=disc_30$b[3], linetype="dashed",lwd = 1, color = col[3])
```


We now consider the setting in which we have `frac_overlap = 1`. 


```{r}
set.seed(1998)
data <- sim_mr_ss(n_snps = 5000, h2 = 0.4, frac_overlap = 1, n_x = 100000, n_y = 100000, cor_xy = 0.6, beta_xy = 0.3)

disc_30 <- prss_comp(data, frac=0.3)
disc_40 <- prss_comp(data, frac=0.4)
disc_50 <- prss_comp(data, frac=0.5)
disc_60 <- prss_comp(data, frac=0.6)
disc_70 <- prss_comp(data, frac=0.7)
disc_80 <- prss_comp(data, frac=0.8)
disc_90 <- prss_comp(data, frac=0.9)

summary <- data.frame(Disc_split = c(rep(30,3),rep(40,3),rep(50,3),rep(60,3),rep(70,3),rep(80,3),rep(90,3)), Method = rep(disc_30$method,7), b_PRSS = c(disc_30$b_prss, disc_40$b_prss, disc_50$b_prss, disc_60$b_prss, disc_70$b_prss, disc_80$b_prss, disc_90$b_prss), se_PRSS = c(disc_30$se_prss, disc_40$se_prss, disc_50$se_prss, disc_60$se_prss, disc_70$se_prss, disc_80$se_prss, disc_90$se_prss))

ggplot(summary, aes(x = Disc_split, y= b_PRSS, color=Method, shape=Method)) + geom_point(size=3) + scale_color_manual(values=c(col[1],col[2],col[3])) +
  geom_errorbar(aes(ymin=b_PRSS-se_PRSS, ymax=b_PRSS+se_PRSS), width=3,
                 position=position_dodge(0.05)) +
    geom_line() + geom_hline(yintercept=0.3, color = "black", lwd=1) + 
labs(x = "% discovery split", y="Causal effect estimate (PRSS)") + geom_hline(yintercept=disc_30$b[1], linetype="dashed", lwd= 1, color = col[1]) + geom_hline(yintercept=disc_30$b[2], linetype="dashed", lwd = 1, color = col[2]) + 
geom_hline(yintercept=disc_30$b[3], linetype="dashed",lwd = 1, color = col[3]) 
```


From the plots above, it appears that 50/50% is the most appropriate split, as expected. Therefore, in our approach, we will fix the split fraction at 0.5. It is quite concerning how poor our approach with MR Egger is performing - we anticipate that this observation is common to all sets of simulated summary statistics. We will further back this up by executing a simulation study and then, we will focus on obtaining the reason for this extremely poor performance.   


## Results of Simulation Study 

Sources: 

- https://github.com/amandaforde/winnerscurse_MR/winnerscurse_MR_sim1.R 
- https://github.com/amandaforde/winnerscurse_MR/useful_funs_MR.R

As a first step in our simulation study, we will consider the scenario in which the correlation between the exposure and the outcome is fixed at 0.6, `cor_xy = 0.6`, the causal effect of the exposure on the outcome is 0.3, `beta_xy = 0.3` and both the number of samples for the exposure and the number of samples for the outcome is equal to 100,000, `n_x = 100000` and `n_y = 100000`. Thus, the parameters that will be varied here are as follows:

-   Number of causal SNPs, `n_snps` $\in \{ 5,000 , 10,000 \}$

-   Heritability, `h2` $\in \{ 0.3, 0.7\}$

-   Fraction of overlap, `frac_overlap` $\in \{0, 0.25, 0.5, 0.75, 1\}$

This provides us with $2 \times 2 \times 5 = 20$ individual simulation settings, with us first considering a total of 20 repetitions, i.e. `tot_sim = 20`. As suggested above, the code for this simulation study is contained in `winnerscurse_MR_sim1.R`, with various functions such as those already provided in this document contained in `useful_funs_MR.R`. It should be noted here that this is a very simple set-up in which **no SNPs demonstrate pleiotropy**, and we just focus on the use of three MR methods; inverse variance weighted regression, Egger regression and weighted median. 


### Quantifying extent of Winner's Curse bias 

Before analysing the results of application of MR methods, we first have a look at how much *Winner's Curse* bias is present in each simulated set of SNP-exposure association estimates. The first of the two plots below, plot A, shows the number of significant SNPs on the x-axis plotted against the percentage of these SNPs that are overestimated, as defined in the function `wc_summary()`, for each combination of heritability and number of causal SNPs. If no bias was present, it would be anticipated that the percentage of overestimated SNPs would be around 50%. Plot B is very similar except now on the y-axis, we have the percentage of significant SNPs that have been *significantly* overestimated, a quantity which is also defined in `wc_summary()`. Large values fo this metric, percentage of significant SNPs that have been *significantly* overestimated, can be seen as strong indicators for the presence of *Winner's Curse* bias in our SNP-exposure association estimates.

```{r, eval=FALSE}
summary_results_wc <- read.csv("degreee_wc_20sim.csv")

plotA <- ggplot(summary_results_wc, aes(x=as.numeric(n_sig_snps),y=as.numeric(per_over),colour=scenario)) + geom_point(aes(color=scenario)) +  scale_color_manual(values=c(col[4],col[5],col[6],col[8])) + xlab("No. sig SNPs")+ ylab(expression(paste("% sig SNPs overestimated"))) + theme(legend.title = element_blank(),legend.box.background = element_rect(colour = "black"),legend.spacing.y = unit(0, "mm"), legend.background=element_blank())
plotB <- ggplot(summary_results_wc, aes(x=as.numeric(n_sig_snps),y=as.numeric(per_sig_over),colour=scenario)) + geom_point(aes(color=scenario)) +  scale_color_manual(values=c(col[4],col[5],col[6],col[8])) + xlab("No. sig SNPs")+ ylab(expression(paste("% sig SNPs", italic(" significantly "), "overestimated"))) + theme(legend.title = element_blank(),legend.box.background = element_rect(colour = "black"),legend.spacing.y = unit(0, "mm"), legend.background=element_blank())
figure <- plotA + plotB & theme(legend.position = "bottom", legend.box.background = element_rect(colour = "black"))
```

```{r}
figure + plot_layout(guides = "collect") + plot_annotation(tag_levels = 'A') &
  theme(plot.tag = element_text(face = "bold"))
```


The two plots above suggest that as the number of significant SNPs increases, the percentage of significant SNPs *significantly* overestimated decreases and thus, *Winner's Curse* is less of an issue. The plots also demonstrate that *Winner's Curse* seems to be more problematic for traits that have low heritability but greater polygenicity, i.e. traits with a larger number of causal variants. These same observations were previously noted in prior work where we focused on methods which reduce *Winner's Curse* bias in genetic association studies. 

In addition to the above graph, we create four boxplots which display the distribution of MSE for significant SNPs computed for each combination of heritability and number of causal SNPs. 

```{r}
ggplot(summary_results_wc,aes(x=scenario,y=MSE,colour=scenario)) + geom_boxplot(size=0.5,aes(color=scenario)) +  scale_color_manual(values=c(col[4],col[5],col[6],col[8])) + ylab("MSE of sig SNPs") + xlab("") + theme(legend.title = element_blank(),legend.box.background = element_rect(colour = "black"),legend.spacing.y = unit(0, "mm"), legend.background=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank())
```


### Comparison of MR methods


In order to assess the performance of our method, we first load our results files. We also create a function, `summary_table` which will provide us with an indication of when our method performs well and when it performs particularly poorly. For each of the three considered MR methods, this function outputs three values: 

1. `Per_improve` - the percentage of causal effect estimates which are closer to the true value of 0.3, i.e. improved, due to the application of our proposed approach
2. `MSE_naive` - the MSE of the *original* unadjusted approach
3. `MSE_PRSS` - the MSE when our proposed approach is used, which we hope is smaller in value than its corresponding `MSE_naive`

We first apply `summary_table` to our entire set of results, as shown below. This table suggests that our approach works best in conjunction with the IVW method. Concerningly, it seems to result in worse causal effect estimates for MR Egger - an observation which we have previously noted. We will compute this summary table for each fraction of overlap value as it is quite possible that these results and degrees of improvement or disimprovement will vary according to how many overlapping samples there are in the exposure and outcome data sets.  

```{r}
res_ivw <- read.csv("compare_ivw_20sim.csv")
res_egger <- read.csv("compare_egger_20sim.csv")
res_med <- read.csv("compare_med_20sim.csv")
res_comp_A <- rbind(res_ivw, res_egger, res_med)
res_ivw_long <- read.csv("compare_ivw_20sim_long.csv")
res_egger_long <- read.csv("compare_egger_20sim_long.csv")
res_med_long <- read.csv("compare_med_20sim_long.csv")
res_comp <- rbind(res_ivw_long, res_egger_long, res_med_long)

summary_table <- function(res_ivw, res_egger, res_med, res_comp_A){
table_res <- data.frame(Method = unique(res_comp_A$method), Per_improve = c((sum(abs(as.numeric(res_ivw$b) - 0.3) > abs(as.numeric(res_ivw$b_prss)-0.3))/nrow(res_ivw))*100, (sum(abs(as.numeric(res_egger$b) - 0.3) > abs(as.numeric(res_egger$b_prss)-0.3))/nrow(res_egger))*100, (sum(abs(as.numeric(res_med$b) - 0.3) > abs(as.numeric(res_med$b_prss)-0.3))/nrow(res_med))*100), MSE_naive = c(mean((as.numeric(res_ivw$b) - 0.3)^2), mean((as.numeric(res_egger$b) - 0.3)^2), mean((as.numeric(res_med$b) - 0.3)^2)), MSE_PRSS = c(mean((as.numeric(res_ivw$b_prss) - 0.3)^2), mean((as.numeric(res_egger$b_prss) - 0.3)^2), mean((as.numeric(res_med$b_prss) - 0.3)^2)))
return(table_res)
}

summary_table(res_ivw, res_egger, res_med, res_comp_A)
```


<br> 

<span style="color: purple;">**$\star$ No overlap:**</span> 

```{r}
summary_table(res_ivw[res_ivw$frac_overlap == 0,], res_egger[res_egger$frac_overlap == 0,], res_med[res_med$frac_overlap == 0,], res_comp_A[res_comp_A$frac_overlap == 0,])
```

```{r}
res_0 <- res_comp[res_comp$frac_overlap == 0, ]
ggplot(res_0,aes(x=PRSS,y=as.numeric(b),colour=Method)) + geom_boxplot(aes(color=Method), position=position_dodge(0.8)) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3]))  + ylab("Causal effect estimate") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "No overlap")
ggplot(res_0,aes(x=as.numeric(sim),y=as.numeric(b),colour=Method, linetype=PRSS)) + geom_line(aes(color=Method, linetype=PRSS), size=0.8) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3])) + ylab("Causal effect estimate") + xlab("Simulation") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "No overlap") 
```

<br>

<span style="color: purple;">**$\star$ Partial overlap (25%):**</span> 

```{r}
summary_table(res_ivw[res_ivw$frac_overlap == 0.25,], res_egger[res_egger$frac_overlap == 0.25,], res_med[res_med$frac_overlap == 0.25,], res_comp_A[res_comp_A$frac_overlap == 0.25,])
```

```{r}
res_25 <- res_comp[res_comp$frac_overlap == 0.25, ]
ggplot(res_25,aes(x=PRSS,y=as.numeric(b),colour=Method)) + geom_boxplot(aes(color=Method), position=position_dodge(0.8)) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3]))  +
  ylab("Causal effect estimate") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Partial overlap (25%)")
ggplot(res_25,aes(x=as.numeric(sim),y=as.numeric(b),colour=Method, linetype=PRSS)) + geom_line(aes(color=Method, linetype=PRSS), size=0.8) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3])) + ylab("Causal effect estimate") + xlab("Simulation") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Partial overlap (25%)")
```



<br>

<span style="color: purple;">**$\star$ Partial overlap (50%):**</span> 

```{r}
summary_table(res_ivw[res_ivw$frac_overlap == 0.5,], res_egger[res_egger$frac_overlap == 0.5,], res_med[res_med$frac_overlap == 0.5,], res_comp_A[res_comp_A$frac_overlap == 0.5,])
```

```{r}
res_50 <- res_comp[res_comp$frac_overlap == 0.5, ]
ggplot(res_50,aes(x=PRSS,y=as.numeric(b),colour=Method)) + geom_boxplot(aes(color=Method), position=position_dodge(0.8)) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3]))  +
  ylab("Causal effect estimate") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Partial overlap (50%)")
ggplot(res_50,aes(x=as.numeric(sim),y=as.numeric(b),colour=Method, linetype=PRSS)) + geom_line(aes(color=Method, linetype=PRSS), size=0.8) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3])) + ylab("Causal effect estimate") + xlab("Simulation") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Partial overlap (50%)") 
```


<br>

<span style="color: purple;">**$\star$ Partial overlap (75%):**</span> 

```{r}
summary_table(res_ivw[res_ivw$frac_overlap == 0.75,], res_egger[res_egger$frac_overlap == 0.75,], res_med[res_med$frac_overlap == 0.75,], res_comp_A[res_comp_A$frac_overlap == 0.75,])
```

```{r}
res_75 <- res_comp[res_comp$frac_overlap == 0.75, ]
ggplot(res_75,aes(x=PRSS,y=as.numeric(b),colour=Method)) + geom_boxplot(aes(color=Method), position=position_dodge(0.8)) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3]))  +
  ylab("Causal effect estimate") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Partial overlap (75%)")
ggplot(res_75,aes(x=as.numeric(sim),y=as.numeric(b),colour=Method, linetype=PRSS)) + geom_line(aes(color=Method, linetype=PRSS), size=0.8) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3])) + ylab("Causal effect estimate") + xlab("Simulation") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Partial overlap (75%)")
```



<br>

<span style="color: purple;">**$\star$ Full overlap:**</span> 

```{r}
summary_table(res_ivw[res_ivw$frac_overlap == 1,], res_egger[res_egger$frac_overlap == 1,], res_med[res_med$frac_overlap == 1,], res_comp_A[res_comp_A$frac_overlap == 1,])
```

```{r}
res_100 <- res_comp[res_comp$frac_overlap == 1, ]
ggplot(res_100,aes(x=PRSS,y=as.numeric(b),colour=Method)) + geom_boxplot(aes(color=Method), position=position_dodge(0.8)) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3]))  +
  ylab("Causal effect estimate") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Full overlap")
ggplot(res_100,aes(x=as.numeric(sim),y=as.numeric(b),colour=Method, linetype=PRSS)) + geom_line(aes(color=Method, linetype=PRSS), size=0.8) + facet_grid(h2~n_snps,labeller=label_both) + scale_color_manual(values=c(col[1],col[2],col[3])) + ylab("Causal effect estimate") + xlab("Simulation") + geom_hline(yintercept=0.3, colour="black", size=1) + ggtitle(label = "Full overlap")
```

<br> 

<span style="color: purple;">***Summary of above results:***</span> 

- A similar trend is observed for both **IVW and Weighted median** approaches. The **best results** for these two methods seem to occur when we have **fully overlapping** exposure and outcome data sets as for both methods, application of our approach results in an improvement in the causal effect estimate more than 90% of the time. Similar performance is also observed in the case of **no overlap**, especially for IVW. However, when partial overlap is introduced in the form of 25% or 75%, a clear dilution in the effectiveness of our approach for these two methods is evident. Values greater than 50% are still being observed for improvement, however there are more cases in which our approach has resulted in causal effect estimates further from the true value. Finally, when overlap is at 50%, our approach performs very poorly for both IVW and weighted median with values of percentage of improvement as low as 33.75% and 26.25% respectively. This is a concerning result and perhaps is due to the additional variance introduced by our approach. Also, we note that it is known that the direction of weak instrument bias is towards the null when exposure and outcome samples are independent and towards the observed (confounded) estimate when there is complete sample overlap. Thus, naturally, there will be a point of overlap in which the bias acting in both directions will even out if you might say. We hypothesize that this is what is occurring at around 50% in our simulated examples.

- There is an obvious contrast between the results obtained for **MR Egger** and those of the other two methods. The only instance in which the value for percentage of improvement is above 50% is when there is 50% overlap. In all other instance, our approach demonstrates **very poor performance**, with percentage of improvement as low as 7.5% when the exposure and outcomes data sets are fully overlapping. 

<br> 

## Poor performance of MR Egger

In this section, we will attempt to answer one of the most pressing questions that have arose from our study so far:
*Why is the MR Egger method performing so badly with our proposed approach?* 

In order to do so, we will focus on one of the data sets that was simulated, namely the first simulation using MR Egger in which there are 10000 causal SNPs, the heritability is 0.3 and there is no overlap between the outcome and exposure data sets. 

```{r}
ss <- read.csv("bad_egger_example.csv")

## naive MR Egger approach
ss %>% dplyr::filter(pval.exposure < 5e-8) %>% 
  TwoSampleMR::mr(.,method_list=c("mr_egger_regression"))

## simulated MR Egger approach, removing Winner's Curses
set.seed(1998)
prss_2(ss, mr_method = "mr_egger_regression")

set.seed(1998)
res_egger_dist <- prss_res(ss, mr_method = "mr_egger_regression")
res_ivw_dist <- prss_res(ss, mr_method = "mr_ivw")
res_med_dist <- prss_res(ss, mr_method = "mr_weighted_median")
res_all <- rbind(res_egger_dist, res_ivw_dist, res_med_dist)

ggplot(res_all, aes(x=b, fill=method)) + geom_density(alpha=0.4) + geom_vline(aes(xintercept = 0.3), linetype="dashed",size = 1) + scale_fill_manual(values=c(col[1],col[2],col[3])) 

```


Here, can see that the causal effect estimate obtained using our approach is much lower and much further away from the true value of 0.3 than the original MR Egger estimate (0.35525). This would suggest that on each iteration of our approach that poor causal effect estimates are being obtained. This can be seen in a more clear manner in our density plot where we see that the MR Egger estimates on each iteration are a lot more spread out and variable than those obtained with IVW and the weighted median. Thus, we could first investigate if MR Egger simply needs more iterations in order for it to perform better. 


```{r, message=FALSE, warning=FALSE}
set.seed(1998)
egger_mr <- ss %>% dplyr::filter(pval.exposure < 5e-8) %>% 
  TwoSampleMR::mr(.,method_list=c("mr_egger_regression"))

results <- prss_res(ss, n.iter=1000, mr_method="mr_egger_regression")
est_se <- sqrt((sum(results$se[1]^2)-sum((results$b[1]-mean(results$b[1]))^2))/(nrow(results[1,])))
summary <- data.frame(n.iter = 1, method = results$method[1], nsnp = results$nsnp[1], b = results$b[1], se = est_se, pval = (2*pnorm(mean(results$b[1])/est_se, lower.tail=FALSE))) 
for(i in 2:1000){
  est_se <- sqrt((sum(results$se[1:i]^2)-sum((results$b[1:i]-mean(results$b[1:i]))^2))/(nrow(results[1:i,])))
  summary_new <- data.frame(n.iter = i, method=results$method[i], nsnp = c(mean(results$nsnp[1:i])), b = c(mean(results$b[1:i])), se=c(est_se), pval=c(2*pnorm(mean(results$b[1:i])/est_se, lower.tail=FALSE)))
  summary <- rbind(summary,summary_new)
}

ggplot(summary,aes(x=n.iter,y=b)) + geom_point(size=2) + xlab("No. of iterations") +
  ylab("Causal effect estimate") + geom_hline(yintercept=egger_mr$b, colour="#E67E22", linetype="dashed", size=1) + geom_hline(yintercept=0.3, size=1) + geom_vline(xintercept=100, linetype="dotted",size=1) 

ggplot(summary,aes(x=n.iter,y=se)) + geom_point(size=2) + xlab("No. of iterations") +
  ylab("Standard error of causal effect estimate") + geom_hline(yintercept=egger_mr$se, colour="#E67E22", linetype="dashed", size=1) + geom_vline(xintercept=100, linetype="dotted",size=1)
    
```

In the plot above, the corresponding values of our naive approach using the MR Egger method are represented by the dashed <span style="color: #E67E22;">orange</span> lines. Our current value, `n.iter = 100`, is denoted by the dotted black vertical line. In addition, in the first plot, the true causal effect of the exposure on the outcome of 0.3 is shown by the thick black horizontal line. We note that the standard error will still be larger than the original unadjusted form of the matter regardless of the number of iterations. With respect to the causal effect estimate, increasing the number of iterations does seem to move the causal effect estimate closer to the true value of 0.3. However, after 1000 iterations, the estimate is only 0.2298, which is still further from 0.3 than the effect estimate of the original MR Egger method. Therefore, increasing the number of iterations does not solve the problem at hand. 


We will investigate this thoroughly for one iteration, first producing the summary of the linear model coefficients we get when we use the MR Egger approach: 

```{r}
set.seed(1998)
## code for our method
data <- ss
data$maf <- data$eaf.exposure
cond_var_gx <- ((1-pi)/(pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf)))
cond_var_gy <- ((1-pi)/(pi))*(1/(data$N.outcome[1]*2*data$maf*(1-data$maf)))
cond_cov_gx_gy <- ((1-pi)/(pi))*(((data$N.overlap[1]*data$correlation[1])/(data$N.exposure[1]*data$N.outcome[1]))*(1/(2*data$maf*(1-data$maf))))
cond_cov_array <- array(dim=c(2, 2, nrow(data)))
cond_cov_array[1,1,] <- cond_var_gx
cond_cov_array[2,1,] <- cond_cov_gx_gy
cond_cov_array[1,2,] <- cond_cov_array[2,1,]
cond_cov_array[2,2,] <- cond_var_gy
summary_stats_sub <- apply(cond_cov_array, 3, function(x) {MASS::mvrnorm(n=1, mu=c(0,0), Sigma=x)})
summary_stats_sub1 <- t(summary_stats_sub + rbind(data$beta.exposure, data$beta.outcome))
colnames(summary_stats_sub1) <- c("beta.exposure.1", "beta.outcome.1")
data <- cbind(data, summary_stats_sub1)
se.exposure.1 <-  sqrt(((1)/(pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf))))
pval.exposure.1 <- pf((data$beta.exposure.1/se.exposure.1)^2, df1=1, df2=(pi*data$N.exposure)-1, lower.tail=FALSE)
data <- data %>% dplyr::filter(pval.exposure.1 < 5e-8)
beta.exposure.2 <- (data$beta.exposure - pi*data$beta.exposure.1)/(1-pi)
beta.outcome.2 <- (data$beta.outcome - pi*data$beta.outcome.1)/(1-pi)
se.exposure.2 <- sqrt(((1)/(1-pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf))))
se.outcome.2 <- sqrt(((1)/(1-pi))*(1/(data$N.outcome[1]*2*data$maf*(1-data$maf))))
data <- tibble(
      SNP = data$SNP,
      id.exposure="X",
      id.outcome="Y",
      exposure="X",
      outcome="Y",
      beta.exposure = beta.exposure.2,
      beta.outcome = beta.outcome.2,
      se.exposure = se.exposure.2,
      se.outcome = se.outcome.2,
      fval.exposure = (beta.exposure/se.exposure)^2,
      fval.outcome = (beta.outcome/se.outcome)^2,
      pval.exposure = pf(fval.exposure, df1=1, df2=data$N.exposure-1, lower.tail=FALSE),
      pval.outcome = pf(fval.outcome, df1=1, df2=data$N.outcome-1, lower.tail=FALSE),
      eaf.exposure = data$maf,
      eaf.outcome = data$maf,
      mr_keep=TRUE
    )

## code for MR Egger
b_exp <- data$beta.exposure
b_out <- data$beta.outcome
se_exp <- data$se.exposure
se_out <- data$se.outcome
sign0 <- function(x){
  x[x==0] <- 1
  return(sign(x))
}
to_flip <- sign0(b_exp) == -1
b_out = b_out*sign0(b_exp)
b_exp = abs(b_exp)
summary(lm(b_out ~ b_exp, weights=1/se_out^2))$coefficients
```

We see above that the slope parameter is in fact in the opposite direction and nowhere near the true causal effect. We perform a similar weighed linear regression but the slope is fixed at zero, i.e. the inverse variance weighted method, and obtain the following summary of coefficients. The slope is now much closer to the true value. 

```{r}
summary(lm(b_out ~ -1 + b_exp, weights = 1/se_out^2))$coefficients
```

The above summaries can be depicted in the plot below in which the dashed <span style="color: #E67E22;">orange</span> line represents the MR Egger method and the dashed <span style="color: ##388E3C;">green</span> line represents the IVW method. The black line depicts the true causal effect in the absence of pleiotropy.

```{r}
data_flip <- data.frame(beta.exposure = b_exp, beta.outcome = b_out, se_out = se_out)

ggplot(data_flip, aes(x=beta.exposure, y=beta.outcome)) + geom_point() + geom_abline(intercept = 0, slope = 0.3, size=1) + geom_abline(intercept = 0, slope = 0.30800, colour="#388E3C", linetype="dashed", size=1) + geom_abline(intercept = 0.02185, slope = -0.03032, colour="#E67E22", linetype="dashed", size=1) + xlim(c(0,0.1)) 
```


**TO DO: Expand on this exploration of MR Egger's poor performance when it is accompanied by our approach as commenced below.**   


<span style="color: #C21B1E;">**Question:** ***Does our approach introduce more weak instrument bias?***</span>


```{r}
## Extra functions to also calculate Isq statisitic on each iteration
Isq <- function(y,s){
  k = length(y)
  w = 1/s^2; sum.w = sum(w)
  mu.hat = sum(y*w)/sum.w
  Q = sum(w*(y-mu.hat)^2)
  Isq = (Q - (k-1))/Q
  Isq = max(0,Isq)
  return(Isq)
}

wc_debias_Isq <- function(data,pi=0.5,mr_method="mr_ivw", threshold=5e-8){
  data$maf <- data$eaf.exposure
  # create covariance matrix for the conditional distribution of each SNP
  cond_cov_array <- array(dim=c(2, 2, nrow(data)))
  cond_cov_array[1,1,] <- ((1-pi)/(pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf)))
  cond_cov_array[2,1,] <- ((1-pi)/(pi))*(((data$N.overlap[1]*data$correlation[1])/(data$N.exposure[1]*data$N.outcome[1]))*(1/(2*data$maf*(1-data$maf))))
  cond_cov_array[1,2,] <- cond_cov_array[2,1,]
  cond_cov_array[2,2,] <- ((1-pi)/(pi))*(1/(data$N.outcome[1]*2*data$maf*(1-data$maf)))

  summary_stats_sub <- apply(cond_cov_array, 3, function(x) {MASS::mvrnorm(n=1, mu=c(0,0), Sigma=x)})
  summary_stats_sub1 <- t(summary_stats_sub + rbind(data$beta.exposure, data$beta.outcome))
  colnames(summary_stats_sub1) <- c("beta.exposure.1", "beta.outcome.1")
  data <- cbind(data, summary_stats_sub1)
  se.exposure.1 <-  sqrt(((1)/(pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf))))
  pval.exposure.1 <- pf((data$beta.exposure.1/se.exposure.1)^2, df1=1, df2=(pi*data$N.exposure)-1, lower.tail=FALSE)
  
  data <- data %>% dplyr::filter(pval.exposure.1 < threshold)
  if(nrow(data) < 3){return(NULL)}else{
    beta.exposure.2 <- (data$beta.exposure - pi*data$beta.exposure.1)/(1-pi)
    beta.outcome.2 <- (data$beta.outcome - pi*data$beta.outcome.1)/(1-pi)
    se.exposure.2 <- sqrt(((1)/(1-pi))*(1/(data$N.exposure[1]*2*data$maf*(1-data$maf))))
    se.outcome.2 <- sqrt(((1)/(1-pi))*(1/(data$N.outcome[1]*2*data$maf*(1-data$maf))))
    data <- tibble(
      SNP = data$SNP,
      id.exposure="X",
      id.outcome="Y",
      exposure="X",
      outcome="Y",
      beta.exposure = beta.exposure.2,
      beta.outcome = beta.outcome.2,
      se.exposure = se.exposure.2,
      se.outcome = se.outcome.2,
      fval.exposure = (beta.exposure/se.exposure)^2,
      fval.outcome = (beta.outcome/se.outcome)^2,
      pval.exposure = pf(fval.exposure, df1=1, df2=data$N.exposure-1, lower.tail=FALSE),
      pval.outcome = pf(fval.outcome, df1=1, df2=data$N.outcome-1, lower.tail=FALSE),
      eaf.exposure = data$maf,
      eaf.outcome = data$maf,
      mr_keep=TRUE
    )

    Isq_stat <- Isq(data$beta.exposure/data$se.outcome, data$se.exposure/data$se.outcome)
    ave_f <- mean(data$fval.exposure)
    results <- data %>% TwoSampleMR::mr(.,method_list=mr_method)
    results <- cbind(results[,5:9], Isq_stat, ave_f)
    return(results)
  }
}

prss_res_Isq <- function(data,n.iter=100,pi=0.5,mr_method="mr_ivw", threshold=5e-8){
  results <- c()
  for (i in 1:n.iter){
    wc_remove <- wc_debias_Isq(data,pi,mr_method, threshold)
    if(is.null(wc_remove) == FALSE){results <- rbind(results,wc_remove)}
  }
  if(length(results) == 0){return(NULL)}else{
    return(results)
  }
}

set.seed(1998)
results_Isq <- prss_res_Isq(ss,mr_method="mr_egger_regression")

```




```{r}
ss_sig <- ss %>% dplyr::filter(pval.exposure < 5e-8)
Isq_orig <- Isq(ss_sig$beta.exposure/ss_sig$se.outcome, ss_sig$se.exposure/ss_sig$beta.outcome)
f_val <- mean(ss_sig$fval.exposure)

ggplot(results_Isq,aes(x=Isq_stat, fill=method)) + geom_density(alpha=0.4) + xlab("Isq") + scale_fill_manual(values=c(col[4])) + geom_vline(aes(xintercept = Isq_orig), linetype="dashed",size = 1, col=col[4]) + geom_vline(aes(xintercept=1), size=1)

results_Isq$method <- c(rep("Inverse variance weighted", nrow(results_Isq)))
ggplot(results_Isq,aes(x=ave_f, fill=method)) + geom_density(alpha=0.4) + xlab("Average F-statistic") + scale_fill_manual(values=c(col[5])) + geom_vline(aes(xintercept = f_val), linetype="dashed",size = 1, col=col[5]) + geom_vline(aes(xintercept=10), size=1)
```








